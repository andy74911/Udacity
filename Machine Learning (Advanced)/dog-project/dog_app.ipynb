{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络（Convolutional Neural Network, CNN）\n",
    "\n",
    "## 项目：实现一个狗品种识别算法App\n",
    "\n",
    "在这个notebook文件中，有些模板代码已经提供给你，但你还需要实现更多的功能来完成这个项目。除非有明确要求，你无须修改任何已给出的代码。以**'(练习)'**开始的标题表示接下来的代码部分中有你需要实现的功能。这些部分都配有详细的指导，需要实现的部分也会在注释中以'TODO'标出。请仔细阅读所有的提示。\n",
    "\n",
    "除了实现代码外，你还**需要**回答一些与项目及代码相关的问题。每个需要回答的问题都会以 **'问题 X'** 标记。请仔细阅读每个问题，并且在问题后的 **'回答'** 部分写出完整的答案。我们将根据 你对问题的回答 和 撰写代码实现的功能 来对你提交的项目进行评分。\n",
    "\n",
    ">**提示：**Code 和 Markdown 区域可通过 **Shift + Enter** 快捷键运行。此外，Markdown可以通过双击进入编辑模式。\n",
    "\n",
    "项目中显示为_选做_的部分可以帮助你的项目脱颖而出，而不是仅仅达到通过的最低要求。如果你决定追求更高的挑战，请在此 notebook 中完成_选做_部分的代码。\n",
    "\n",
    "---\n",
    "\n",
    "### 让我们开始吧\n",
    "在这个notebook中，你将迈出第一步，来开发可以作为移动端或 Web应用程序一部分的算法。在这个项目的最后，你的程序将能够把用户提供的任何一个图像作为输入。如果可以从图像中检测到一只狗，它会输出对狗品种的预测。如果图像中是一个人脸，它会预测一个与其最相似的狗的种类。下面这张图展示了完成项目后可能的输出结果。（……实际上我们希望每个学生的输出结果不相同！）\n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "在现实世界中，你需要拼凑一系列的模型来完成不同的任务；举个例子，用来预测狗种类的算法会与预测人类的算法不同。在做项目的过程中，你可能会遇到不少失败的预测，因为并不存在完美的算法和模型。你最终提交的不完美的解决方案也一定会给你带来一个有趣的学习经验！\n",
    "\n",
    "### 项目内容\n",
    "\n",
    "我们将这个notebook分为不同的步骤，你可以使用下面的链接来浏览此notebook。\n",
    "\n",
    "* [Step 0](#step0): 导入数据集\n",
    "* [Step 1](#step1): 检测人脸\n",
    "* [Step 2](#step2): 检测狗狗\n",
    "* [Step 3](#step3): 从头创建一个CNN来分类狗品种\n",
    "* [Step 4](#step4): 使用一个CNN来区分狗的品种(使用迁移学习)\n",
    "* [Step 5](#step5): 建立一个CNN来分类狗的品种（使用迁移学习）\n",
    "* [Step 6](#step6): 完成你的算法\n",
    "* [Step 7](#step7): 测试你的算法\n",
    "\n",
    "在该项目中包含了如下的问题：\n",
    "\n",
    "* [问题 1](#question1)\n",
    "* [问题 2](#question2)\n",
    "* [问题 3](#question3)\n",
    "* [问题 4](#question4)\n",
    "* [问题 5](#question5)\n",
    "* [问题 6](#question6)\n",
    "* [问题 7](#question7)\n",
    "* [问题 8](#question8)\n",
    "* [问题 9](#question9)\n",
    "* [问题 10](#question10)\n",
    "* [问题 11](#question11)\n",
    "\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## 步骤 0: 导入数据集\n",
    "\n",
    "### 导入狗数据集\n",
    "在下方的代码单元（cell）中，我们导入了一个狗图像的数据集。我们使用 scikit-learn 库中的 `load_files` 函数来获取一些变量：\n",
    "- `train_files`, `valid_files`, `test_files` - 包含图像的文件路径的numpy数组\n",
    "- `train_targets`, `valid_targets`, `test_targets` - 包含独热编码分类标签的numpy数组\n",
    "- `dog_names` - 由字符串构成的与标签相对应的狗的种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# 定义函数来加载train，test和validation数据集\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# 加载train，test和validation数据集\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# 加载狗品种列表\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# 打印数据统计描述\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入人脸数据集\n",
    "\n",
    "在下方的代码单元中，我们导入人脸图像数据集，文件所在路径存储在名为 `human_files` 的 numpy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# 加载打乱后的人脸数据集的文件名\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# 打印数据集的数据量\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## 步骤1：检测人脸\n",
    " \n",
    "我们将使用 OpenCV 中的 [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) 来检测图像中的人脸。OpenCV 提供了很多预训练的人脸检测模型，它们以XML文件保存在 [github](https://github.com/opencv/opencv/tree/master/data/haarcascades)。我们已经下载了其中一个检测模型，并且把它存储在 `haarcascades` 的目录中。\n",
    "\n",
    "在如下代码单元中，我们将演示如何使用这个检测模型在样本图像中找到人脸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# 提取预训练的人脸检测模型\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# 加载彩色（通道顺序为BGR）图像\n",
    "img = cv2.imread(human_files[3])\n",
    "\n",
    "# 将BGR图像进行灰度处理\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 在图像中找出脸\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# 打印图像中检测到的脸的个数\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# 获取每一个所检测到的脸的识别框\n",
    "for (x,y,w,h) in faces:\n",
    "    # 在人脸图像中绘制出识别框\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# 将BGR图像转变为RGB图像以打印\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 展示含有识别框的图像\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用任何一个检测模型之前，将图像转换为灰度图是常用过程。`detectMultiScale` 函数使用储存在 `face_cascade` 中的的数据，对输入的灰度图像进行分类。\n",
    "\n",
    "在上方的代码中，`faces` 以 numpy 数组的形式，保存了识别到的面部信息。它其中每一行表示一个被检测到的脸，该数据包括如下四个信息：前两个元素  `x`、`y` 代表识别框左上角的 x 和 y 坐标（参照上图，注意 y 坐标的方向和我们默认的方向不同）；后两个元素代表识别框在 x 和 y 轴两个方向延伸的长度 `w` 和 `d`。 \n",
    "\n",
    "### 写一个人脸识别器\n",
    "\n",
    "我们可以将这个程序封装为一个函数。该函数的输入为人脸图像的**路径**，当图像中包含人脸时，该函数返回 `True`，反之返回 `False`。该函数定义如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果img_path路径表示的图像检测到了脸，返回\"True\" \n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **【练习】** 评估人脸检测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<a id='question1'></a>\n",
    "### __问题 1:__ \n",
    "\n",
    "在下方的代码块中，使用 `face_detector` 函数，计算：\n",
    "\n",
    "- `human_files` 的前100张图像中，能够检测到**人脸**的图像占比多少？\n",
    "- `dog_files` 的前100张图像中，能够检测到**人脸**的图像占比多少？\n",
    "\n",
    "理想情况下，人图像中检测到人脸的概率应当为100%，而狗图像中检测到人脸的概率应该为0%。你会发现我们的算法并非完美，但结果仍然是可以接受的。我们从每个数据集中提取前100个图像的文件路径，并将它们存储在`human_files_short`和`dog_files_short`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% files are detected face in human_files_short\n",
      "11% files are detected face in dog_files_short\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "## 请不要修改上方代码\n",
    "\n",
    "\n",
    "## TODO: 基于human_files_short和dog_files_short\n",
    "## 中的图像测试face_detector的表现\n",
    "def print_human_dog_face_cnt(detector, face_or_dog):\n",
    "    human_short_face_cnt, dog_short_face_cnt = 0,0\n",
    "    for file in human_files_short:\n",
    "        if detector(file):\n",
    "            human_short_face_cnt+= 1\n",
    "    for file in dog_files_short:\n",
    "        if detector(file):\n",
    "            dog_short_face_cnt+=1\n",
    "    print('{0:.0%} files are detected {1} in human_files_short'.format(human_short_face_cnt/len(human_files_short),face_or_dog))\n",
    "    print('{0:.0%} files are detected {1} in dog_files_short'.format(dog_short_face_cnt/len(dog_files_short), face_or_dog))\n",
    "    \n",
    "print_human_dog_face_cnt(face_detector, 'face')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='question2'></a>\n",
    "\n",
    "### __问题 2:__ \n",
    "\n",
    "就算法而言，该算法成功与否的关键在于，用户能否提供含有清晰面部特征的人脸图像。\n",
    "那么你认为，这样的要求在实际使用中对用户合理吗？如果你觉得不合理，你能否想到一个方法，即使图像中并没有清晰的面部特征，也能够检测到人脸？\n",
    "\n",
    "__回答:__\n",
    "\n",
    "要求用户提供清晰面部特征是不尽合理的方式，因如何定义何谓是清晰面部特征，用户如何容易了解，这在使用互动上都不太实用。如透过特征方式不易辨别，可以尝试透过类神经网路、支持向量机等机器学习的工具进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='Selection1'></a>\n",
    "### 选做：\n",
    "\n",
    "我们建议在你的算法中使用opencv的人脸检测模型去检测人类图像，不过你可以自由地探索其他的方法，尤其是尝试使用深度学习来解决它:)。请用下方的代码单元来设计和测试你的面部监测算法。如果你决定完成这个_选做_任务，你需要报告算法在每一个数据集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (选做) TODO: 报告另一个面部检测算法在LFW数据集上的表现\n",
    "### 你可以随意使用所需的代码单元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "\n",
    "## 步骤 2: 检测狗狗\n",
    "\n",
    "在这个部分中，我们使用预训练的 [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) 模型去检测图像中的狗。下方的第一行代码就是下载了 ResNet-50 模型的网络结构参数，以及基于 [ImageNet](http://www.image-net.org/) 数据集的预训练权重。\n",
    "\n",
    "ImageNet 这目前一个非常流行的数据集，常被用来测试图像分类等计算机视觉任务相关的算法。它包含超过一千万个 URL，每一个都链接到 [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) 中所对应的一个物体的图像。任给输入一个图像，该 ResNet-50 模型会返回一个对图像中物体的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# 定义ResNet50模型\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "- 在使用 TensorFlow 作为后端的时候，在 Keras 中，CNN 的输入是一个4维数组（也被称作4维张量），它的各维度尺寸为 `(nb_samples, rows, columns, channels)`。其中 `nb_samples` 表示图像（或者样本）的总数，`rows`, `columns`, 和 `channels` 分别表示图像的行数、列数和通道数。\n",
    "\n",
    "\n",
    "- 下方的 `path_to_tensor` 函数实现如下将彩色图像的字符串型的文件路径作为输入，返回一个4维张量，作为 Keras CNN 输入。因为我们的输入图像是彩色图像，因此它们具有三个通道（ `channels` 为 `3`）。\n",
    "    1. 该函数首先读取一张图像，然后将其缩放为 224×224 的图像。\n",
    "    2. 随后，该图像被调整为具有4个维度的张量。\n",
    "    3. 对于任一输入图像，最后返回的张量的维度是：`(1, 224, 224, 3)`。\n",
    "\n",
    "\n",
    "- `paths_to_tensor` 函数将图像路径的字符串组成的 numpy 数组作为输入，并返回一个4维张量，各维度尺寸为 `(nb_samples, 224, 224, 3)`。 在这里，`nb_samples`是提供的图像路径的数据中的样本数量或图像数量。你也可以将 `nb_samples` 理解为数据集中3维张量的个数（每个3维张量表示一个不同的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # 用PIL加载RGB图像为PIL.Image.Image类型\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # 将PIL.Image.Image类型转化为格式为(224, 224, 3)的3维张量\n",
    "    x = image.img_to_array(img)\n",
    "    # 将3维张量转化为格式为(1, 224, 224, 3)的4维张量并返回\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 ResNet-50 架构进行预测\n",
    "\n",
    "对于通过上述步骤得到的四维张量，在把它们输入到 ResNet-50 网络、或 Keras 中其他类似的预训练模型之前，还需要进行一些额外的处理：\n",
    "1. 首先，这些图像的通道顺序为 RGB，我们需要重排他们的通道顺序为 BGR。\n",
    "2. 其次，预训练模型的输入都进行了额外的归一化过程。因此我们在这里也要对这些张量进行归一化，即对所有图像所有像素都减去像素均值 `[103.939, 116.779, 123.68]`（以 RGB 模式表示，根据所有的 ImageNet 图像算出）。\n",
    "\n",
    "导入的 `preprocess_input` 函数实现了这些功能。如果你对此很感兴趣，可以在 [这里](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py) 查看 `preprocess_input`的代码。\n",
    "\n",
    "\n",
    "在实现了图像处理的部分之后，我们就可以使用模型来进行预测。这一步通过 `predict` 方法来实现，它返回一个向量，向量的第 i 个元素表示该图像属于第 i 个 ImageNet 类别的概率。这通过如下的 `ResNet50_predict_labels` 函数实现。\n",
    "\n",
    "通过对预测出的向量取用 argmax 函数（找到有最大概率值的下标序号），我们可以得到一个整数，即模型预测到的物体的类别。进而根据这个 [清单](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)，我们能够知道这具体是哪个品种的狗狗。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # 返回img_path路径的图像的预测向量\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成狗检测模型\n",
    "\n",
    "\n",
    "在研究该 [清单](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) 的时候，你会注意到，狗类别对应的序号为151-268。因此，在检查预训练模型判断图像是否包含狗的时候，我们只需要检查如上的 `ResNet50_predict_labels` 函数是否返回一个介于151和268之间（包含区间端点）的值。\n",
    "\n",
    "我们通过这些想法来完成下方的 `dog_detector` 函数，如果从图像中检测到狗就返回 `True`，否则返回 `False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【作业】评估狗狗检测模型\n",
    "\n",
    "---\n",
    "\n",
    "<a id='question3'></a>\n",
    "### __问题 3:__ \n",
    "\n",
    "在下方的代码块中，使用 `dog_detector` 函数，计算：\n",
    "\n",
    "- `human_files_short`中图像检测到狗狗的百分比？\n",
    "- `dog_files_short`中图像检测到狗狗的百分比？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% files are detected dog in human_files_short\n",
      "100% files are detected dog in dog_files_short\n"
     ]
    }
   ],
   "source": [
    "### TODO: 测试dog_detector函数在human_files_short和dog_files_short的表现\n",
    "print_human_dog_face_cnt(dog_detector, 'dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='step3'></a>\n",
    "\n",
    "## 步骤 3: 从头开始创建一个CNN来分类狗品种\n",
    "\n",
    "\n",
    "现在我们已经实现了一个函数，能够在图像中识别人类及狗狗。但我们需要更进一步的方法，来对狗的类别进行识别。在这一步中，你需要实现一个卷积神经网络来对狗的品种进行分类。你需要__从头实现__你的卷积神经网络（在这一阶段，你还不能使用迁移学习），并且你需要达到超过1%的测试集准确率。在本项目的步骤五种，你还有机会使用迁移学习来实现一个准确率大大提高的模型。\n",
    "\n",
    "在添加卷积层的时候，注意不要加上太多的（可训练的）层。更多的参数意味着更长的训练时间，也就是说你更可能需要一个 GPU 来加速训练过程。万幸的是，Keras 提供了能够轻松预测每次迭代（epoch）花费时间所需的函数。你可以据此推断你算法所需的训练时间。\n",
    "\n",
    "值得注意的是，对狗的图像进行分类是一项极具挑战性的任务。因为即便是一个正常人，也很难区分布列塔尼犬和威尔士史宾格犬。\n",
    "\n",
    "\n",
    "布列塔尼犬（Brittany） | 威尔士史宾格犬（Welsh Springer Spaniel）\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "不难发现其他的狗品种会有很小的类间差别（比如金毛寻回犬和美国水猎犬）。\n",
    "\n",
    "\n",
    "金毛寻回犬（Curly-Coated Retriever） | 美国水猎犬（American Water Spaniel）\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "同样，拉布拉多犬（labradors）有黄色、棕色和黑色这三种。那么你设计的基于视觉的算法将不得不克服这种较高的类间差别，以达到能够将这些不同颜色的同类狗分到同一个品种中。\n",
    "\n",
    "黄色拉布拉多犬（Yellow Labrador） | 棕色拉布拉多犬（Chocolate Labrador） | 黑色拉布拉多犬（Black Labrador）\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "我们也提到了随机分类将得到一个非常低的结果：不考虑品种略有失衡的影响，随机猜测到正确品种的概率是1/133，相对应的准确率是低于1%的。\n",
    "\n",
    "请记住，在深度学习领域，实践远远高于理论。大量尝试不同的框架吧，相信你的直觉！当然，玩得开心！\n",
    "\n",
    "\n",
    "### 数据预处理\n",
    "\n",
    "\n",
    "通过对每张图像的像素值除以255，我们对图像实现了归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6680/6680 [01:23<00:00, 80.32it/s]\n",
      "100%|████████████████████████████████████████| 835/835 [00:11<00:00, 75.56it/s]\n",
      "100%|████████████████████████████████████████| 836/836 [00:10<00:00, 79.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# Keras中的数据预处理过程\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【练习】模型架构\n",
    "\n",
    "\n",
    "创建一个卷积神经网络来对狗品种进行分类。在你代码块的最后，执行 `model.summary()` 来输出你模型的总结信息。\n",
    "    \n",
    "我们已经帮你导入了一些所需的 Python 库，如有需要你可以自行导入。如果你在过程中遇到了困难，如下是给你的一点小提示——该模型能够在5个 epoch 内取得超过1%的测试准确率，并且能在CPU上很快地训练。\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='question4'></a>  \n",
    "\n",
    "### __问题 4:__ \n",
    "\n",
    "在下方的代码块中尝试使用 Keras 搭建卷积网络的架构，并回答相关的问题。\n",
    "\n",
    "1. 你可以尝试自己搭建一个卷积网络的模型，那么你需要回答你搭建卷积网络的具体步骤（用了哪些层）以及为什么这样搭建。\n",
    "2. 你也可以根据上图提示的步骤搭建卷积网络，那么请说明为何如上的架构能够在该问题上取得很好的表现。\n",
    "\n",
    "__回答:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此模型是透过三个卷积层及三个沉化层，卷积层类似截取不同的特征，第一层卷积层经过16个filter，截取16个特征，而沉化层可以减化变数，但又不会失去过多的特征，第二层则透过前一层16个特征，提取32个特征，再透过沉化层的减化，最后一个积层则再提取64个特征出来，这如同滤波器，透过不同的滤波器，解析不同的频率的特征，而64个特征去分析133个品种，应该是蛮有机会可以分辨狗的品种，因此认为应该有一定的预测能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: 定义你的网络架构\n",
    "model.add(Conv2D(filters=16, kernel_size=(2,2), padding='valid', activation='relu', \n",
    "                 input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(2,2), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(2,2), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(133,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 编译模型\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【练习】训练模型\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id='question5'></a>  \n",
    "\n",
    "### __问题 5:__ \n",
    "\n",
    "在下方代码单元训练模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。\n",
    "\n",
    "可选题：你也可以对训练集进行 [数据增强](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)，来优化模型的表现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6340/6680 [===========================>..] - ETA: 7:04 - loss: 4.8807 - acc: 0.0000e+0 - ETA: 3:41 - loss: 4.8889 - acc: 0.0000e+0 - ETA: 2:33 - loss: 4.8959 - acc: 0.0000e+0 - ETA: 1:58 - loss: 4.8965 - acc: 0.0000e+0 - ETA: 1:23 - loss: 4.8983 - acc: 0.0000e+0 - ETA: 1:14 - loss: 4.9014 - acc: 0.0000e+0 - ETA: 1:06 - loss: 4.9004 - acc: 0.0000e+0 - ETA: 55s - loss: 4.8951 - acc: 0.0000e+0 - ETA: 52s - loss: 4.8943 - acc: 0.0000e+ - ETA: 46s - loss: 4.8945 - acc: 0.0000e+ - ETA: 43s - loss: 4.8945 - acc: 0.0036   - ETA: 41s - loss: 4.8933 - acc: 0.00 - ETA: 40s - loss: 4.8931 - acc: 0.00 - ETA: 37s - loss: 4.8933 - acc: 0.00 - ETA: 35s - loss: 4.8933 - acc: 0.00 - ETA: 34s - loss: 4.8932 - acc: 0.00 - ETA: 32s - loss: 4.8945 - acc: 0.00 - ETA: 30s - loss: 4.8938 - acc: 0.00 - ETA: 29s - loss: 4.8932 - acc: 0.00 - ETA: 28s - loss: 4.8931 - acc: 0.00 - ETA: 27s - loss: 4.8933 - acc: 0.00 - ETA: 26s - loss: 4.8930 - acc: 0.00 - ETA: 26s - loss: 4.8930 - acc: 0.00 - ETA: 25s - loss: 4.8934 - acc: 0.00 - ETA: 24s - loss: 4.8930 - acc: 0.00 - ETA: 24s - loss: 4.8927 - acc: 0.00 - ETA: 23s - loss: 4.8921 - acc: 0.00 - ETA: 23s - loss: 4.8928 - acc: 0.00 - ETA: 22s - loss: 4.8928 - acc: 0.00 - ETA: 22s - loss: 4.8921 - acc: 0.00 - ETA: 21s - loss: 4.8917 - acc: 0.00 - ETA: 21s - loss: 4.8916 - acc: 0.00 - ETA: 21s - loss: 4.8915 - acc: 0.00 - ETA: 21s - loss: 4.8911 - acc: 0.00 - ETA: 20s - loss: 4.8909 - acc: 0.00 - ETA: 20s - loss: 4.8910 - acc: 0.00 - ETA: 19s - loss: 4.8907 - acc: 0.00 - ETA: 19s - loss: 4.8908 - acc: 0.00 - ETA: 19s - loss: 4.8903 - acc: 0.00 - ETA: 19s - loss: 4.8904 - acc: 0.00 - ETA: 19s - loss: 4.8904 - acc: 0.00 - ETA: 18s - loss: 4.8904 - acc: 0.00 - ETA: 18s - loss: 4.8905 - acc: 0.00 - ETA: 18s - loss: 4.8902 - acc: 0.00 - ETA: 18s - loss: 4.8900 - acc: 0.00 - ETA: 17s - loss: 4.8901 - acc: 0.00 - ETA: 17s - loss: 4.8903 - acc: 0.00 - ETA: 17s - loss: 4.8898 - acc: 0.00 - ETA: 17s - loss: 4.8896 - acc: 0.00 - ETA: 16s - loss: 4.8897 - acc: 0.00 - ETA: 16s - loss: 4.8897 - acc: 0.00 - ETA: 16s - loss: 4.8897 - acc: 0.00 - ETA: 16s - loss: 4.8896 - acc: 0.00 - ETA: 16s - loss: 4.8898 - acc: 0.00 - ETA: 16s - loss: 4.8900 - acc: 0.00 - ETA: 15s - loss: 4.8900 - acc: 0.00 - ETA: 15s - loss: 4.8896 - acc: 0.00 - ETA: 15s - loss: 4.8895 - acc: 0.00 - ETA: 15s - loss: 4.8893 - acc: 0.00 - ETA: 15s - loss: 4.8893 - acc: 0.00 - ETA: 15s - loss: 4.8889 - acc: 0.00 - ETA: 15s - loss: 4.8890 - acc: 0.00 - ETA: 15s - loss: 4.8885 - acc: 0.00 - ETA: 14s - loss: 4.8888 - acc: 0.00 - ETA: 14s - loss: 4.8884 - acc: 0.00 - ETA: 14s - loss: 4.8884 - acc: 0.00 - ETA: 14s - loss: 4.8882 - acc: 0.00 - ETA: 14s - loss: 4.8884 - acc: 0.00 - ETA: 14s - loss: 4.8883 - acc: 0.00 - ETA: 13s - loss: 4.8880 - acc: 0.00 - ETA: 13s - loss: 4.8879 - acc: 0.00 - ETA: 13s - loss: 4.8882 - acc: 0.00 - ETA: 13s - loss: 4.8882 - acc: 0.00 - ETA: 13s - loss: 4.8881 - acc: 0.00 - ETA: 13s - loss: 4.8881 - acc: 0.00 - ETA: 13s - loss: 4.8881 - acc: 0.00 - ETA: 13s - loss: 4.8882 - acc: 0.00 - ETA: 13s - loss: 4.8881 - acc: 0.00 - ETA: 13s - loss: 4.8881 - acc: 0.00 - ETA: 13s - loss: 4.8880 - acc: 0.00 - ETA: 12s - loss: 4.8880 - acc: 0.00 - ETA: 12s - loss: 4.8879 - acc: 0.00 - ETA: 12s - loss: 4.8876 - acc: 0.00 - ETA: 12s - loss: 4.8872 - acc: 0.00 - ETA: 12s - loss: 4.8867 - acc: 0.01 - ETA: 12s - loss: 4.8864 - acc: 0.00 - ETA: 12s - loss: 4.8864 - acc: 0.00 - ETA: 12s - loss: 4.8866 - acc: 0.00 - ETA: 12s - loss: 4.8864 - acc: 0.00 - ETA: 12s - loss: 4.8865 - acc: 0.00 - ETA: 12s - loss: 4.8864 - acc: 0.00 - ETA: 11s - loss: 4.8866 - acc: 0.00 - ETA: 11s - loss: 4.8867 - acc: 0.00 - ETA: 11s - loss: 4.8870 - acc: 0.00 - ETA: 11s - loss: 4.8873 - acc: 0.00 - ETA: 11s - loss: 4.8872 - acc: 0.00 - ETA: 11s - loss: 4.8866 - acc: 0.00 - ETA: 11s - loss: 4.8867 - acc: 0.00 - ETA: 11s - loss: 4.8869 - acc: 0.00 - ETA: 11s - loss: 4.8869 - acc: 0.00 - ETA: 11s - loss: 4.8869 - acc: 0.00 - ETA: 10s - loss: 4.8867 - acc: 0.00 - ETA: 10s - loss: 4.8866 - acc: 0.00 - ETA: 10s - loss: 4.8871 - acc: 0.00 - ETA: 10s - loss: 4.8872 - acc: 0.00 - ETA: 10s - loss: 4.8871 - acc: 0.00 - ETA: 10s - loss: 4.8874 - acc: 0.00 - ETA: 10s - loss: 4.8873 - acc: 0.01 - ETA: 10s - loss: 4.8874 - acc: 0.01 - ETA: 10s - loss: 4.8872 - acc: 0.01 - ETA: 10s - loss: 4.8871 - acc: 0.01 - ETA: 9s - loss: 4.8871 - acc: 0.0099 - ETA: 9s - loss: 4.8874 - acc: 0.009 - ETA: 9s - loss: 4.8874 - acc: 0.009 - ETA: 9s - loss: 4.8875 - acc: 0.009 - ETA: 9s - loss: 4.8874 - acc: 0.009 - ETA: 9s - loss: 4.8873 - acc: 0.009 - ETA: 9s - loss: 4.8869 - acc: 0.009 - ETA: 9s - loss: 4.8869 - acc: 0.009 - ETA: 9s - loss: 4.8870 - acc: 0.009 - ETA: 9s - loss: 4.8869 - acc: 0.009 - ETA: 8s - loss: 4.8866 - acc: 0.009 - ETA: 8s - loss: 4.8864 - acc: 0.009 - ETA: 8s - loss: 4.8863 - acc: 0.009 - ETA: 8s - loss: 4.8862 - acc: 0.009 - ETA: 8s - loss: 4.8861 - acc: 0.009 - ETA: 8s - loss: 4.8862 - acc: 0.009 - ETA: 8s - loss: 4.8863 - acc: 0.009 - ETA: 8s - loss: 4.8859 - acc: 0.009 - ETA: 8s - loss: 4.8858 - acc: 0.009 - ETA: 8s - loss: 4.8858 - acc: 0.009 - ETA: 7s - loss: 4.8860 - acc: 0.009 - ETA: 7s - loss: 4.8859 - acc: 0.009 - ETA: 7s - loss: 4.8862 - acc: 0.009 - ETA: 7s - loss: 4.8861 - acc: 0.009 - ETA: 7s - loss: 4.8860 - acc: 0.009 - ETA: 7s - loss: 4.8859 - acc: 0.009 - ETA: 7s - loss: 4.8859 - acc: 0.009 - ETA: 7s - loss: 4.8857 - acc: 0.009 - ETA: 7s - loss: 4.8858 - acc: 0.009 - ETA: 7s - loss: 4.8858 - acc: 0.009 - ETA: 7s - loss: 4.8857 - acc: 0.009 - ETA: 6s - loss: 4.8855 - acc: 0.009 - ETA: 6s - loss: 4.8855 - acc: 0.009 - ETA: 6s - loss: 4.8857 - acc: 0.009 - ETA: 6s - loss: 4.8860 - acc: 0.009 - ETA: 6s - loss: 4.8862 - acc: 0.009 - ETA: 6s - loss: 4.8860 - acc: 0.008 - ETA: 6s - loss: 4.8858 - acc: 0.008 - ETA: 6s - loss: 4.8857 - acc: 0.009 - ETA: 6s - loss: 4.8857 - acc: 0.008 - ETA: 6s - loss: 4.8855 - acc: 0.008 - ETA: 5s - loss: 4.8854 - acc: 0.008 - ETA: 5s - loss: 4.8853 - acc: 0.008 - ETA: 5s - loss: 4.8851 - acc: 0.008 - ETA: 5s - loss: 4.8851 - acc: 0.008 - ETA: 5s - loss: 4.8854 - acc: 0.008 - ETA: 5s - loss: 4.8854 - acc: 0.008 - ETA: 5s - loss: 4.8853 - acc: 0.008 - ETA: 5s - loss: 4.8853 - acc: 0.008 - ETA: 5s - loss: 4.8853 - acc: 0.009 - ETA: 5s - loss: 4.8853 - acc: 0.008 - ETA: 5s - loss: 4.8853 - acc: 0.008 - ETA: 5s - loss: 4.8851 - acc: 0.008 - ETA: 4s - loss: 4.8847 - acc: 0.008 - ETA: 4s - loss: 4.8846 - acc: 0.008 - ETA: 4s - loss: 4.8847 - acc: 0.008 - ETA: 4s - loss: 4.8846 - acc: 0.008 - ETA: 4s - loss: 4.8841 - acc: 0.008 - ETA: 4s - loss: 4.8838 - acc: 0.009 - ETA: 4s - loss: 4.8838 - acc: 0.009 - ETA: 4s - loss: 4.8835 - acc: 0.009 - ETA: 4s - loss: 4.8836 - acc: 0.009 - ETA: 4s - loss: 4.8838 - acc: 0.009 - ETA: 3s - loss: 4.8839 - acc: 0.009 - ETA: 3s - loss: 4.8837 - acc: 0.009 - ETA: 3s - loss: 4.8840 - acc: 0.009 - ETA: 3s - loss: 4.8842 - acc: 0.009 - ETA: 3s - loss: 4.8841 - acc: 0.009 - ETA: 3s - loss: 4.8843 - acc: 0.009 - ETA: 3s - loss: 4.8845 - acc: 0.009 - ETA: 3s - loss: 4.8844 - acc: 0.009 - ETA: 3s - loss: 4.8842 - acc: 0.009 - ETA: 3s - loss: 4.8840 - acc: 0.009 - ETA: 3s - loss: 4.8838 - acc: 0.009 - ETA: 3s - loss: 4.8838 - acc: 0.009 - ETA: 2s - loss: 4.8837 - acc: 0.009 - ETA: 2s - loss: 4.8837 - acc: 0.009 - ETA: 2s - loss: 4.8835 - acc: 0.009 - ETA: 2s - loss: 4.8838 - acc: 0.009 - ETA: 2s - loss: 4.8839 - acc: 0.009 - ETA: 2s - loss: 4.8838 - acc: 0.009 - ETA: 2s - loss: 4.8837 - acc: 0.009 - ETA: 2s - loss: 4.8835 - acc: 0.009 - ETA: 2s - loss: 4.8836 - acc: 0.009 - ETA: 2s - loss: 4.8835 - acc: 0.009 - ETA: 2s - loss: 4.8834 - acc: 0.009 - ETA: 1s - loss: 4.8835 - acc: 0.009 - ETA: 1s - loss: 4.8835 - acc: 0.009 - ETA: 1s - loss: 4.8835 - acc: 0.009 - ETA: 1s - loss: 4.8836 - acc: 0.009 - ETA: 1s - loss: 4.8836 - acc: 0.009 - ETA: 1s - loss: 4.8835 - acc: 0.009 - ETA: 1s - loss: 4.8833 - acc: 0.009 - ETA: 1s - loss: 4.8830 - acc: 0.009 - ETA: 1s - loss: 4.8832 - acc: 0.009 - ETA: 1s - loss: 4.8834 - acc: 0.009 - ETA: 1s - loss: 4.8835 - acc: 0.009 - ETA: 1s - loss: 4.8832 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - ETA: 0s - loss: 4.8831 - acc: 0.009 - ETA: 0s - loss: 4.8829 - acc: 0.0099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.8828 - acc: 0.009 - ETA: 0s - loss: 4.8828 - acc: 0.009 - ETA: 0s - loss: 4.8829 - acc: 0.009 - ETA: 0s - loss: 4.8831 - acc: 0.009 - ETA: 0s - loss: 4.8831 - acc: 0.009 - ETA: 0s - loss: 4.8834 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - ETA: 0s - loss: 4.8833 - acc: 0.009 - 18s 3ms/step - loss: 4.8832 - acc: 0.0096 - val_loss: 4.8700 - val_acc: 0.0144\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.87001, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 14s - loss: 4.8588 - acc: 0.0000e+ - ETA: 15s - loss: 4.8527 - acc: 0.0167   - ETA: 15s - loss: 4.8631 - acc: 0.01 - ETA: 14s - loss: 4.8537 - acc: 0.00 - ETA: 15s - loss: 4.8560 - acc: 0.00 - ETA: 15s - loss: 4.8611 - acc: 0.00 - ETA: 15s - loss: 4.8637 - acc: 0.01 - ETA: 15s - loss: 4.8692 - acc: 0.01 - ETA: 15s - loss: 4.8683 - acc: 0.01 - ETA: 15s - loss: 4.8659 - acc: 0.01 - ETA: 15s - loss: 4.8652 - acc: 0.00 - ETA: 14s - loss: 4.8632 - acc: 0.01 - ETA: 14s - loss: 4.8664 - acc: 0.01 - ETA: 14s - loss: 4.8676 - acc: 0.00 - ETA: 14s - loss: 4.8692 - acc: 0.01 - ETA: 14s - loss: 4.8690 - acc: 0.01 - ETA: 14s - loss: 4.8677 - acc: 0.01 - ETA: 14s - loss: 4.8614 - acc: 0.01 - ETA: 14s - loss: 4.8631 - acc: 0.01 - ETA: 14s - loss: 4.8626 - acc: 0.01 - ETA: 14s - loss: 4.8624 - acc: 0.01 - ETA: 14s - loss: 4.8598 - acc: 0.01 - ETA: 14s - loss: 4.8576 - acc: 0.01 - ETA: 14s - loss: 4.8571 - acc: 0.01 - ETA: 14s - loss: 4.8572 - acc: 0.01 - ETA: 14s - loss: 4.8596 - acc: 0.01 - ETA: 14s - loss: 4.8617 - acc: 0.01 - ETA: 14s - loss: 4.8617 - acc: 0.01 - ETA: 13s - loss: 4.8608 - acc: 0.01 - ETA: 13s - loss: 4.8612 - acc: 0.01 - ETA: 13s - loss: 4.8605 - acc: 0.01 - ETA: 13s - loss: 4.8608 - acc: 0.01 - ETA: 13s - loss: 4.8619 - acc: 0.01 - ETA: 13s - loss: 4.8597 - acc: 0.01 - ETA: 13s - loss: 4.8610 - acc: 0.01 - ETA: 13s - loss: 4.8614 - acc: 0.01 - ETA: 13s - loss: 4.8624 - acc: 0.01 - ETA: 13s - loss: 4.8631 - acc: 0.01 - ETA: 13s - loss: 4.8645 - acc: 0.01 - ETA: 13s - loss: 4.8648 - acc: 0.01 - ETA: 13s - loss: 4.8642 - acc: 0.01 - ETA: 13s - loss: 4.8643 - acc: 0.01 - ETA: 13s - loss: 4.8647 - acc: 0.01 - ETA: 13s - loss: 4.8659 - acc: 0.01 - ETA: 13s - loss: 4.8651 - acc: 0.01 - ETA: 12s - loss: 4.8656 - acc: 0.01 - ETA: 12s - loss: 4.8667 - acc: 0.01 - ETA: 12s - loss: 4.8671 - acc: 0.00 - ETA: 12s - loss: 4.8672 - acc: 0.01 - ETA: 12s - loss: 4.8681 - acc: 0.01 - ETA: 12s - loss: 4.8691 - acc: 0.01 - ETA: 12s - loss: 4.8693 - acc: 0.01 - ETA: 12s - loss: 4.8697 - acc: 0.01 - ETA: 12s - loss: 4.8709 - acc: 0.01 - ETA: 12s - loss: 4.8710 - acc: 0.01 - ETA: 12s - loss: 4.8706 - acc: 0.01 - ETA: 12s - loss: 4.8707 - acc: 0.01 - ETA: 12s - loss: 4.8706 - acc: 0.01 - ETA: 12s - loss: 4.8698 - acc: 0.01 - ETA: 11s - loss: 4.8701 - acc: 0.01 - ETA: 11s - loss: 4.8697 - acc: 0.01 - ETA: 11s - loss: 4.8688 - acc: 0.01 - ETA: 11s - loss: 4.8690 - acc: 0.01 - ETA: 11s - loss: 4.8688 - acc: 0.01 - ETA: 11s - loss: 4.8688 - acc: 0.01 - ETA: 11s - loss: 4.8682 - acc: 0.01 - ETA: 11s - loss: 4.8668 - acc: 0.01 - ETA: 11s - loss: 4.8678 - acc: 0.01 - ETA: 11s - loss: 4.8684 - acc: 0.01 - ETA: 11s - loss: 4.8683 - acc: 0.01 - ETA: 11s - loss: 4.8679 - acc: 0.01 - ETA: 11s - loss: 4.8681 - acc: 0.01 - ETA: 11s - loss: 4.8683 - acc: 0.01 - ETA: 10s - loss: 4.8693 - acc: 0.01 - ETA: 10s - loss: 4.8689 - acc: 0.01 - ETA: 10s - loss: 4.8698 - acc: 0.01 - ETA: 10s - loss: 4.8708 - acc: 0.01 - ETA: 10s - loss: 4.8703 - acc: 0.01 - ETA: 10s - loss: 4.8705 - acc: 0.01 - ETA: 10s - loss: 4.8703 - acc: 0.01 - ETA: 10s - loss: 4.8701 - acc: 0.01 - ETA: 10s - loss: 4.8698 - acc: 0.01 - ETA: 10s - loss: 4.8696 - acc: 0.01 - ETA: 10s - loss: 4.8703 - acc: 0.01 - ETA: 10s - loss: 4.8700 - acc: 0.01 - ETA: 10s - loss: 4.8697 - acc: 0.01 - ETA: 9s - loss: 4.8699 - acc: 0.0136 - ETA: 9s - loss: 4.8689 - acc: 0.013 - ETA: 9s - loss: 4.8690 - acc: 0.013 - ETA: 9s - loss: 4.8689 - acc: 0.013 - ETA: 9s - loss: 4.8693 - acc: 0.013 - ETA: 9s - loss: 4.8688 - acc: 0.013 - ETA: 9s - loss: 4.8677 - acc: 0.013 - ETA: 9s - loss: 4.8681 - acc: 0.013 - ETA: 9s - loss: 4.8685 - acc: 0.013 - ETA: 9s - loss: 4.8688 - acc: 0.013 - ETA: 9s - loss: 4.8687 - acc: 0.013 - ETA: 9s - loss: 4.8681 - acc: 0.013 - ETA: 8s - loss: 4.8689 - acc: 0.013 - ETA: 8s - loss: 4.8690 - acc: 0.013 - ETA: 8s - loss: 4.8691 - acc: 0.013 - ETA: 8s - loss: 4.8691 - acc: 0.013 - ETA: 8s - loss: 4.8696 - acc: 0.013 - ETA: 8s - loss: 4.8696 - acc: 0.013 - ETA: 8s - loss: 4.8701 - acc: 0.013 - ETA: 8s - loss: 4.8703 - acc: 0.013 - ETA: 8s - loss: 4.8701 - acc: 0.012 - ETA: 8s - loss: 4.8701 - acc: 0.013 - ETA: 8s - loss: 4.8701 - acc: 0.013 - ETA: 8s - loss: 4.8698 - acc: 0.013 - ETA: 8s - loss: 4.8692 - acc: 0.013 - ETA: 7s - loss: 4.8691 - acc: 0.013 - ETA: 7s - loss: 4.8691 - acc: 0.013 - ETA: 7s - loss: 4.8691 - acc: 0.013 - ETA: 7s - loss: 4.8687 - acc: 0.012 - ETA: 7s - loss: 4.8682 - acc: 0.012 - ETA: 7s - loss: 4.8677 - acc: 0.012 - ETA: 7s - loss: 4.8682 - acc: 0.012 - ETA: 7s - loss: 4.8681 - acc: 0.012 - ETA: 7s - loss: 4.8679 - acc: 0.012 - ETA: 7s - loss: 4.8674 - acc: 0.012 - ETA: 7s - loss: 4.8674 - acc: 0.012 - ETA: 7s - loss: 4.8683 - acc: 0.012 - ETA: 7s - loss: 4.8685 - acc: 0.012 - ETA: 6s - loss: 4.8686 - acc: 0.012 - ETA: 6s - loss: 4.8687 - acc: 0.012 - ETA: 6s - loss: 4.8687 - acc: 0.012 - ETA: 6s - loss: 4.8695 - acc: 0.012 - ETA: 6s - loss: 4.8695 - acc: 0.012 - ETA: 6s - loss: 4.8695 - acc: 0.012 - ETA: 6s - loss: 4.8692 - acc: 0.012 - ETA: 6s - loss: 4.8695 - acc: 0.011 - ETA: 6s - loss: 4.8692 - acc: 0.011 - ETA: 6s - loss: 4.8692 - acc: 0.011 - ETA: 6s - loss: 4.8693 - acc: 0.011 - ETA: 6s - loss: 4.8696 - acc: 0.011 - ETA: 6s - loss: 4.8697 - acc: 0.011 - ETA: 5s - loss: 4.8696 - acc: 0.011 - ETA: 5s - loss: 4.8696 - acc: 0.012 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8700 - acc: 0.011 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8698 - acc: 0.011 - ETA: 5s - loss: 4.8697 - acc: 0.011 - ETA: 5s - loss: 4.8697 - acc: 0.011 - ETA: 5s - loss: 4.8694 - acc: 0.011 - ETA: 4s - loss: 4.8688 - acc: 0.011 - ETA: 4s - loss: 4.8692 - acc: 0.011 - ETA: 4s - loss: 4.8688 - acc: 0.011 - ETA: 4s - loss: 4.8688 - acc: 0.011 - ETA: 4s - loss: 4.8689 - acc: 0.011 - ETA: 4s - loss: 4.8689 - acc: 0.011 - ETA: 4s - loss: 4.8689 - acc: 0.011 - ETA: 4s - loss: 4.8683 - acc: 0.011 - ETA: 4s - loss: 4.8683 - acc: 0.011 - ETA: 4s - loss: 4.8683 - acc: 0.011 - ETA: 4s - loss: 4.8681 - acc: 0.011 - ETA: 4s - loss: 4.8680 - acc: 0.011 - ETA: 4s - loss: 4.8678 - acc: 0.011 - ETA: 4s - loss: 4.8675 - acc: 0.011 - ETA: 4s - loss: 4.8681 - acc: 0.011 - ETA: 3s - loss: 4.8680 - acc: 0.011 - ETA: 3s - loss: 4.8683 - acc: 0.011 - ETA: 3s - loss: 4.8676 - acc: 0.011 - ETA: 3s - loss: 4.8679 - acc: 0.011 - ETA: 3s - loss: 4.8676 - acc: 0.011 - ETA: 3s - loss: 4.8682 - acc: 0.011 - ETA: 3s - loss: 4.8678 - acc: 0.011 - ETA: 3s - loss: 4.8679 - acc: 0.011 - ETA: 3s - loss: 4.8681 - acc: 0.011 - ETA: 3s - loss: 4.8684 - acc: 0.011 - ETA: 3s - loss: 4.8683 - acc: 0.011 - ETA: 3s - loss: 4.8680 - acc: 0.011 - ETA: 3s - loss: 4.8679 - acc: 0.011 - ETA: 3s - loss: 4.8685 - acc: 0.011 - ETA: 2s - loss: 4.8686 - acc: 0.011 - ETA: 2s - loss: 4.8685 - acc: 0.011 - ETA: 2s - loss: 4.8685 - acc: 0.011 - ETA: 2s - loss: 4.8683 - acc: 0.011 - ETA: 2s - loss: 4.8684 - acc: 0.011 - ETA: 2s - loss: 4.8684 - acc: 0.011 - ETA: 2s - loss: 4.8685 - acc: 0.011 - ETA: 2s - loss: 4.8680 - acc: 0.011 - ETA: 2s - loss: 4.8681 - acc: 0.011 - ETA: 2s - loss: 4.8680 - acc: 0.011 - ETA: 2s - loss: 4.8681 - acc: 0.011 - ETA: 2s - loss: 4.8679 - acc: 0.011 - ETA: 1s - loss: 4.8678 - acc: 0.011 - ETA: 1s - loss: 4.8678 - acc: 0.011 - ETA: 1s - loss: 4.8679 - acc: 0.011 - ETA: 1s - loss: 4.8676 - acc: 0.011 - ETA: 1s - loss: 4.8680 - acc: 0.011 - ETA: 1s - loss: 4.8679 - acc: 0.011 - ETA: 1s - loss: 4.8681 - acc: 0.011 - ETA: 1s - loss: 4.8681 - acc: 0.011 - ETA: 1s - loss: 4.8683 - acc: 0.012 - ETA: 1s - loss: 4.8679 - acc: 0.012 - ETA: 1s - loss: 4.8676 - acc: 0.012 - ETA: 1s - loss: 4.8678 - acc: 0.012 - ETA: 0s - loss: 4.8679 - acc: 0.012 - ETA: 0s - loss: 4.8679 - acc: 0.012 - ETA: 0s - loss: 4.8680 - acc: 0.012 - ETA: 0s - loss: 4.8679 - acc: 0.012 - ETA: 0s - loss: 4.8677 - acc: 0.012 - ETA: 0s - loss: 4.8677 - acc: 0.012 - ETA: 0s - loss: 4.8675 - acc: 0.012 - ETA: 0s - loss: 4.8674 - acc: 0.012 - ETA: 0s - loss: 4.8672 - acc: 0.012 - ETA: 0s - loss: 4.8672 - acc: 0.012 - ETA: 0s - loss: 4.8671 - acc: 0.012 - ETA: 0s - loss: 4.8670 - acc: 0.012 - ETA: 0s - loss: 4.8670 - acc: 0.0124"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.8667 - acc: 0.012 - 17s 3ms/step - loss: 4.8666 - acc: 0.0124 - val_loss: 4.8572 - val_acc: 0.0156\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.87001 to 4.85720, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6680 [===========================>..] - ETA: 14s - loss: 4.8001 - acc: 0.05 - ETA: 15s - loss: 4.8554 - acc: 0.02 - ETA: 15s - loss: 4.8764 - acc: 0.01 - ETA: 16s - loss: 4.8341 - acc: 0.02 - ETA: 16s - loss: 4.8143 - acc: 0.02 - ETA: 16s - loss: 4.8176 - acc: 0.02 - ETA: 16s - loss: 4.8318 - acc: 0.02 - ETA: 16s - loss: 4.8292 - acc: 0.01 - ETA: 15s - loss: 4.8271 - acc: 0.01 - ETA: 15s - loss: 4.8333 - acc: 0.01 - ETA: 15s - loss: 4.8345 - acc: 0.01 - ETA: 15s - loss: 4.8364 - acc: 0.01 - ETA: 15s - loss: 4.8376 - acc: 0.00 - ETA: 15s - loss: 4.8397 - acc: 0.01 - ETA: 15s - loss: 4.8433 - acc: 0.01 - ETA: 15s - loss: 4.8459 - acc: 0.01 - ETA: 15s - loss: 4.8414 - acc: 0.01 - ETA: 15s - loss: 4.8471 - acc: 0.00 - ETA: 15s - loss: 4.8476 - acc: 0.00 - ETA: 15s - loss: 4.8494 - acc: 0.01 - ETA: 15s - loss: 4.8485 - acc: 0.01 - ETA: 15s - loss: 4.8456 - acc: 0.00 - ETA: 15s - loss: 4.8426 - acc: 0.00 - ETA: 14s - loss: 4.8422 - acc: 0.00 - ETA: 14s - loss: 4.8456 - acc: 0.00 - ETA: 14s - loss: 4.8409 - acc: 0.00 - ETA: 14s - loss: 4.8431 - acc: 0.00 - ETA: 14s - loss: 4.8447 - acc: 0.00 - ETA: 14s - loss: 4.8458 - acc: 0.00 - ETA: 14s - loss: 4.8467 - acc: 0.01 - ETA: 14s - loss: 4.8458 - acc: 0.01 - ETA: 14s - loss: 4.8426 - acc: 0.01 - ETA: 14s - loss: 4.8394 - acc: 0.01 - ETA: 14s - loss: 4.8404 - acc: 0.01 - ETA: 14s - loss: 4.8454 - acc: 0.01 - ETA: 14s - loss: 4.8445 - acc: 0.01 - ETA: 13s - loss: 4.8454 - acc: 0.01 - ETA: 13s - loss: 4.8455 - acc: 0.01 - ETA: 13s - loss: 4.8455 - acc: 0.01 - ETA: 13s - loss: 4.8448 - acc: 0.01 - ETA: 13s - loss: 4.8462 - acc: 0.01 - ETA: 13s - loss: 4.8468 - acc: 0.01 - ETA: 13s - loss: 4.8454 - acc: 0.01 - ETA: 13s - loss: 4.8449 - acc: 0.01 - ETA: 13s - loss: 4.8442 - acc: 0.01 - ETA: 13s - loss: 4.8436 - acc: 0.01 - ETA: 13s - loss: 4.8427 - acc: 0.01 - ETA: 13s - loss: 4.8443 - acc: 0.01 - ETA: 12s - loss: 4.8440 - acc: 0.01 - ETA: 12s - loss: 4.8460 - acc: 0.01 - ETA: 12s - loss: 4.8451 - acc: 0.01 - ETA: 12s - loss: 4.8453 - acc: 0.01 - ETA: 12s - loss: 4.8464 - acc: 0.01 - ETA: 12s - loss: 4.8450 - acc: 0.01 - ETA: 12s - loss: 4.8445 - acc: 0.01 - ETA: 12s - loss: 4.8454 - acc: 0.01 - ETA: 12s - loss: 4.8452 - acc: 0.01 - ETA: 12s - loss: 4.8447 - acc: 0.01 - ETA: 12s - loss: 4.8463 - acc: 0.01 - ETA: 12s - loss: 4.8467 - acc: 0.01 - ETA: 12s - loss: 4.8482 - acc: 0.01 - ETA: 11s - loss: 4.8483 - acc: 0.01 - ETA: 11s - loss: 4.8473 - acc: 0.01 - ETA: 11s - loss: 4.8455 - acc: 0.01 - ETA: 11s - loss: 4.8453 - acc: 0.01 - ETA: 11s - loss: 4.8461 - acc: 0.01 - ETA: 11s - loss: 4.8461 - acc: 0.01 - ETA: 11s - loss: 4.8455 - acc: 0.01 - ETA: 11s - loss: 4.8456 - acc: 0.01 - ETA: 11s - loss: 4.8450 - acc: 0.01 - ETA: 11s - loss: 4.8454 - acc: 0.01 - ETA: 11s - loss: 4.8447 - acc: 0.01 - ETA: 11s - loss: 4.8448 - acc: 0.01 - ETA: 10s - loss: 4.8449 - acc: 0.01 - ETA: 10s - loss: 4.8448 - acc: 0.01 - ETA: 10s - loss: 4.8443 - acc: 0.01 - ETA: 10s - loss: 4.8444 - acc: 0.01 - ETA: 10s - loss: 4.8448 - acc: 0.01 - ETA: 10s - loss: 4.8457 - acc: 0.01 - ETA: 10s - loss: 4.8462 - acc: 0.01 - ETA: 10s - loss: 4.8470 - acc: 0.01 - ETA: 10s - loss: 4.8455 - acc: 0.01 - ETA: 10s - loss: 4.8467 - acc: 0.01 - ETA: 10s - loss: 4.8466 - acc: 0.01 - ETA: 10s - loss: 4.8469 - acc: 0.01 - ETA: 10s - loss: 4.8473 - acc: 0.01 - ETA: 10s - loss: 4.8473 - acc: 0.01 - ETA: 9s - loss: 4.8473 - acc: 0.0120 - ETA: 9s - loss: 4.8473 - acc: 0.012 - ETA: 9s - loss: 4.8473 - acc: 0.012 - ETA: 9s - loss: 4.8478 - acc: 0.012 - ETA: 9s - loss: 4.8474 - acc: 0.012 - ETA: 9s - loss: 4.8478 - acc: 0.012 - ETA: 9s - loss: 4.8474 - acc: 0.011 - ETA: 9s - loss: 4.8474 - acc: 0.011 - ETA: 9s - loss: 4.8474 - acc: 0.012 - ETA: 9s - loss: 4.8471 - acc: 0.012 - ETA: 9s - loss: 4.8474 - acc: 0.012 - ETA: 9s - loss: 4.8477 - acc: 0.012 - ETA: 9s - loss: 4.8478 - acc: 0.012 - ETA: 9s - loss: 4.8475 - acc: 0.012 - ETA: 9s - loss: 4.8476 - acc: 0.012 - ETA: 8s - loss: 4.8474 - acc: 0.012 - ETA: 8s - loss: 4.8476 - acc: 0.012 - ETA: 8s - loss: 4.8472 - acc: 0.012 - ETA: 8s - loss: 4.8467 - acc: 0.013 - ETA: 8s - loss: 4.8468 - acc: 0.013 - ETA: 8s - loss: 4.8470 - acc: 0.012 - ETA: 8s - loss: 4.8471 - acc: 0.012 - ETA: 8s - loss: 4.8466 - acc: 0.012 - ETA: 8s - loss: 4.8476 - acc: 0.012 - ETA: 8s - loss: 4.8482 - acc: 0.012 - ETA: 8s - loss: 4.8481 - acc: 0.012 - ETA: 8s - loss: 4.8481 - acc: 0.012 - ETA: 8s - loss: 4.8483 - acc: 0.012 - ETA: 7s - loss: 4.8478 - acc: 0.012 - ETA: 7s - loss: 4.8484 - acc: 0.012 - ETA: 7s - loss: 4.8484 - acc: 0.012 - ETA: 7s - loss: 4.8484 - acc: 0.012 - ETA: 7s - loss: 4.8485 - acc: 0.013 - ETA: 7s - loss: 4.8481 - acc: 0.013 - ETA: 7s - loss: 4.8482 - acc: 0.013 - ETA: 7s - loss: 4.8479 - acc: 0.014 - ETA: 7s - loss: 4.8479 - acc: 0.013 - ETA: 7s - loss: 4.8473 - acc: 0.014 - ETA: 7s - loss: 4.8476 - acc: 0.014 - ETA: 7s - loss: 4.8471 - acc: 0.014 - ETA: 6s - loss: 4.8476 - acc: 0.014 - ETA: 6s - loss: 4.8473 - acc: 0.014 - ETA: 6s - loss: 4.8470 - acc: 0.014 - ETA: 6s - loss: 4.8473 - acc: 0.014 - ETA: 6s - loss: 4.8468 - acc: 0.014 - ETA: 6s - loss: 4.8467 - acc: 0.014 - ETA: 6s - loss: 4.8474 - acc: 0.014 - ETA: 6s - loss: 4.8475 - acc: 0.014 - ETA: 6s - loss: 4.8478 - acc: 0.014 - ETA: 6s - loss: 4.8482 - acc: 0.014 - ETA: 6s - loss: 4.8477 - acc: 0.014 - ETA: 6s - loss: 4.8477 - acc: 0.014 - ETA: 6s - loss: 4.8479 - acc: 0.014 - ETA: 5s - loss: 4.8475 - acc: 0.014 - ETA: 5s - loss: 4.8467 - acc: 0.015 - ETA: 5s - loss: 4.8476 - acc: 0.015 - ETA: 5s - loss: 4.8474 - acc: 0.015 - ETA: 5s - loss: 4.8472 - acc: 0.015 - ETA: 5s - loss: 4.8467 - acc: 0.015 - ETA: 5s - loss: 4.8463 - acc: 0.014 - ETA: 5s - loss: 4.8453 - acc: 0.014 - ETA: 5s - loss: 4.8449 - acc: 0.014 - ETA: 5s - loss: 4.8443 - acc: 0.014 - ETA: 5s - loss: 4.8445 - acc: 0.014 - ETA: 5s - loss: 4.8445 - acc: 0.014 - ETA: 5s - loss: 4.8438 - acc: 0.014 - ETA: 5s - loss: 4.8434 - acc: 0.014 - ETA: 4s - loss: 4.8435 - acc: 0.015 - ETA: 4s - loss: 4.8434 - acc: 0.014 - ETA: 4s - loss: 4.8435 - acc: 0.014 - ETA: 4s - loss: 4.8428 - acc: 0.015 - ETA: 4s - loss: 4.8428 - acc: 0.015 - ETA: 4s - loss: 4.8430 - acc: 0.015 - ETA: 4s - loss: 4.8427 - acc: 0.015 - ETA: 4s - loss: 4.8427 - acc: 0.015 - ETA: 4s - loss: 4.8425 - acc: 0.015 - ETA: 4s - loss: 4.8427 - acc: 0.015 - ETA: 4s - loss: 4.8419 - acc: 0.015 - ETA: 4s - loss: 4.8417 - acc: 0.015 - ETA: 3s - loss: 4.8419 - acc: 0.015 - ETA: 3s - loss: 4.8413 - acc: 0.016 - ETA: 3s - loss: 4.8410 - acc: 0.016 - ETA: 3s - loss: 4.8404 - acc: 0.016 - ETA: 3s - loss: 4.8409 - acc: 0.016 - ETA: 3s - loss: 4.8409 - acc: 0.016 - ETA: 3s - loss: 4.8403 - acc: 0.016 - ETA: 3s - loss: 4.8403 - acc: 0.016 - ETA: 3s - loss: 4.8401 - acc: 0.016 - ETA: 3s - loss: 4.8401 - acc: 0.016 - ETA: 3s - loss: 4.8401 - acc: 0.016 - ETA: 3s - loss: 4.8399 - acc: 0.016 - ETA: 3s - loss: 4.8397 - acc: 0.016 - ETA: 3s - loss: 4.8393 - acc: 0.016 - ETA: 3s - loss: 4.8394 - acc: 0.016 - ETA: 2s - loss: 4.8397 - acc: 0.016 - ETA: 2s - loss: 4.8403 - acc: 0.016 - ETA: 2s - loss: 4.8397 - acc: 0.016 - ETA: 2s - loss: 4.8395 - acc: 0.016 - ETA: 2s - loss: 4.8397 - acc: 0.016 - ETA: 2s - loss: 4.8395 - acc: 0.016 - ETA: 2s - loss: 4.8391 - acc: 0.016 - ETA: 2s - loss: 4.8393 - acc: 0.016 - ETA: 2s - loss: 4.8395 - acc: 0.016 - ETA: 2s - loss: 4.8396 - acc: 0.016 - ETA: 2s - loss: 4.8398 - acc: 0.016 - ETA: 2s - loss: 4.8398 - acc: 0.016 - ETA: 2s - loss: 4.8400 - acc: 0.016 - ETA: 2s - loss: 4.8399 - acc: 0.016 - ETA: 2s - loss: 4.8398 - acc: 0.016 - ETA: 2s - loss: 4.8398 - acc: 0.016 - ETA: 1s - loss: 4.8402 - acc: 0.016 - ETA: 1s - loss: 4.8402 - acc: 0.016 - ETA: 1s - loss: 4.8405 - acc: 0.016 - ETA: 1s - loss: 4.8404 - acc: 0.016 - ETA: 1s - loss: 4.8407 - acc: 0.016 - ETA: 1s - loss: 4.8411 - acc: 0.016 - ETA: 1s - loss: 4.8406 - acc: 0.016 - ETA: 1s - loss: 4.8410 - acc: 0.016 - ETA: 1s - loss: 4.8410 - acc: 0.016 - ETA: 1s - loss: 4.8409 - acc: 0.016 - ETA: 1s - loss: 4.8409 - acc: 0.016 - ETA: 1s - loss: 4.8410 - acc: 0.016 - ETA: 0s - loss: 4.8404 - acc: 0.017 - ETA: 0s - loss: 4.8393 - acc: 0.017 - ETA: 0s - loss: 4.8391 - acc: 0.017 - ETA: 0s - loss: 4.8391 - acc: 0.017 - ETA: 0s - loss: 4.8395 - acc: 0.017 - ETA: 0s - loss: 4.8397 - acc: 0.0175"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.8394 - acc: 0.017 - ETA: 0s - loss: 4.8391 - acc: 0.017 - ETA: 0s - loss: 4.8386 - acc: 0.017 - ETA: 0s - loss: 4.8383 - acc: 0.017 - ETA: 0s - loss: 4.8378 - acc: 0.017 - ETA: 0s - loss: 4.8375 - acc: 0.017 - ETA: 0s - loss: 4.8379 - acc: 0.017 - 17s 3ms/step - loss: 4.8379 - acc: 0.0172 - val_loss: 4.8315 - val_acc: 0.0192\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.85720 to 4.83148, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 15s - loss: 4.7003 - acc: 0.10 - ETA: 15s - loss: 4.7743 - acc: 0.05 - ETA: 15s - loss: 4.7579 - acc: 0.04 - ETA: 15s - loss: 4.7711 - acc: 0.03 - ETA: 15s - loss: 4.7987 - acc: 0.03 - ETA: 15s - loss: 4.7962 - acc: 0.02 - ETA: 15s - loss: 4.8080 - acc: 0.02 - ETA: 15s - loss: 4.8114 - acc: 0.02 - ETA: 15s - loss: 4.8128 - acc: 0.02 - ETA: 15s - loss: 4.8121 - acc: 0.02 - ETA: 14s - loss: 4.8121 - acc: 0.02 - ETA: 14s - loss: 4.8036 - acc: 0.02 - ETA: 14s - loss: 4.7997 - acc: 0.02 - ETA: 14s - loss: 4.8041 - acc: 0.02 - ETA: 14s - loss: 4.8064 - acc: 0.01 - ETA: 14s - loss: 4.8088 - acc: 0.02 - ETA: 14s - loss: 4.8052 - acc: 0.02 - ETA: 14s - loss: 4.8053 - acc: 0.02 - ETA: 14s - loss: 4.8052 - acc: 0.01 - ETA: 14s - loss: 4.8011 - acc: 0.01 - ETA: 14s - loss: 4.8043 - acc: 0.01 - ETA: 14s - loss: 4.8083 - acc: 0.01 - ETA: 14s - loss: 4.8060 - acc: 0.01 - ETA: 14s - loss: 4.8008 - acc: 0.01 - ETA: 14s - loss: 4.7951 - acc: 0.01 - ETA: 14s - loss: 4.7951 - acc: 0.01 - ETA: 14s - loss: 4.7972 - acc: 0.01 - ETA: 13s - loss: 4.8018 - acc: 0.01 - ETA: 13s - loss: 4.8061 - acc: 0.01 - ETA: 13s - loss: 4.8058 - acc: 0.01 - ETA: 13s - loss: 4.8066 - acc: 0.01 - ETA: 13s - loss: 4.8092 - acc: 0.01 - ETA: 13s - loss: 4.8079 - acc: 0.01 - ETA: 13s - loss: 4.8100 - acc: 0.01 - ETA: 13s - loss: 4.8092 - acc: 0.01 - ETA: 13s - loss: 4.8074 - acc: 0.01 - ETA: 13s - loss: 4.8036 - acc: 0.02 - ETA: 13s - loss: 4.8029 - acc: 0.02 - ETA: 13s - loss: 4.8030 - acc: 0.02 - ETA: 13s - loss: 4.8026 - acc: 0.02 - ETA: 12s - loss: 4.8075 - acc: 0.02 - ETA: 12s - loss: 4.8061 - acc: 0.01 - ETA: 12s - loss: 4.8050 - acc: 0.02 - ETA: 12s - loss: 4.8058 - acc: 0.01 - ETA: 12s - loss: 4.8045 - acc: 0.02 - ETA: 12s - loss: 4.8042 - acc: 0.02 - ETA: 12s - loss: 4.8040 - acc: 0.02 - ETA: 12s - loss: 4.8029 - acc: 0.01 - ETA: 12s - loss: 4.8004 - acc: 0.01 - ETA: 12s - loss: 4.8011 - acc: 0.01 - ETA: 12s - loss: 4.7997 - acc: 0.01 - ETA: 12s - loss: 4.8011 - acc: 0.01 - ETA: 12s - loss: 4.8018 - acc: 0.02 - ETA: 12s - loss: 4.8006 - acc: 0.02 - ETA: 11s - loss: 4.7980 - acc: 0.02 - ETA: 11s - loss: 4.7951 - acc: 0.02 - ETA: 11s - loss: 4.7951 - acc: 0.02 - ETA: 11s - loss: 4.7949 - acc: 0.02 - ETA: 11s - loss: 4.7948 - acc: 0.02 - ETA: 11s - loss: 4.7920 - acc: 0.02 - ETA: 11s - loss: 4.7910 - acc: 0.02 - ETA: 11s - loss: 4.7909 - acc: 0.01 - ETA: 11s - loss: 4.7919 - acc: 0.01 - ETA: 11s - loss: 4.7933 - acc: 0.01 - ETA: 11s - loss: 4.7932 - acc: 0.01 - ETA: 11s - loss: 4.7938 - acc: 0.01 - ETA: 10s - loss: 4.7937 - acc: 0.01 - ETA: 10s - loss: 4.7957 - acc: 0.01 - ETA: 10s - loss: 4.7958 - acc: 0.01 - ETA: 10s - loss: 4.7969 - acc: 0.01 - ETA: 10s - loss: 4.7965 - acc: 0.01 - ETA: 10s - loss: 4.7974 - acc: 0.01 - ETA: 10s - loss: 4.7967 - acc: 0.01 - ETA: 10s - loss: 4.7962 - acc: 0.01 - ETA: 10s - loss: 4.7936 - acc: 0.01 - ETA: 10s - loss: 4.7966 - acc: 0.01 - ETA: 10s - loss: 4.7970 - acc: 0.01 - ETA: 10s - loss: 4.7957 - acc: 0.01 - ETA: 10s - loss: 4.7958 - acc: 0.01 - ETA: 10s - loss: 4.7952 - acc: 0.01 - ETA: 10s - loss: 4.7955 - acc: 0.01 - ETA: 10s - loss: 4.7959 - acc: 0.01 - ETA: 10s - loss: 4.7956 - acc: 0.01 - ETA: 9s - loss: 4.7962 - acc: 0.0196 - ETA: 9s - loss: 4.7955 - acc: 0.019 - ETA: 9s - loss: 4.7957 - acc: 0.019 - ETA: 9s - loss: 4.7953 - acc: 0.019 - ETA: 9s - loss: 4.7950 - acc: 0.019 - ETA: 9s - loss: 4.7965 - acc: 0.019 - ETA: 9s - loss: 4.7967 - acc: 0.019 - ETA: 9s - loss: 4.7967 - acc: 0.019 - ETA: 9s - loss: 4.7959 - acc: 0.019 - ETA: 9s - loss: 4.7957 - acc: 0.019 - ETA: 9s - loss: 4.7944 - acc: 0.019 - ETA: 9s - loss: 4.7939 - acc: 0.019 - ETA: 9s - loss: 4.7938 - acc: 0.019 - ETA: 9s - loss: 4.7908 - acc: 0.020 - ETA: 8s - loss: 4.7905 - acc: 0.020 - ETA: 8s - loss: 4.7900 - acc: 0.019 - ETA: 8s - loss: 4.7896 - acc: 0.020 - ETA: 8s - loss: 4.7899 - acc: 0.020 - ETA: 8s - loss: 4.7888 - acc: 0.020 - ETA: 8s - loss: 4.7884 - acc: 0.020 - ETA: 8s - loss: 4.7892 - acc: 0.020 - ETA: 8s - loss: 4.7904 - acc: 0.020 - ETA: 8s - loss: 4.7898 - acc: 0.020 - ETA: 8s - loss: 4.7911 - acc: 0.020 - ETA: 8s - loss: 4.7902 - acc: 0.020 - ETA: 8s - loss: 4.7900 - acc: 0.021 - ETA: 7s - loss: 4.7917 - acc: 0.021 - ETA: 7s - loss: 4.7916 - acc: 0.021 - ETA: 7s - loss: 4.7917 - acc: 0.021 - ETA: 7s - loss: 4.7912 - acc: 0.022 - ETA: 7s - loss: 4.7911 - acc: 0.022 - ETA: 7s - loss: 4.7916 - acc: 0.021 - ETA: 7s - loss: 4.7930 - acc: 0.021 - ETA: 7s - loss: 4.7929 - acc: 0.021 - ETA: 7s - loss: 4.7933 - acc: 0.021 - ETA: 7s - loss: 4.7932 - acc: 0.021 - ETA: 7s - loss: 4.7934 - acc: 0.021 - ETA: 7s - loss: 4.7917 - acc: 0.021 - ETA: 6s - loss: 4.7908 - acc: 0.021 - ETA: 6s - loss: 4.7900 - acc: 0.021 - ETA: 6s - loss: 4.7901 - acc: 0.021 - ETA: 6s - loss: 4.7892 - acc: 0.021 - ETA: 6s - loss: 4.7902 - acc: 0.021 - ETA: 6s - loss: 4.7917 - acc: 0.021 - ETA: 6s - loss: 4.7917 - acc: 0.021 - ETA: 6s - loss: 4.7924 - acc: 0.021 - ETA: 6s - loss: 4.7922 - acc: 0.021 - ETA: 6s - loss: 4.7916 - acc: 0.021 - ETA: 6s - loss: 4.7916 - acc: 0.021 - ETA: 6s - loss: 4.7919 - acc: 0.021 - ETA: 5s - loss: 4.7923 - acc: 0.021 - ETA: 5s - loss: 4.7913 - acc: 0.021 - ETA: 5s - loss: 4.7910 - acc: 0.021 - ETA: 5s - loss: 4.7909 - acc: 0.021 - ETA: 5s - loss: 4.7902 - acc: 0.021 - ETA: 5s - loss: 4.7902 - acc: 0.021 - ETA: 5s - loss: 4.7906 - acc: 0.021 - ETA: 5s - loss: 4.7896 - acc: 0.021 - ETA: 5s - loss: 4.7891 - acc: 0.021 - ETA: 5s - loss: 4.7895 - acc: 0.021 - ETA: 5s - loss: 4.7894 - acc: 0.022 - ETA: 5s - loss: 4.7896 - acc: 0.022 - ETA: 5s - loss: 4.7887 - acc: 0.022 - ETA: 5s - loss: 4.7889 - acc: 0.022 - ETA: 4s - loss: 4.7896 - acc: 0.022 - ETA: 4s - loss: 4.7899 - acc: 0.022 - ETA: 4s - loss: 4.7894 - acc: 0.022 - ETA: 4s - loss: 4.7889 - acc: 0.022 - ETA: 4s - loss: 4.7886 - acc: 0.022 - ETA: 4s - loss: 4.7890 - acc: 0.022 - ETA: 4s - loss: 4.7891 - acc: 0.022 - ETA: 4s - loss: 4.7895 - acc: 0.022 - ETA: 4s - loss: 4.7898 - acc: 0.021 - ETA: 4s - loss: 4.7894 - acc: 0.021 - ETA: 4s - loss: 4.7900 - acc: 0.021 - ETA: 4s - loss: 4.7900 - acc: 0.021 - ETA: 4s - loss: 4.7895 - acc: 0.021 - ETA: 3s - loss: 4.7897 - acc: 0.021 - ETA: 3s - loss: 4.7885 - acc: 0.021 - ETA: 3s - loss: 4.7892 - acc: 0.021 - ETA: 3s - loss: 4.7889 - acc: 0.021 - ETA: 3s - loss: 4.7894 - acc: 0.021 - ETA: 3s - loss: 4.7895 - acc: 0.021 - ETA: 3s - loss: 4.7886 - acc: 0.021 - ETA: 3s - loss: 4.7885 - acc: 0.022 - ETA: 3s - loss: 4.7888 - acc: 0.022 - ETA: 3s - loss: 4.7890 - acc: 0.021 - ETA: 3s - loss: 4.7892 - acc: 0.021 - ETA: 3s - loss: 4.7888 - acc: 0.021 - ETA: 2s - loss: 4.7892 - acc: 0.021 - ETA: 2s - loss: 4.7888 - acc: 0.021 - ETA: 2s - loss: 4.7881 - acc: 0.021 - ETA: 2s - loss: 4.7881 - acc: 0.021 - ETA: 2s - loss: 4.7882 - acc: 0.021 - ETA: 2s - loss: 4.7879 - acc: 0.021 - ETA: 2s - loss: 4.7883 - acc: 0.021 - ETA: 2s - loss: 4.7881 - acc: 0.021 - ETA: 2s - loss: 4.7881 - acc: 0.021 - ETA: 2s - loss: 4.7889 - acc: 0.020 - ETA: 2s - loss: 4.7888 - acc: 0.020 - ETA: 2s - loss: 4.7884 - acc: 0.021 - ETA: 1s - loss: 4.7884 - acc: 0.021 - ETA: 1s - loss: 4.7883 - acc: 0.020 - ETA: 1s - loss: 4.7883 - acc: 0.020 - ETA: 1s - loss: 4.7879 - acc: 0.020 - ETA: 1s - loss: 4.7875 - acc: 0.020 - ETA: 1s - loss: 4.7880 - acc: 0.020 - ETA: 1s - loss: 4.7874 - acc: 0.020 - ETA: 1s - loss: 4.7869 - acc: 0.021 - ETA: 1s - loss: 4.7873 - acc: 0.020 - ETA: 1s - loss: 4.7873 - acc: 0.020 - ETA: 1s - loss: 4.7872 - acc: 0.020 - ETA: 1s - loss: 4.7874 - acc: 0.020 - ETA: 1s - loss: 4.7865 - acc: 0.020 - ETA: 0s - loss: 4.7868 - acc: 0.020 - ETA: 0s - loss: 4.7870 - acc: 0.020 - ETA: 0s - loss: 4.7872 - acc: 0.020 - ETA: 0s - loss: 4.7869 - acc: 0.020 - ETA: 0s - loss: 4.7875 - acc: 0.020 - ETA: 0s - loss: 4.7874 - acc: 0.020 - ETA: 0s - loss: 4.7874 - acc: 0.020 - ETA: 0s - loss: 4.7882 - acc: 0.020 - ETA: 0s - loss: 4.7880 - acc: 0.020 - ETA: 0s - loss: 4.7885 - acc: 0.019 - ETA: 0s - loss: 4.7883 - acc: 0.020 - ETA: 0s - loss: 4.7883 - acc: 0.020 - ETA: 0s - loss: 4.7883 - acc: 0.019 - ETA: 0s - loss: 4.7889 - acc: 0.019 - ETA: 0s - loss: 4.7890 - acc: 0.020 - 17s 3ms/step - loss: 4.7893 - acc: 0.0199 - val_loss: 4.7923 - val_acc: 0.0228\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss improved from 4.83148 to 4.79235, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 14s - loss: 4.9228 - acc: 0.0000e+ - ETA: 15s - loss: 4.8554 - acc: 0.0000e+ - ETA: 15s - loss: 4.8208 - acc: 0.0000e+ - ETA: 15s - loss: 4.7994 - acc: 0.0100   - ETA: 15s - loss: 4.7859 - acc: 0.00 - ETA: 15s - loss: 4.7815 - acc: 0.00 - ETA: 14s - loss: 4.7746 - acc: 0.00 - ETA: 14s - loss: 4.7756 - acc: 0.01 - ETA: 14s - loss: 4.7859 - acc: 0.01 - ETA: 14s - loss: 4.7773 - acc: 0.01 - ETA: 14s - loss: 4.7718 - acc: 0.01 - ETA: 14s - loss: 4.7644 - acc: 0.01 - ETA: 14s - loss: 4.7525 - acc: 0.01 - ETA: 14s - loss: 4.7601 - acc: 0.01 - ETA: 14s - loss: 4.7630 - acc: 0.01 - ETA: 14s - loss: 4.7637 - acc: 0.01 - ETA: 14s - loss: 4.7613 - acc: 0.02 - ETA: 14s - loss: 4.7585 - acc: 0.02 - ETA: 14s - loss: 4.7468 - acc: 0.01 - ETA: 14s - loss: 4.7478 - acc: 0.01 - ETA: 14s - loss: 4.7449 - acc: 0.01 - ETA: 14s - loss: 4.7482 - acc: 0.01 - ETA: 14s - loss: 4.7410 - acc: 0.02 - ETA: 14s - loss: 4.7378 - acc: 0.02 - ETA: 14s - loss: 4.7433 - acc: 0.02 - ETA: 14s - loss: 4.7456 - acc: 0.02 - ETA: 14s - loss: 4.7462 - acc: 0.02 - ETA: 14s - loss: 4.7436 - acc: 0.02 - ETA: 13s - loss: 4.7416 - acc: 0.02 - ETA: 13s - loss: 4.7432 - acc: 0.02 - ETA: 13s - loss: 4.7473 - acc: 0.02 - ETA: 13s - loss: 4.7463 - acc: 0.02 - ETA: 13s - loss: 4.7493 - acc: 0.02 - ETA: 13s - loss: 4.7516 - acc: 0.02 - ETA: 13s - loss: 4.7550 - acc: 0.02 - ETA: 13s - loss: 4.7525 - acc: 0.02 - ETA: 13s - loss: 4.7524 - acc: 0.02 - ETA: 13s - loss: 4.7515 - acc: 0.02 - ETA: 13s - loss: 4.7512 - acc: 0.02 - ETA: 13s - loss: 4.7536 - acc: 0.02 - ETA: 12s - loss: 4.7529 - acc: 0.02 - ETA: 12s - loss: 4.7530 - acc: 0.02 - ETA: 12s - loss: 4.7514 - acc: 0.02 - ETA: 12s - loss: 4.7491 - acc: 0.02 - ETA: 12s - loss: 4.7540 - acc: 0.02 - ETA: 12s - loss: 4.7530 - acc: 0.02 - ETA: 12s - loss: 4.7525 - acc: 0.02 - ETA: 12s - loss: 4.7534 - acc: 0.01 - ETA: 12s - loss: 4.7585 - acc: 0.01 - ETA: 12s - loss: 4.7567 - acc: 0.01 - ETA: 12s - loss: 4.7578 - acc: 0.01 - ETA: 12s - loss: 4.7563 - acc: 0.01 - ETA: 11s - loss: 4.7560 - acc: 0.01 - ETA: 11s - loss: 4.7564 - acc: 0.01 - ETA: 11s - loss: 4.7562 - acc: 0.01 - ETA: 11s - loss: 4.7593 - acc: 0.02 - ETA: 11s - loss: 4.7576 - acc: 0.01 - ETA: 11s - loss: 4.7571 - acc: 0.01 - ETA: 11s - loss: 4.7568 - acc: 0.01 - ETA: 11s - loss: 4.7587 - acc: 0.01 - ETA: 11s - loss: 4.7584 - acc: 0.02 - ETA: 11s - loss: 4.7596 - acc: 0.01 - ETA: 11s - loss: 4.7582 - acc: 0.02 - ETA: 10s - loss: 4.7581 - acc: 0.02 - ETA: 10s - loss: 4.7593 - acc: 0.02 - ETA: 10s - loss: 4.7585 - acc: 0.02 - ETA: 10s - loss: 4.7577 - acc: 0.02 - ETA: 10s - loss: 4.7579 - acc: 0.01 - ETA: 10s - loss: 4.7585 - acc: 0.01 - ETA: 10s - loss: 4.7571 - acc: 0.01 - ETA: 10s - loss: 4.7567 - acc: 0.02 - ETA: 10s - loss: 4.7549 - acc: 0.02 - ETA: 10s - loss: 4.7565 - acc: 0.01 - ETA: 10s - loss: 4.7573 - acc: 0.01 - ETA: 10s - loss: 4.7574 - acc: 0.02 - ETA: 9s - loss: 4.7563 - acc: 0.0197 - ETA: 9s - loss: 4.7575 - acc: 0.019 - ETA: 9s - loss: 4.7583 - acc: 0.019 - ETA: 9s - loss: 4.7575 - acc: 0.019 - ETA: 9s - loss: 4.7553 - acc: 0.018 - ETA: 9s - loss: 4.7545 - acc: 0.018 - ETA: 9s - loss: 4.7540 - acc: 0.018 - ETA: 9s - loss: 4.7561 - acc: 0.018 - ETA: 9s - loss: 4.7571 - acc: 0.019 - ETA: 9s - loss: 4.7576 - acc: 0.019 - ETA: 9s - loss: 4.7582 - acc: 0.019 - ETA: 9s - loss: 4.7571 - acc: 0.019 - ETA: 9s - loss: 4.7569 - acc: 0.019 - ETA: 8s - loss: 4.7553 - acc: 0.019 - ETA: 8s - loss: 4.7567 - acc: 0.019 - ETA: 8s - loss: 4.7570 - acc: 0.019 - ETA: 8s - loss: 4.7570 - acc: 0.019 - ETA: 8s - loss: 4.7572 - acc: 0.018 - ETA: 8s - loss: 4.7565 - acc: 0.019 - ETA: 8s - loss: 4.7568 - acc: 0.019 - ETA: 8s - loss: 4.7579 - acc: 0.019 - ETA: 8s - loss: 4.7568 - acc: 0.019 - ETA: 8s - loss: 4.7580 - acc: 0.019 - ETA: 8s - loss: 4.7572 - acc: 0.019 - ETA: 8s - loss: 4.7584 - acc: 0.019 - ETA: 8s - loss: 4.7585 - acc: 0.018 - ETA: 7s - loss: 4.7608 - acc: 0.019 - ETA: 7s - loss: 4.7600 - acc: 0.018 - ETA: 7s - loss: 4.7595 - acc: 0.019 - ETA: 7s - loss: 4.7588 - acc: 0.019 - ETA: 7s - loss: 4.7594 - acc: 0.019 - ETA: 7s - loss: 4.7596 - acc: 0.019 - ETA: 7s - loss: 4.7591 - acc: 0.019 - ETA: 7s - loss: 4.7583 - acc: 0.019 - ETA: 7s - loss: 4.7585 - acc: 0.019 - ETA: 7s - loss: 4.7587 - acc: 0.019 - ETA: 7s - loss: 4.7566 - acc: 0.019 - ETA: 7s - loss: 4.7572 - acc: 0.019 - ETA: 7s - loss: 4.7573 - acc: 0.019 - ETA: 7s - loss: 4.7575 - acc: 0.019 - ETA: 7s - loss: 4.7578 - acc: 0.019 - ETA: 6s - loss: 4.7581 - acc: 0.019 - ETA: 6s - loss: 4.7570 - acc: 0.020 - ETA: 6s - loss: 4.7574 - acc: 0.019 - ETA: 6s - loss: 4.7564 - acc: 0.020 - ETA: 6s - loss: 4.7562 - acc: 0.020 - ETA: 6s - loss: 4.7556 - acc: 0.020 - ETA: 6s - loss: 4.7552 - acc: 0.019 - ETA: 6s - loss: 4.7566 - acc: 0.020 - ETA: 6s - loss: 4.7564 - acc: 0.020 - ETA: 6s - loss: 4.7561 - acc: 0.021 - ETA: 6s - loss: 4.7552 - acc: 0.021 - ETA: 6s - loss: 4.7555 - acc: 0.020 - ETA: 6s - loss: 4.7560 - acc: 0.020 - ETA: 6s - loss: 4.7561 - acc: 0.020 - ETA: 6s - loss: 4.7571 - acc: 0.020 - ETA: 5s - loss: 4.7576 - acc: 0.020 - ETA: 5s - loss: 4.7573 - acc: 0.020 - ETA: 5s - loss: 4.7576 - acc: 0.019 - ETA: 5s - loss: 4.7581 - acc: 0.020 - ETA: 5s - loss: 4.7577 - acc: 0.020 - ETA: 5s - loss: 4.7571 - acc: 0.020 - ETA: 5s - loss: 4.7569 - acc: 0.020 - ETA: 5s - loss: 4.7586 - acc: 0.020 - ETA: 5s - loss: 4.7582 - acc: 0.020 - ETA: 5s - loss: 4.7585 - acc: 0.020 - ETA: 5s - loss: 4.7570 - acc: 0.020 - ETA: 5s - loss: 4.7559 - acc: 0.020 - ETA: 4s - loss: 4.7548 - acc: 0.020 - ETA: 4s - loss: 4.7538 - acc: 0.020 - ETA: 4s - loss: 4.7539 - acc: 0.020 - ETA: 4s - loss: 4.7541 - acc: 0.020 - ETA: 4s - loss: 4.7537 - acc: 0.020 - ETA: 4s - loss: 4.7540 - acc: 0.020 - ETA: 4s - loss: 4.7550 - acc: 0.020 - ETA: 4s - loss: 4.7552 - acc: 0.020 - ETA: 4s - loss: 4.7560 - acc: 0.020 - ETA: 4s - loss: 4.7560 - acc: 0.020 - ETA: 4s - loss: 4.7557 - acc: 0.020 - ETA: 4s - loss: 4.7564 - acc: 0.019 - ETA: 4s - loss: 4.7557 - acc: 0.020 - ETA: 3s - loss: 4.7556 - acc: 0.020 - ETA: 3s - loss: 4.7564 - acc: 0.020 - ETA: 3s - loss: 4.7568 - acc: 0.020 - ETA: 3s - loss: 4.7572 - acc: 0.020 - ETA: 3s - loss: 4.7565 - acc: 0.020 - ETA: 3s - loss: 4.7570 - acc: 0.020 - ETA: 3s - loss: 4.7576 - acc: 0.020 - ETA: 3s - loss: 4.7575 - acc: 0.020 - ETA: 3s - loss: 4.7579 - acc: 0.020 - ETA: 3s - loss: 4.7582 - acc: 0.020 - ETA: 3s - loss: 4.7576 - acc: 0.020 - ETA: 3s - loss: 4.7577 - acc: 0.020 - ETA: 3s - loss: 4.7583 - acc: 0.020 - ETA: 2s - loss: 4.7578 - acc: 0.020 - ETA: 2s - loss: 4.7574 - acc: 0.020 - ETA: 2s - loss: 4.7571 - acc: 0.020 - ETA: 2s - loss: 4.7566 - acc: 0.020 - ETA: 2s - loss: 4.7570 - acc: 0.020 - ETA: 2s - loss: 4.7570 - acc: 0.020 - ETA: 2s - loss: 4.7563 - acc: 0.020 - ETA: 2s - loss: 4.7558 - acc: 0.020 - ETA: 2s - loss: 4.7555 - acc: 0.020 - ETA: 2s - loss: 4.7544 - acc: 0.020 - ETA: 2s - loss: 4.7542 - acc: 0.021 - ETA: 2s - loss: 4.7535 - acc: 0.021 - ETA: 2s - loss: 4.7544 - acc: 0.021 - ETA: 2s - loss: 4.7546 - acc: 0.021 - ETA: 2s - loss: 4.7549 - acc: 0.021 - ETA: 1s - loss: 4.7551 - acc: 0.020 - ETA: 1s - loss: 4.7551 - acc: 0.020 - ETA: 1s - loss: 4.7541 - acc: 0.021 - ETA: 1s - loss: 4.7540 - acc: 0.021 - ETA: 1s - loss: 4.7538 - acc: 0.021 - ETA: 1s - loss: 4.7537 - acc: 0.021 - ETA: 1s - loss: 4.7536 - acc: 0.021 - ETA: 1s - loss: 4.7539 - acc: 0.021 - ETA: 1s - loss: 4.7527 - acc: 0.021 - ETA: 1s - loss: 4.7528 - acc: 0.021 - ETA: 1s - loss: 4.7539 - acc: 0.021 - ETA: 1s - loss: 4.7534 - acc: 0.021 - ETA: 0s - loss: 4.7534 - acc: 0.021 - ETA: 0s - loss: 4.7533 - acc: 0.021 - ETA: 0s - loss: 4.7538 - acc: 0.021 - ETA: 0s - loss: 4.7543 - acc: 0.021 - ETA: 0s - loss: 4.7548 - acc: 0.021 - ETA: 0s - loss: 4.7547 - acc: 0.021 - ETA: 0s - loss: 4.7544 - acc: 0.021 - ETA: 0s - loss: 4.7545 - acc: 0.021 - ETA: 0s - loss: 4.7545 - acc: 0.021 - ETA: 0s - loss: 4.7542 - acc: 0.021 - ETA: 0s - loss: 4.7536 - acc: 0.021 - ETA: 0s - loss: 4.7536 - acc: 0.021 - 17s 3ms/step - loss: 4.7539 - acc: 0.0213 - val_loss: 4.7671 - val_acc: 0.0251\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss improved from 4.79235 to 4.76707, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 14s - loss: 4.8411 - acc: 0.0000e+ - ETA: 15s - loss: 4.7368 - acc: 0.0333   - ETA: 15s - loss: 4.7712 - acc: 0.03 - ETA: 15s - loss: 4.7384 - acc: 0.02 - ETA: 14s - loss: 4.7487 - acc: 0.02 - ETA: 14s - loss: 4.7528 - acc: 0.02 - ETA: 14s - loss: 4.7569 - acc: 0.01 - ETA: 14s - loss: 4.7322 - acc: 0.01 - ETA: 14s - loss: 4.7280 - acc: 0.01 - ETA: 14s - loss: 4.7306 - acc: 0.01 - ETA: 14s - loss: 4.7288 - acc: 0.01 - ETA: 14s - loss: 4.7352 - acc: 0.01 - ETA: 14s - loss: 4.7342 - acc: 0.01 - ETA: 14s - loss: 4.7419 - acc: 0.01 - ETA: 14s - loss: 4.7401 - acc: 0.01 - ETA: 14s - loss: 4.7432 - acc: 0.01 - ETA: 14s - loss: 4.7376 - acc: 0.01 - ETA: 14s - loss: 4.7364 - acc: 0.01 - ETA: 14s - loss: 4.7300 - acc: 0.02 - ETA: 14s - loss: 4.7323 - acc: 0.02 - ETA: 14s - loss: 4.7290 - acc: 0.02 - ETA: 14s - loss: 4.7308 - acc: 0.02 - ETA: 14s - loss: 4.7331 - acc: 0.02 - ETA: 14s - loss: 4.7290 - acc: 0.02 - ETA: 14s - loss: 4.7225 - acc: 0.02 - ETA: 14s - loss: 4.7260 - acc: 0.02 - ETA: 14s - loss: 4.7254 - acc: 0.02 - ETA: 14s - loss: 4.7267 - acc: 0.02 - ETA: 14s - loss: 4.7262 - acc: 0.02 - ETA: 13s - loss: 4.7199 - acc: 0.02 - ETA: 13s - loss: 4.7208 - acc: 0.02 - ETA: 13s - loss: 4.7246 - acc: 0.02 - ETA: 13s - loss: 4.7245 - acc: 0.02 - ETA: 13s - loss: 4.7235 - acc: 0.02 - ETA: 13s - loss: 4.7237 - acc: 0.02 - ETA: 13s - loss: 4.7238 - acc: 0.02 - ETA: 13s - loss: 4.7246 - acc: 0.02 - ETA: 13s - loss: 4.7206 - acc: 0.02 - ETA: 13s - loss: 4.7152 - acc: 0.02 - ETA: 13s - loss: 4.7193 - acc: 0.02 - ETA: 13s - loss: 4.7175 - acc: 0.02 - ETA: 13s - loss: 4.7188 - acc: 0.02 - ETA: 13s - loss: 4.7206 - acc: 0.02 - ETA: 13s - loss: 4.7196 - acc: 0.02 - ETA: 13s - loss: 4.7202 - acc: 0.02 - ETA: 13s - loss: 4.7205 - acc: 0.02 - ETA: 13s - loss: 4.7204 - acc: 0.02 - ETA: 13s - loss: 4.7213 - acc: 0.02 - ETA: 12s - loss: 4.7209 - acc: 0.02 - ETA: 12s - loss: 4.7209 - acc: 0.02 - ETA: 12s - loss: 4.7231 - acc: 0.02 - ETA: 12s - loss: 4.7224 - acc: 0.02 - ETA: 12s - loss: 4.7247 - acc: 0.02 - ETA: 12s - loss: 4.7238 - acc: 0.02 - ETA: 12s - loss: 4.7228 - acc: 0.02 - ETA: 12s - loss: 4.7218 - acc: 0.02 - ETA: 12s - loss: 4.7196 - acc: 0.02 - ETA: 12s - loss: 4.7203 - acc: 0.02 - ETA: 12s - loss: 4.7222 - acc: 0.02 - ETA: 12s - loss: 4.7207 - acc: 0.02 - ETA: 12s - loss: 4.7253 - acc: 0.02 - ETA: 12s - loss: 4.7254 - acc: 0.02 - ETA: 12s - loss: 4.7279 - acc: 0.02 - ETA: 11s - loss: 4.7288 - acc: 0.02 - ETA: 11s - loss: 4.7276 - acc: 0.02 - ETA: 11s - loss: 4.7279 - acc: 0.02 - ETA: 11s - loss: 4.7290 - acc: 0.02 - ETA: 11s - loss: 4.7292 - acc: 0.02 - ETA: 11s - loss: 4.7291 - acc: 0.02 - ETA: 11s - loss: 4.7290 - acc: 0.02 - ETA: 11s - loss: 4.7256 - acc: 0.02 - ETA: 11s - loss: 4.7271 - acc: 0.02 - ETA: 11s - loss: 4.7277 - acc: 0.02 - ETA: 11s - loss: 4.7279 - acc: 0.02 - ETA: 11s - loss: 4.7290 - acc: 0.02 - ETA: 11s - loss: 4.7281 - acc: 0.02 - ETA: 10s - loss: 4.7308 - acc: 0.02 - ETA: 10s - loss: 4.7310 - acc: 0.02 - ETA: 10s - loss: 4.7309 - acc: 0.02 - ETA: 10s - loss: 4.7306 - acc: 0.02 - ETA: 10s - loss: 4.7296 - acc: 0.02 - ETA: 10s - loss: 4.7295 - acc: 0.02 - ETA: 10s - loss: 4.7299 - acc: 0.02 - ETA: 10s - loss: 4.7296 - acc: 0.02 - ETA: 10s - loss: 4.7307 - acc: 0.02 - ETA: 10s - loss: 4.7305 - acc: 0.02 - ETA: 10s - loss: 4.7306 - acc: 0.02 - ETA: 9s - loss: 4.7302 - acc: 0.0268 - ETA: 9s - loss: 4.7322 - acc: 0.027 - ETA: 9s - loss: 4.7326 - acc: 0.026 - ETA: 9s - loss: 4.7317 - acc: 0.026 - ETA: 9s - loss: 4.7318 - acc: 0.026 - ETA: 9s - loss: 4.7329 - acc: 0.026 - ETA: 9s - loss: 4.7340 - acc: 0.026 - ETA: 9s - loss: 4.7338 - acc: 0.026 - ETA: 9s - loss: 4.7310 - acc: 0.026 - ETA: 9s - loss: 4.7320 - acc: 0.026 - ETA: 9s - loss: 4.7324 - acc: 0.027 - ETA: 9s - loss: 4.7331 - acc: 0.027 - ETA: 9s - loss: 4.7343 - acc: 0.027 - ETA: 8s - loss: 4.7352 - acc: 0.027 - ETA: 8s - loss: 4.7357 - acc: 0.026 - ETA: 8s - loss: 4.7347 - acc: 0.026 - ETA: 8s - loss: 4.7349 - acc: 0.027 - ETA: 8s - loss: 4.7330 - acc: 0.026 - ETA: 8s - loss: 4.7332 - acc: 0.027 - ETA: 8s - loss: 4.7341 - acc: 0.027 - ETA: 8s - loss: 4.7340 - acc: 0.027 - ETA: 8s - loss: 4.7358 - acc: 0.027 - ETA: 8s - loss: 4.7358 - acc: 0.026 - ETA: 8s - loss: 4.7366 - acc: 0.027 - ETA: 8s - loss: 4.7362 - acc: 0.027 - ETA: 7s - loss: 4.7339 - acc: 0.026 - ETA: 7s - loss: 4.7338 - acc: 0.026 - ETA: 7s - loss: 4.7340 - acc: 0.026 - ETA: 7s - loss: 4.7330 - acc: 0.026 - ETA: 7s - loss: 4.7328 - acc: 0.026 - ETA: 7s - loss: 4.7341 - acc: 0.026 - ETA: 7s - loss: 4.7348 - acc: 0.026 - ETA: 7s - loss: 4.7346 - acc: 0.026 - ETA: 7s - loss: 4.7355 - acc: 0.026 - ETA: 7s - loss: 4.7351 - acc: 0.026 - ETA: 7s - loss: 4.7340 - acc: 0.026 - ETA: 7s - loss: 4.7347 - acc: 0.026 - ETA: 7s - loss: 4.7348 - acc: 0.025 - ETA: 6s - loss: 4.7334 - acc: 0.025 - ETA: 6s - loss: 4.7331 - acc: 0.026 - ETA: 6s - loss: 4.7323 - acc: 0.026 - ETA: 6s - loss: 4.7335 - acc: 0.025 - ETA: 6s - loss: 4.7325 - acc: 0.026 - ETA: 6s - loss: 4.7346 - acc: 0.025 - ETA: 6s - loss: 4.7338 - acc: 0.025 - ETA: 6s - loss: 4.7345 - acc: 0.025 - ETA: 6s - loss: 4.7345 - acc: 0.025 - ETA: 6s - loss: 4.7345 - acc: 0.025 - ETA: 6s - loss: 4.7339 - acc: 0.025 - ETA: 6s - loss: 4.7331 - acc: 0.025 - ETA: 5s - loss: 4.7350 - acc: 0.025 - ETA: 5s - loss: 4.7352 - acc: 0.025 - ETA: 5s - loss: 4.7349 - acc: 0.025 - ETA: 5s - loss: 4.7354 - acc: 0.025 - ETA: 5s - loss: 4.7349 - acc: 0.026 - ETA: 5s - loss: 4.7353 - acc: 0.026 - ETA: 5s - loss: 4.7351 - acc: 0.025 - ETA: 5s - loss: 4.7348 - acc: 0.025 - ETA: 5s - loss: 4.7357 - acc: 0.025 - ETA: 5s - loss: 4.7354 - acc: 0.025 - ETA: 5s - loss: 4.7363 - acc: 0.025 - ETA: 5s - loss: 4.7361 - acc: 0.025 - ETA: 5s - loss: 4.7353 - acc: 0.025 - ETA: 4s - loss: 4.7352 - acc: 0.025 - ETA: 4s - loss: 4.7353 - acc: 0.025 - ETA: 4s - loss: 4.7351 - acc: 0.025 - ETA: 4s - loss: 4.7343 - acc: 0.025 - ETA: 4s - loss: 4.7341 - acc: 0.025 - ETA: 4s - loss: 4.7340 - acc: 0.025 - ETA: 4s - loss: 4.7337 - acc: 0.025 - ETA: 4s - loss: 4.7330 - acc: 0.025 - ETA: 4s - loss: 4.7326 - acc: 0.025 - ETA: 4s - loss: 4.7337 - acc: 0.025 - ETA: 4s - loss: 4.7342 - acc: 0.026 - ETA: 4s - loss: 4.7339 - acc: 0.026 - ETA: 3s - loss: 4.7338 - acc: 0.026 - ETA: 3s - loss: 4.7340 - acc: 0.026 - ETA: 3s - loss: 4.7342 - acc: 0.026 - ETA: 3s - loss: 4.7342 - acc: 0.026 - ETA: 3s - loss: 4.7347 - acc: 0.026 - ETA: 3s - loss: 4.7357 - acc: 0.026 - ETA: 3s - loss: 4.7355 - acc: 0.026 - ETA: 3s - loss: 4.7352 - acc: 0.026 - ETA: 3s - loss: 4.7345 - acc: 0.026 - ETA: 3s - loss: 4.7342 - acc: 0.026 - ETA: 3s - loss: 4.7342 - acc: 0.025 - ETA: 3s - loss: 4.7338 - acc: 0.026 - ETA: 2s - loss: 4.7341 - acc: 0.026 - ETA: 2s - loss: 4.7335 - acc: 0.026 - ETA: 2s - loss: 4.7331 - acc: 0.026 - ETA: 2s - loss: 4.7331 - acc: 0.026 - ETA: 2s - loss: 4.7343 - acc: 0.026 - ETA: 2s - loss: 4.7340 - acc: 0.026 - ETA: 2s - loss: 4.7336 - acc: 0.025 - ETA: 2s - loss: 4.7339 - acc: 0.025 - ETA: 2s - loss: 4.7334 - acc: 0.025 - ETA: 2s - loss: 4.7336 - acc: 0.025 - ETA: 2s - loss: 4.7340 - acc: 0.026 - ETA: 2s - loss: 4.7340 - acc: 0.026 - ETA: 2s - loss: 4.7345 - acc: 0.025 - ETA: 1s - loss: 4.7340 - acc: 0.026 - ETA: 1s - loss: 4.7337 - acc: 0.025 - ETA: 1s - loss: 4.7330 - acc: 0.025 - ETA: 1s - loss: 4.7320 - acc: 0.025 - ETA: 1s - loss: 4.7324 - acc: 0.025 - ETA: 1s - loss: 4.7311 - acc: 0.025 - ETA: 1s - loss: 4.7309 - acc: 0.025 - ETA: 1s - loss: 4.7309 - acc: 0.025 - ETA: 1s - loss: 4.7304 - acc: 0.025 - ETA: 1s - loss: 4.7306 - acc: 0.025 - ETA: 1s - loss: 4.7305 - acc: 0.025 - ETA: 1s - loss: 4.7308 - acc: 0.025 - ETA: 0s - loss: 4.7300 - acc: 0.025 - ETA: 0s - loss: 4.7299 - acc: 0.025 - ETA: 0s - loss: 4.7298 - acc: 0.025 - ETA: 0s - loss: 4.7293 - acc: 0.026 - ETA: 0s - loss: 4.7285 - acc: 0.026 - ETA: 0s - loss: 4.7284 - acc: 0.025 - ETA: 0s - loss: 4.7274 - acc: 0.025 - ETA: 0s - loss: 4.7275 - acc: 0.025 - ETA: 0s - loss: 4.7274 - acc: 0.025 - ETA: 0s - loss: 4.7267 - acc: 0.025 - ETA: 0s - loss: 4.7270 - acc: 0.025 - ETA: 0s - loss: 4.7272 - acc: 0.025 - ETA: 0s - loss: 4.7275 - acc: 0.025 - 17s 3ms/step - loss: 4.7273 - acc: 0.0254 - val_loss: 4.7542 - val_acc: 0.0287\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: val_loss improved from 4.76707 to 4.75424, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 14s - loss: 4.7512 - acc: 0.0000e+ - ETA: 15s - loss: 4.7462 - acc: 0.0500   - ETA: 15s - loss: 4.7559 - acc: 0.03 - ETA: 15s - loss: 4.7380 - acc: 0.03 - ETA: 15s - loss: 4.7277 - acc: 0.03 - ETA: 15s - loss: 4.7056 - acc: 0.03 - ETA: 14s - loss: 4.6788 - acc: 0.02 - ETA: 15s - loss: 4.6977 - acc: 0.02 - ETA: 14s - loss: 4.7135 - acc: 0.02 - ETA: 14s - loss: 4.6877 - acc: 0.02 - ETA: 14s - loss: 4.6834 - acc: 0.02 - ETA: 14s - loss: 4.6923 - acc: 0.02 - ETA: 14s - loss: 4.6880 - acc: 0.01 - ETA: 14s - loss: 4.6942 - acc: 0.02 - ETA: 14s - loss: 4.6824 - acc: 0.02 - ETA: 14s - loss: 4.6797 - acc: 0.02 - ETA: 14s - loss: 4.6647 - acc: 0.02 - ETA: 14s - loss: 4.6853 - acc: 0.02 - ETA: 14s - loss: 4.6870 - acc: 0.02 - ETA: 14s - loss: 4.6862 - acc: 0.02 - ETA: 14s - loss: 4.6901 - acc: 0.02 - ETA: 14s - loss: 4.6961 - acc: 0.02 - ETA: 13s - loss: 4.6830 - acc: 0.02 - ETA: 13s - loss: 4.6861 - acc: 0.02 - ETA: 13s - loss: 4.6888 - acc: 0.02 - ETA: 13s - loss: 4.6838 - acc: 0.02 - ETA: 13s - loss: 4.6811 - acc: 0.02 - ETA: 13s - loss: 4.6910 - acc: 0.02 - ETA: 13s - loss: 4.6853 - acc: 0.02 - ETA: 13s - loss: 4.6897 - acc: 0.02 - ETA: 13s - loss: 4.6915 - acc: 0.02 - ETA: 13s - loss: 4.6877 - acc: 0.02 - ETA: 13s - loss: 4.6894 - acc: 0.02 - ETA: 13s - loss: 4.6944 - acc: 0.03 - ETA: 12s - loss: 4.6964 - acc: 0.02 - ETA: 12s - loss: 4.6970 - acc: 0.02 - ETA: 12s - loss: 4.6950 - acc: 0.02 - ETA: 12s - loss: 4.6959 - acc: 0.03 - ETA: 12s - loss: 4.6977 - acc: 0.02 - ETA: 12s - loss: 4.7016 - acc: 0.02 - ETA: 12s - loss: 4.7042 - acc: 0.02 - ETA: 12s - loss: 4.7011 - acc: 0.02 - ETA: 12s - loss: 4.6982 - acc: 0.02 - ETA: 12s - loss: 4.7000 - acc: 0.02 - ETA: 12s - loss: 4.6980 - acc: 0.02 - ETA: 12s - loss: 4.6982 - acc: 0.02 - ETA: 12s - loss: 4.6948 - acc: 0.02 - ETA: 12s - loss: 4.6972 - acc: 0.02 - ETA: 11s - loss: 4.6967 - acc: 0.02 - ETA: 11s - loss: 4.6983 - acc: 0.02 - ETA: 11s - loss: 4.7022 - acc: 0.02 - ETA: 11s - loss: 4.6994 - acc: 0.02 - ETA: 11s - loss: 4.6972 - acc: 0.02 - ETA: 11s - loss: 4.6989 - acc: 0.02 - ETA: 11s - loss: 4.6987 - acc: 0.02 - ETA: 11s - loss: 4.6963 - acc: 0.02 - ETA: 11s - loss: 4.6973 - acc: 0.02 - ETA: 11s - loss: 4.6992 - acc: 0.02 - ETA: 11s - loss: 4.6997 - acc: 0.02 - ETA: 11s - loss: 4.6983 - acc: 0.02 - ETA: 11s - loss: 4.6996 - acc: 0.02 - ETA: 11s - loss: 4.6993 - acc: 0.02 - ETA: 11s - loss: 4.7006 - acc: 0.02 - ETA: 11s - loss: 4.7004 - acc: 0.02 - ETA: 11s - loss: 4.7038 - acc: 0.02 - ETA: 10s - loss: 4.7051 - acc: 0.02 - ETA: 10s - loss: 4.7065 - acc: 0.02 - ETA: 10s - loss: 4.7044 - acc: 0.02 - ETA: 10s - loss: 4.7048 - acc: 0.02 - ETA: 10s - loss: 4.7069 - acc: 0.02 - ETA: 10s - loss: 4.7065 - acc: 0.02 - ETA: 10s - loss: 4.7069 - acc: 0.02 - ETA: 10s - loss: 4.7067 - acc: 0.02 - ETA: 10s - loss: 4.7062 - acc: 0.02 - ETA: 10s - loss: 4.7084 - acc: 0.02 - ETA: 10s - loss: 4.7100 - acc: 0.02 - ETA: 10s - loss: 4.7099 - acc: 0.02 - ETA: 10s - loss: 4.7095 - acc: 0.02 - ETA: 10s - loss: 4.7078 - acc: 0.02 - ETA: 10s - loss: 4.7092 - acc: 0.02 - ETA: 9s - loss: 4.7079 - acc: 0.0274 - ETA: 9s - loss: 4.7085 - acc: 0.027 - ETA: 9s - loss: 4.7089 - acc: 0.027 - ETA: 9s - loss: 4.7064 - acc: 0.027 - ETA: 9s - loss: 4.7056 - acc: 0.027 - ETA: 9s - loss: 4.7063 - acc: 0.027 - ETA: 9s - loss: 4.7056 - acc: 0.028 - ETA: 9s - loss: 4.7049 - acc: 0.027 - ETA: 9s - loss: 4.7056 - acc: 0.027 - ETA: 9s - loss: 4.7070 - acc: 0.027 - ETA: 9s - loss: 4.7066 - acc: 0.028 - ETA: 9s - loss: 4.7046 - acc: 0.027 - ETA: 8s - loss: 4.7037 - acc: 0.028 - ETA: 8s - loss: 4.7044 - acc: 0.028 - ETA: 8s - loss: 4.7060 - acc: 0.028 - ETA: 8s - loss: 4.7047 - acc: 0.028 - ETA: 8s - loss: 4.7048 - acc: 0.028 - ETA: 8s - loss: 4.7048 - acc: 0.028 - ETA: 8s - loss: 4.7034 - acc: 0.028 - ETA: 8s - loss: 4.7023 - acc: 0.028 - ETA: 8s - loss: 4.7052 - acc: 0.028 - ETA: 8s - loss: 4.7047 - acc: 0.028 - ETA: 8s - loss: 4.7052 - acc: 0.028 - ETA: 8s - loss: 4.7062 - acc: 0.028 - ETA: 8s - loss: 4.7056 - acc: 0.028 - ETA: 8s - loss: 4.7058 - acc: 0.028 - ETA: 7s - loss: 4.7058 - acc: 0.028 - ETA: 7s - loss: 4.7057 - acc: 0.028 - ETA: 7s - loss: 4.7052 - acc: 0.028 - ETA: 7s - loss: 4.7034 - acc: 0.028 - ETA: 7s - loss: 4.7014 - acc: 0.028 - ETA: 7s - loss: 4.7036 - acc: 0.028 - ETA: 7s - loss: 4.7024 - acc: 0.028 - ETA: 7s - loss: 4.7014 - acc: 0.028 - ETA: 7s - loss: 4.7008 - acc: 0.028 - ETA: 7s - loss: 4.6995 - acc: 0.028 - ETA: 7s - loss: 4.7008 - acc: 0.028 - ETA: 7s - loss: 4.7011 - acc: 0.028 - ETA: 6s - loss: 4.7008 - acc: 0.028 - ETA: 6s - loss: 4.7015 - acc: 0.028 - ETA: 6s - loss: 4.7009 - acc: 0.028 - ETA: 6s - loss: 4.7004 - acc: 0.028 - ETA: 6s - loss: 4.7012 - acc: 0.028 - ETA: 6s - loss: 4.7020 - acc: 0.028 - ETA: 6s - loss: 4.7024 - acc: 0.027 - ETA: 6s - loss: 4.7021 - acc: 0.027 - ETA: 6s - loss: 4.7031 - acc: 0.027 - ETA: 6s - loss: 4.7039 - acc: 0.027 - ETA: 6s - loss: 4.7034 - acc: 0.027 - ETA: 6s - loss: 4.7034 - acc: 0.027 - ETA: 6s - loss: 4.7027 - acc: 0.027 - ETA: 6s - loss: 4.7022 - acc: 0.027 - ETA: 5s - loss: 4.7020 - acc: 0.027 - ETA: 5s - loss: 4.7029 - acc: 0.027 - ETA: 5s - loss: 4.7013 - acc: 0.027 - ETA: 5s - loss: 4.7032 - acc: 0.027 - ETA: 5s - loss: 4.7034 - acc: 0.027 - ETA: 5s - loss: 4.7026 - acc: 0.027 - ETA: 5s - loss: 4.7041 - acc: 0.028 - ETA: 5s - loss: 4.7045 - acc: 0.027 - ETA: 5s - loss: 4.7033 - acc: 0.028 - ETA: 5s - loss: 4.7045 - acc: 0.027 - ETA: 5s - loss: 4.7046 - acc: 0.028 - ETA: 4s - loss: 4.7045 - acc: 0.028 - ETA: 4s - loss: 4.7040 - acc: 0.028 - ETA: 4s - loss: 4.7029 - acc: 0.028 - ETA: 4s - loss: 4.7026 - acc: 0.028 - ETA: 4s - loss: 4.7023 - acc: 0.028 - ETA: 4s - loss: 4.7028 - acc: 0.028 - ETA: 4s - loss: 4.7031 - acc: 0.027 - ETA: 4s - loss: 4.7033 - acc: 0.027 - ETA: 4s - loss: 4.7037 - acc: 0.027 - ETA: 4s - loss: 4.7034 - acc: 0.027 - ETA: 4s - loss: 4.7032 - acc: 0.027 - ETA: 4s - loss: 4.7027 - acc: 0.027 - ETA: 4s - loss: 4.7020 - acc: 0.027 - ETA: 4s - loss: 4.7017 - acc: 0.027 - ETA: 3s - loss: 4.7023 - acc: 0.026 - ETA: 3s - loss: 4.7020 - acc: 0.026 - ETA: 3s - loss: 4.7008 - acc: 0.026 - ETA: 3s - loss: 4.6997 - acc: 0.027 - ETA: 3s - loss: 4.6998 - acc: 0.027 - ETA: 3s - loss: 4.7001 - acc: 0.027 - ETA: 3s - loss: 4.7009 - acc: 0.027 - ETA: 3s - loss: 4.7007 - acc: 0.027 - ETA: 3s - loss: 4.7013 - acc: 0.027 - ETA: 3s - loss: 4.7017 - acc: 0.027 - ETA: 3s - loss: 4.7016 - acc: 0.027 - ETA: 3s - loss: 4.7031 - acc: 0.027 - ETA: 3s - loss: 4.7035 - acc: 0.027 - ETA: 3s - loss: 4.7038 - acc: 0.027 - ETA: 2s - loss: 4.7031 - acc: 0.027 - ETA: 2s - loss: 4.7025 - acc: 0.027 - ETA: 2s - loss: 4.7017 - acc: 0.027 - ETA: 2s - loss: 4.7021 - acc: 0.027 - ETA: 2s - loss: 4.7021 - acc: 0.027 - ETA: 2s - loss: 4.7026 - acc: 0.027 - ETA: 2s - loss: 4.7029 - acc: 0.027 - ETA: 2s - loss: 4.7035 - acc: 0.027 - ETA: 2s - loss: 4.7040 - acc: 0.027 - ETA: 2s - loss: 4.7052 - acc: 0.027 - ETA: 2s - loss: 4.7056 - acc: 0.027 - ETA: 2s - loss: 4.7056 - acc: 0.027 - ETA: 2s - loss: 4.7064 - acc: 0.027 - ETA: 1s - loss: 4.7061 - acc: 0.027 - ETA: 1s - loss: 4.7067 - acc: 0.027 - ETA: 1s - loss: 4.7061 - acc: 0.027 - ETA: 1s - loss: 4.7054 - acc: 0.027 - ETA: 1s - loss: 4.7052 - acc: 0.027 - ETA: 1s - loss: 4.7038 - acc: 0.027 - ETA: 1s - loss: 4.7046 - acc: 0.027 - ETA: 1s - loss: 4.7044 - acc: 0.027 - ETA: 1s - loss: 4.7033 - acc: 0.027 - ETA: 1s - loss: 4.7031 - acc: 0.027 - ETA: 1s - loss: 4.7026 - acc: 0.027 - ETA: 1s - loss: 4.7038 - acc: 0.027 - ETA: 0s - loss: 4.7040 - acc: 0.027 - ETA: 0s - loss: 4.7048 - acc: 0.027 - ETA: 0s - loss: 4.7046 - acc: 0.027 - ETA: 0s - loss: 4.7046 - acc: 0.027 - ETA: 0s - loss: 4.7054 - acc: 0.027 - ETA: 0s - loss: 4.7047 - acc: 0.026 - ETA: 0s - loss: 4.7040 - acc: 0.027 - ETA: 0s - loss: 4.7041 - acc: 0.027 - ETA: 0s - loss: 4.7039 - acc: 0.027 - ETA: 0s - loss: 4.7042 - acc: 0.026 - ETA: 0s - loss: 4.7037 - acc: 0.027 - ETA: 0s - loss: 4.7023 - acc: 0.027 - ETA: 0s - loss: 4.7016 - acc: 0.027 - 17s 3ms/step - loss: 4.7013 - acc: 0.0272 - val_loss: 4.7402 - val_acc: 0.0216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss improved from 4.75424 to 4.74020, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 14s - loss: 4.4950 - acc: 0.05 - ETA: 14s - loss: 4.5903 - acc: 0.05 - ETA: 14s - loss: 4.6205 - acc: 0.05 - ETA: 14s - loss: 4.6010 - acc: 0.05 - ETA: 14s - loss: 4.6207 - acc: 0.05 - ETA: 14s - loss: 4.6315 - acc: 0.05 - ETA: 14s - loss: 4.6075 - acc: 0.04 - ETA: 14s - loss: 4.6312 - acc: 0.03 - ETA: 14s - loss: 4.6237 - acc: 0.04 - ETA: 14s - loss: 4.6392 - acc: 0.03 - ETA: 14s - loss: 4.6580 - acc: 0.03 - ETA: 14s - loss: 4.6574 - acc: 0.03 - ETA: 14s - loss: 4.6528 - acc: 0.03 - ETA: 14s - loss: 4.6593 - acc: 0.03 - ETA: 14s - loss: 4.6554 - acc: 0.03 - ETA: 14s - loss: 4.6606 - acc: 0.03 - ETA: 14s - loss: 4.6552 - acc: 0.03 - ETA: 14s - loss: 4.6484 - acc: 0.03 - ETA: 14s - loss: 4.6496 - acc: 0.03 - ETA: 14s - loss: 4.6498 - acc: 0.03 - ETA: 14s - loss: 4.6531 - acc: 0.02 - ETA: 13s - loss: 4.6561 - acc: 0.02 - ETA: 13s - loss: 4.6565 - acc: 0.02 - ETA: 13s - loss: 4.6531 - acc: 0.02 - ETA: 13s - loss: 4.6562 - acc: 0.02 - ETA: 13s - loss: 4.6581 - acc: 0.02 - ETA: 13s - loss: 4.6568 - acc: 0.02 - ETA: 13s - loss: 4.6506 - acc: 0.02 - ETA: 13s - loss: 4.6504 - acc: 0.03 - ETA: 13s - loss: 4.6520 - acc: 0.03 - ETA: 13s - loss: 4.6479 - acc: 0.03 - ETA: 13s - loss: 4.6481 - acc: 0.03 - ETA: 13s - loss: 4.6594 - acc: 0.03 - ETA: 13s - loss: 4.6615 - acc: 0.03 - ETA: 13s - loss: 4.6603 - acc: 0.03 - ETA: 13s - loss: 4.6616 - acc: 0.03 - ETA: 12s - loss: 4.6624 - acc: 0.02 - ETA: 12s - loss: 4.6627 - acc: 0.02 - ETA: 12s - loss: 4.6616 - acc: 0.03 - ETA: 12s - loss: 4.6607 - acc: 0.03 - ETA: 12s - loss: 4.6623 - acc: 0.02 - ETA: 12s - loss: 4.6625 - acc: 0.02 - ETA: 12s - loss: 4.6641 - acc: 0.03 - ETA: 12s - loss: 4.6656 - acc: 0.03 - ETA: 12s - loss: 4.6654 - acc: 0.02 - ETA: 12s - loss: 4.6658 - acc: 0.02 - ETA: 12s - loss: 4.6666 - acc: 0.02 - ETA: 12s - loss: 4.6645 - acc: 0.02 - ETA: 12s - loss: 4.6642 - acc: 0.02 - ETA: 12s - loss: 4.6675 - acc: 0.02 - ETA: 12s - loss: 4.6686 - acc: 0.02 - ETA: 12s - loss: 4.6688 - acc: 0.02 - ETA: 12s - loss: 4.6685 - acc: 0.02 - ETA: 12s - loss: 4.6701 - acc: 0.02 - ETA: 12s - loss: 4.6696 - acc: 0.03 - ETA: 12s - loss: 4.6695 - acc: 0.03 - ETA: 12s - loss: 4.6699 - acc: 0.03 - ETA: 12s - loss: 4.6739 - acc: 0.03 - ETA: 12s - loss: 4.6704 - acc: 0.03 - ETA: 12s - loss: 4.6685 - acc: 0.03 - ETA: 12s - loss: 4.6692 - acc: 0.02 - ETA: 11s - loss: 4.6697 - acc: 0.03 - ETA: 11s - loss: 4.6687 - acc: 0.03 - ETA: 11s - loss: 4.6693 - acc: 0.03 - ETA: 11s - loss: 4.6683 - acc: 0.03 - ETA: 11s - loss: 4.6657 - acc: 0.03 - ETA: 11s - loss: 4.6634 - acc: 0.03 - ETA: 11s - loss: 4.6608 - acc: 0.03 - ETA: 11s - loss: 4.6617 - acc: 0.03 - ETA: 11s - loss: 4.6646 - acc: 0.03 - ETA: 11s - loss: 4.6670 - acc: 0.02 - ETA: 11s - loss: 4.6662 - acc: 0.02 - ETA: 11s - loss: 4.6657 - acc: 0.02 - ETA: 10s - loss: 4.6635 - acc: 0.02 - ETA: 10s - loss: 4.6656 - acc: 0.02 - ETA: 10s - loss: 4.6670 - acc: 0.02 - ETA: 10s - loss: 4.6659 - acc: 0.02 - ETA: 10s - loss: 4.6667 - acc: 0.02 - ETA: 10s - loss: 4.6652 - acc: 0.02 - ETA: 10s - loss: 4.6666 - acc: 0.02 - ETA: 10s - loss: 4.6680 - acc: 0.02 - ETA: 10s - loss: 4.6704 - acc: 0.02 - ETA: 10s - loss: 4.6693 - acc: 0.02 - ETA: 10s - loss: 4.6662 - acc: 0.02 - ETA: 10s - loss: 4.6677 - acc: 0.02 - ETA: 9s - loss: 4.6670 - acc: 0.0298 - ETA: 9s - loss: 4.6670 - acc: 0.029 - ETA: 9s - loss: 4.6670 - acc: 0.029 - ETA: 9s - loss: 4.6658 - acc: 0.029 - ETA: 9s - loss: 4.6661 - acc: 0.029 - ETA: 9s - loss: 4.6634 - acc: 0.028 - ETA: 9s - loss: 4.6622 - acc: 0.029 - ETA: 9s - loss: 4.6615 - acc: 0.028 - ETA: 9s - loss: 4.6600 - acc: 0.029 - ETA: 9s - loss: 4.6599 - acc: 0.029 - ETA: 9s - loss: 4.6603 - acc: 0.029 - ETA: 9s - loss: 4.6606 - acc: 0.030 - ETA: 9s - loss: 4.6613 - acc: 0.029 - ETA: 9s - loss: 4.6604 - acc: 0.029 - ETA: 9s - loss: 4.6620 - acc: 0.029 - ETA: 9s - loss: 4.6632 - acc: 0.029 - ETA: 9s - loss: 4.6624 - acc: 0.029 - ETA: 9s - loss: 4.6623 - acc: 0.029 - ETA: 8s - loss: 4.6604 - acc: 0.029 - ETA: 8s - loss: 4.6599 - acc: 0.029 - ETA: 8s - loss: 4.6599 - acc: 0.029 - ETA: 8s - loss: 4.6598 - acc: 0.030 - ETA: 8s - loss: 4.6579 - acc: 0.030 - ETA: 8s - loss: 4.6611 - acc: 0.030 - ETA: 8s - loss: 4.6623 - acc: 0.029 - ETA: 8s - loss: 4.6634 - acc: 0.029 - ETA: 8s - loss: 4.6628 - acc: 0.029 - ETA: 8s - loss: 4.6617 - acc: 0.029 - ETA: 8s - loss: 4.6612 - acc: 0.030 - ETA: 7s - loss: 4.6633 - acc: 0.030 - ETA: 7s - loss: 4.6621 - acc: 0.030 - ETA: 7s - loss: 4.6639 - acc: 0.030 - ETA: 7s - loss: 4.6667 - acc: 0.030 - ETA: 7s - loss: 4.6673 - acc: 0.030 - ETA: 7s - loss: 4.6661 - acc: 0.031 - ETA: 7s - loss: 4.6662 - acc: 0.031 - ETA: 7s - loss: 4.6666 - acc: 0.031 - ETA: 7s - loss: 4.6663 - acc: 0.031 - ETA: 7s - loss: 4.6651 - acc: 0.031 - ETA: 7s - loss: 4.6646 - acc: 0.031 - ETA: 7s - loss: 4.6649 - acc: 0.031 - ETA: 6s - loss: 4.6652 - acc: 0.030 - ETA: 6s - loss: 4.6659 - acc: 0.030 - ETA: 6s - loss: 4.6661 - acc: 0.030 - ETA: 6s - loss: 4.6672 - acc: 0.030 - ETA: 6s - loss: 4.6675 - acc: 0.030 - ETA: 6s - loss: 4.6678 - acc: 0.030 - ETA: 6s - loss: 4.6685 - acc: 0.030 - ETA: 6s - loss: 4.6691 - acc: 0.030 - ETA: 6s - loss: 4.6698 - acc: 0.030 - ETA: 6s - loss: 4.6704 - acc: 0.030 - ETA: 6s - loss: 4.6706 - acc: 0.030 - ETA: 6s - loss: 4.6714 - acc: 0.030 - ETA: 6s - loss: 4.6709 - acc: 0.030 - ETA: 5s - loss: 4.6713 - acc: 0.030 - ETA: 5s - loss: 4.6730 - acc: 0.030 - ETA: 5s - loss: 4.6727 - acc: 0.030 - ETA: 5s - loss: 4.6735 - acc: 0.030 - ETA: 5s - loss: 4.6741 - acc: 0.030 - ETA: 5s - loss: 4.6743 - acc: 0.030 - ETA: 5s - loss: 4.6739 - acc: 0.030 - ETA: 5s - loss: 4.6730 - acc: 0.030 - ETA: 5s - loss: 4.6723 - acc: 0.030 - ETA: 5s - loss: 4.6722 - acc: 0.030 - ETA: 5s - loss: 4.6728 - acc: 0.030 - ETA: 5s - loss: 4.6732 - acc: 0.030 - ETA: 5s - loss: 4.6739 - acc: 0.030 - ETA: 4s - loss: 4.6744 - acc: 0.031 - ETA: 4s - loss: 4.6737 - acc: 0.031 - ETA: 4s - loss: 4.6731 - acc: 0.030 - ETA: 4s - loss: 4.6729 - acc: 0.030 - ETA: 4s - loss: 4.6729 - acc: 0.030 - ETA: 4s - loss: 4.6746 - acc: 0.030 - ETA: 4s - loss: 4.6742 - acc: 0.030 - ETA: 4s - loss: 4.6738 - acc: 0.030 - ETA: 4s - loss: 4.6741 - acc: 0.030 - ETA: 4s - loss: 4.6747 - acc: 0.030 - ETA: 4s - loss: 4.6736 - acc: 0.030 - ETA: 4s - loss: 4.6735 - acc: 0.030 - ETA: 3s - loss: 4.6733 - acc: 0.030 - ETA: 3s - loss: 4.6749 - acc: 0.030 - ETA: 3s - loss: 4.6759 - acc: 0.030 - ETA: 3s - loss: 4.6763 - acc: 0.030 - ETA: 3s - loss: 4.6756 - acc: 0.030 - ETA: 3s - loss: 4.6756 - acc: 0.030 - ETA: 3s - loss: 4.6755 - acc: 0.030 - ETA: 3s - loss: 4.6751 - acc: 0.030 - ETA: 3s - loss: 4.6749 - acc: 0.030 - ETA: 3s - loss: 4.6753 - acc: 0.030 - ETA: 3s - loss: 4.6754 - acc: 0.030 - ETA: 3s - loss: 4.6767 - acc: 0.030 - ETA: 3s - loss: 4.6761 - acc: 0.030 - ETA: 2s - loss: 4.6753 - acc: 0.030 - ETA: 2s - loss: 4.6772 - acc: 0.030 - ETA: 2s - loss: 4.6764 - acc: 0.030 - ETA: 2s - loss: 4.6766 - acc: 0.030 - ETA: 2s - loss: 4.6776 - acc: 0.030 - ETA: 2s - loss: 4.6776 - acc: 0.029 - ETA: 2s - loss: 4.6771 - acc: 0.029 - ETA: 2s - loss: 4.6769 - acc: 0.030 - ETA: 2s - loss: 4.6771 - acc: 0.030 - ETA: 2s - loss: 4.6774 - acc: 0.030 - ETA: 2s - loss: 4.6767 - acc: 0.030 - ETA: 2s - loss: 4.6761 - acc: 0.029 - ETA: 2s - loss: 4.6753 - acc: 0.029 - ETA: 1s - loss: 4.6756 - acc: 0.029 - ETA: 1s - loss: 4.6746 - acc: 0.029 - ETA: 1s - loss: 4.6745 - acc: 0.029 - ETA: 1s - loss: 4.6752 - acc: 0.029 - ETA: 1s - loss: 4.6755 - acc: 0.029 - ETA: 1s - loss: 4.6757 - acc: 0.029 - ETA: 1s - loss: 4.6757 - acc: 0.029 - ETA: 1s - loss: 4.6759 - acc: 0.028 - ETA: 1s - loss: 4.6761 - acc: 0.028 - ETA: 1s - loss: 4.6765 - acc: 0.028 - ETA: 1s - loss: 4.6767 - acc: 0.028 - ETA: 1s - loss: 4.6771 - acc: 0.028 - ETA: 0s - loss: 4.6771 - acc: 0.028 - ETA: 0s - loss: 4.6775 - acc: 0.028 - ETA: 0s - loss: 4.6772 - acc: 0.028 - ETA: 0s - loss: 4.6774 - acc: 0.028 - ETA: 0s - loss: 4.6775 - acc: 0.028 - ETA: 0s - loss: 4.6778 - acc: 0.028 - ETA: 0s - loss: 4.6778 - acc: 0.028 - ETA: 0s - loss: 4.6771 - acc: 0.028 - ETA: 0s - loss: 4.6768 - acc: 0.028 - ETA: 0s - loss: 4.6773 - acc: 0.028 - ETA: 0s - loss: 4.6771 - acc: 0.028 - ETA: 0s - loss: 4.6776 - acc: 0.028 - 17s 3ms/step - loss: 4.6770 - acc: 0.0283 - val_loss: 4.7213 - val_acc: 0.0311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 4.74020 to 4.72132, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 15s - loss: 4.6866 - acc: 0.10 - ETA: 15s - loss: 4.5786 - acc: 0.05 - ETA: 15s - loss: 4.5747 - acc: 0.05 - ETA: 14s - loss: 4.5984 - acc: 0.03 - ETA: 15s - loss: 4.6211 - acc: 0.03 - ETA: 15s - loss: 4.6213 - acc: 0.03 - ETA: 14s - loss: 4.6730 - acc: 0.03 - ETA: 15s - loss: 4.6760 - acc: 0.03 - ETA: 15s - loss: 4.6750 - acc: 0.03 - ETA: 15s - loss: 4.6611 - acc: 0.02 - ETA: 15s - loss: 4.6524 - acc: 0.03 - ETA: 15s - loss: 4.6469 - acc: 0.02 - ETA: 15s - loss: 4.6391 - acc: 0.03 - ETA: 14s - loss: 4.6579 - acc: 0.03 - ETA: 14s - loss: 4.6556 - acc: 0.03 - ETA: 14s - loss: 4.6700 - acc: 0.03 - ETA: 14s - loss: 4.6673 - acc: 0.03 - ETA: 14s - loss: 4.6678 - acc: 0.03 - ETA: 14s - loss: 4.6712 - acc: 0.03 - ETA: 14s - loss: 4.6739 - acc: 0.03 - ETA: 14s - loss: 4.6712 - acc: 0.03 - ETA: 14s - loss: 4.6708 - acc: 0.02 - ETA: 14s - loss: 4.6677 - acc: 0.03 - ETA: 14s - loss: 4.6575 - acc: 0.02 - ETA: 14s - loss: 4.6577 - acc: 0.03 - ETA: 14s - loss: 4.6590 - acc: 0.03 - ETA: 14s - loss: 4.6594 - acc: 0.02 - ETA: 14s - loss: 4.6584 - acc: 0.02 - ETA: 14s - loss: 4.6510 - acc: 0.03 - ETA: 13s - loss: 4.6462 - acc: 0.03 - ETA: 13s - loss: 4.6407 - acc: 0.03 - ETA: 13s - loss: 4.6397 - acc: 0.03 - ETA: 13s - loss: 4.6386 - acc: 0.03 - ETA: 13s - loss: 4.6420 - acc: 0.03 - ETA: 13s - loss: 4.6431 - acc: 0.03 - ETA: 13s - loss: 4.6444 - acc: 0.03 - ETA: 13s - loss: 4.6381 - acc: 0.03 - ETA: 13s - loss: 4.6499 - acc: 0.03 - ETA: 13s - loss: 4.6490 - acc: 0.03 - ETA: 13s - loss: 4.6502 - acc: 0.03 - ETA: 13s - loss: 4.6520 - acc: 0.03 - ETA: 13s - loss: 4.6566 - acc: 0.03 - ETA: 13s - loss: 4.6575 - acc: 0.03 - ETA: 13s - loss: 4.6611 - acc: 0.03 - ETA: 13s - loss: 4.6622 - acc: 0.02 - ETA: 12s - loss: 4.6642 - acc: 0.02 - ETA: 12s - loss: 4.6644 - acc: 0.02 - ETA: 12s - loss: 4.6731 - acc: 0.02 - ETA: 12s - loss: 4.6711 - acc: 0.02 - ETA: 12s - loss: 4.6709 - acc: 0.02 - ETA: 12s - loss: 4.6722 - acc: 0.02 - ETA: 12s - loss: 4.6665 - acc: 0.03 - ETA: 12s - loss: 4.6598 - acc: 0.02 - ETA: 12s - loss: 4.6612 - acc: 0.02 - ETA: 12s - loss: 4.6611 - acc: 0.02 - ETA: 12s - loss: 4.6594 - acc: 0.02 - ETA: 12s - loss: 4.6528 - acc: 0.02 - ETA: 11s - loss: 4.6528 - acc: 0.02 - ETA: 11s - loss: 4.6555 - acc: 0.02 - ETA: 11s - loss: 4.6555 - acc: 0.02 - ETA: 11s - loss: 4.6555 - acc: 0.02 - ETA: 11s - loss: 4.6576 - acc: 0.02 - ETA: 11s - loss: 4.6542 - acc: 0.02 - ETA: 11s - loss: 4.6526 - acc: 0.02 - ETA: 11s - loss: 4.6513 - acc: 0.02 - ETA: 11s - loss: 4.6504 - acc: 0.02 - ETA: 11s - loss: 4.6513 - acc: 0.02 - ETA: 11s - loss: 4.6524 - acc: 0.02 - ETA: 10s - loss: 4.6521 - acc: 0.02 - ETA: 10s - loss: 4.6523 - acc: 0.02 - ETA: 10s - loss: 4.6526 - acc: 0.02 - ETA: 10s - loss: 4.6507 - acc: 0.03 - ETA: 10s - loss: 4.6509 - acc: 0.03 - ETA: 10s - loss: 4.6535 - acc: 0.03 - ETA: 10s - loss: 4.6546 - acc: 0.03 - ETA: 10s - loss: 4.6569 - acc: 0.03 - ETA: 10s - loss: 4.6558 - acc: 0.02 - ETA: 10s - loss: 4.6544 - acc: 0.03 - ETA: 10s - loss: 4.6549 - acc: 0.03 - ETA: 10s - loss: 4.6541 - acc: 0.03 - ETA: 10s - loss: 4.6528 - acc: 0.03 - ETA: 10s - loss: 4.6530 - acc: 0.03 - ETA: 10s - loss: 4.6492 - acc: 0.03 - ETA: 10s - loss: 4.6499 - acc: 0.03 - ETA: 9s - loss: 4.6520 - acc: 0.0306 - ETA: 9s - loss: 4.6531 - acc: 0.030 - ETA: 9s - loss: 4.6561 - acc: 0.030 - ETA: 9s - loss: 4.6554 - acc: 0.030 - ETA: 9s - loss: 4.6537 - acc: 0.030 - ETA: 9s - loss: 4.6536 - acc: 0.030 - ETA: 9s - loss: 4.6538 - acc: 0.029 - ETA: 9s - loss: 4.6545 - acc: 0.029 - ETA: 9s - loss: 4.6528 - acc: 0.031 - ETA: 9s - loss: 4.6531 - acc: 0.032 - ETA: 9s - loss: 4.6521 - acc: 0.032 - ETA: 9s - loss: 4.6526 - acc: 0.032 - ETA: 9s - loss: 4.6520 - acc: 0.032 - ETA: 8s - loss: 4.6536 - acc: 0.032 - ETA: 8s - loss: 4.6536 - acc: 0.032 - ETA: 8s - loss: 4.6534 - acc: 0.032 - ETA: 8s - loss: 4.6520 - acc: 0.032 - ETA: 8s - loss: 4.6524 - acc: 0.032 - ETA: 8s - loss: 4.6534 - acc: 0.032 - ETA: 8s - loss: 4.6524 - acc: 0.032 - ETA: 8s - loss: 4.6514 - acc: 0.032 - ETA: 8s - loss: 4.6508 - acc: 0.032 - ETA: 8s - loss: 4.6490 - acc: 0.033 - ETA: 8s - loss: 4.6499 - acc: 0.033 - ETA: 7s - loss: 4.6524 - acc: 0.033 - ETA: 7s - loss: 4.6519 - acc: 0.033 - ETA: 7s - loss: 4.6529 - acc: 0.033 - ETA: 7s - loss: 4.6534 - acc: 0.033 - ETA: 7s - loss: 4.6547 - acc: 0.033 - ETA: 7s - loss: 4.6554 - acc: 0.033 - ETA: 7s - loss: 4.6559 - acc: 0.033 - ETA: 7s - loss: 4.6564 - acc: 0.034 - ETA: 7s - loss: 4.6558 - acc: 0.034 - ETA: 7s - loss: 4.6547 - acc: 0.034 - ETA: 7s - loss: 4.6528 - acc: 0.034 - ETA: 7s - loss: 4.6534 - acc: 0.035 - ETA: 7s - loss: 4.6523 - acc: 0.035 - ETA: 7s - loss: 4.6527 - acc: 0.035 - ETA: 7s - loss: 4.6517 - acc: 0.035 - ETA: 7s - loss: 4.6512 - acc: 0.035 - ETA: 7s - loss: 4.6507 - acc: 0.035 - ETA: 6s - loss: 4.6509 - acc: 0.035 - ETA: 6s - loss: 4.6498 - acc: 0.035 - ETA: 6s - loss: 4.6495 - acc: 0.035 - ETA: 6s - loss: 4.6507 - acc: 0.035 - ETA: 6s - loss: 4.6515 - acc: 0.034 - ETA: 6s - loss: 4.6512 - acc: 0.034 - ETA: 6s - loss: 4.6488 - acc: 0.035 - ETA: 6s - loss: 4.6497 - acc: 0.034 - ETA: 6s - loss: 4.6502 - acc: 0.034 - ETA: 6s - loss: 4.6506 - acc: 0.034 - ETA: 6s - loss: 4.6502 - acc: 0.034 - ETA: 5s - loss: 4.6512 - acc: 0.034 - ETA: 5s - loss: 4.6510 - acc: 0.034 - ETA: 5s - loss: 4.6513 - acc: 0.033 - ETA: 5s - loss: 4.6502 - acc: 0.033 - ETA: 5s - loss: 4.6500 - acc: 0.033 - ETA: 5s - loss: 4.6511 - acc: 0.033 - ETA: 5s - loss: 4.6524 - acc: 0.033 - ETA: 5s - loss: 4.6536 - acc: 0.033 - ETA: 5s - loss: 4.6529 - acc: 0.034 - ETA: 5s - loss: 4.6525 - acc: 0.034 - ETA: 5s - loss: 4.6508 - acc: 0.034 - ETA: 5s - loss: 4.6502 - acc: 0.034 - ETA: 5s - loss: 4.6514 - acc: 0.034 - ETA: 4s - loss: 4.6532 - acc: 0.034 - ETA: 4s - loss: 4.6553 - acc: 0.034 - ETA: 4s - loss: 4.6547 - acc: 0.034 - ETA: 4s - loss: 4.6542 - acc: 0.033 - ETA: 4s - loss: 4.6542 - acc: 0.034 - ETA: 4s - loss: 4.6531 - acc: 0.033 - ETA: 4s - loss: 4.6530 - acc: 0.033 - ETA: 4s - loss: 4.6524 - acc: 0.033 - ETA: 4s - loss: 4.6516 - acc: 0.033 - ETA: 4s - loss: 4.6522 - acc: 0.033 - ETA: 4s - loss: 4.6516 - acc: 0.033 - ETA: 4s - loss: 4.6508 - acc: 0.033 - ETA: 4s - loss: 4.6528 - acc: 0.033 - ETA: 4s - loss: 4.6532 - acc: 0.033 - ETA: 4s - loss: 4.6544 - acc: 0.033 - ETA: 4s - loss: 4.6544 - acc: 0.034 - ETA: 4s - loss: 4.6541 - acc: 0.034 - ETA: 3s - loss: 4.6528 - acc: 0.034 - ETA: 3s - loss: 4.6530 - acc: 0.034 - ETA: 3s - loss: 4.6538 - acc: 0.034 - ETA: 3s - loss: 4.6540 - acc: 0.034 - ETA: 3s - loss: 4.6543 - acc: 0.034 - ETA: 3s - loss: 4.6549 - acc: 0.034 - ETA: 3s - loss: 4.6553 - acc: 0.034 - ETA: 3s - loss: 4.6550 - acc: 0.034 - ETA: 3s - loss: 4.6554 - acc: 0.034 - ETA: 3s - loss: 4.6549 - acc: 0.035 - ETA: 3s - loss: 4.6545 - acc: 0.035 - ETA: 3s - loss: 4.6551 - acc: 0.034 - ETA: 3s - loss: 4.6552 - acc: 0.034 - ETA: 2s - loss: 4.6560 - acc: 0.034 - ETA: 2s - loss: 4.6552 - acc: 0.034 - ETA: 2s - loss: 4.6552 - acc: 0.034 - ETA: 2s - loss: 4.6557 - acc: 0.034 - ETA: 2s - loss: 4.6557 - acc: 0.034 - ETA: 2s - loss: 4.6561 - acc: 0.034 - ETA: 2s - loss: 4.6546 - acc: 0.034 - ETA: 2s - loss: 4.6543 - acc: 0.034 - ETA: 2s - loss: 4.6533 - acc: 0.035 - ETA: 2s - loss: 4.6527 - acc: 0.034 - ETA: 2s - loss: 4.6520 - acc: 0.034 - ETA: 1s - loss: 4.6510 - acc: 0.034 - ETA: 1s - loss: 4.6526 - acc: 0.034 - ETA: 1s - loss: 4.6529 - acc: 0.034 - ETA: 1s - loss: 4.6531 - acc: 0.034 - ETA: 1s - loss: 4.6538 - acc: 0.034 - ETA: 1s - loss: 4.6535 - acc: 0.034 - ETA: 1s - loss: 4.6526 - acc: 0.034 - ETA: 1s - loss: 4.6536 - acc: 0.034 - ETA: 1s - loss: 4.6531 - acc: 0.034 - ETA: 1s - loss: 4.6539 - acc: 0.034 - ETA: 1s - loss: 4.6540 - acc: 0.034 - ETA: 0s - loss: 4.6539 - acc: 0.034 - ETA: 0s - loss: 4.6541 - acc: 0.034 - ETA: 0s - loss: 4.6546 - acc: 0.034 - ETA: 0s - loss: 4.6551 - acc: 0.034 - ETA: 0s - loss: 4.6551 - acc: 0.034 - ETA: 0s - loss: 4.6545 - acc: 0.034 - ETA: 0s - loss: 4.6539 - acc: 0.034 - ETA: 0s - loss: 4.6539 - acc: 0.033 - ETA: 0s - loss: 4.6534 - acc: 0.033 - ETA: 0s - loss: 4.6525 - acc: 0.034 - ETA: 0s - loss: 4.6523 - acc: 0.034 - ETA: 0s - loss: 4.6535 - acc: 0.034 - 17s 3ms/step - loss: 4.6537 - acc: 0.0340 - val_loss: 4.6963 - val_acc: 0.0347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 4.72132 to 4.69626, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 20s - loss: 4.6039 - acc: 0.10 - ETA: 18s - loss: 4.7140 - acc: 0.05 - ETA: 18s - loss: 4.6677 - acc: 0.05 - ETA: 17s - loss: 4.7129 - acc: 0.05 - ETA: 16s - loss: 4.7058 - acc: 0.04 - ETA: 15s - loss: 4.6726 - acc: 0.04 - ETA: 16s - loss: 4.7008 - acc: 0.04 - ETA: 15s - loss: 4.6731 - acc: 0.04 - ETA: 15s - loss: 4.6516 - acc: 0.03 - ETA: 15s - loss: 4.6353 - acc: 0.03 - ETA: 15s - loss: 4.6145 - acc: 0.03 - ETA: 15s - loss: 4.6030 - acc: 0.03 - ETA: 15s - loss: 4.6169 - acc: 0.03 - ETA: 15s - loss: 4.6080 - acc: 0.04 - ETA: 14s - loss: 4.5995 - acc: 0.03 - ETA: 14s - loss: 4.5950 - acc: 0.03 - ETA: 14s - loss: 4.5936 - acc: 0.03 - ETA: 14s - loss: 4.6005 - acc: 0.03 - ETA: 14s - loss: 4.6015 - acc: 0.03 - ETA: 14s - loss: 4.5875 - acc: 0.04 - ETA: 14s - loss: 4.5879 - acc: 0.03 - ETA: 14s - loss: 4.5879 - acc: 0.04 - ETA: 14s - loss: 4.5898 - acc: 0.04 - ETA: 14s - loss: 4.5884 - acc: 0.04 - ETA: 13s - loss: 4.5874 - acc: 0.04 - ETA: 13s - loss: 4.5897 - acc: 0.04 - ETA: 13s - loss: 4.5968 - acc: 0.04 - ETA: 13s - loss: 4.6014 - acc: 0.04 - ETA: 13s - loss: 4.5988 - acc: 0.04 - ETA: 13s - loss: 4.5985 - acc: 0.04 - ETA: 13s - loss: 4.5971 - acc: 0.04 - ETA: 13s - loss: 4.5955 - acc: 0.03 - ETA: 13s - loss: 4.5985 - acc: 0.03 - ETA: 13s - loss: 4.5915 - acc: 0.04 - ETA: 13s - loss: 4.5970 - acc: 0.03 - ETA: 13s - loss: 4.5959 - acc: 0.03 - ETA: 13s - loss: 4.5978 - acc: 0.03 - ETA: 13s - loss: 4.6001 - acc: 0.03 - ETA: 13s - loss: 4.6068 - acc: 0.03 - ETA: 13s - loss: 4.6029 - acc: 0.03 - ETA: 13s - loss: 4.6022 - acc: 0.03 - ETA: 12s - loss: 4.6050 - acc: 0.03 - ETA: 12s - loss: 4.5992 - acc: 0.03 - ETA: 12s - loss: 4.6026 - acc: 0.03 - ETA: 12s - loss: 4.6061 - acc: 0.03 - ETA: 12s - loss: 4.6065 - acc: 0.03 - ETA: 12s - loss: 4.6074 - acc: 0.03 - ETA: 12s - loss: 4.6050 - acc: 0.04 - ETA: 12s - loss: 4.6060 - acc: 0.03 - ETA: 12s - loss: 4.6090 - acc: 0.03 - ETA: 12s - loss: 4.6112 - acc: 0.03 - ETA: 12s - loss: 4.6127 - acc: 0.03 - ETA: 12s - loss: 4.6119 - acc: 0.03 - ETA: 12s - loss: 4.6104 - acc: 0.03 - ETA: 11s - loss: 4.6117 - acc: 0.03 - ETA: 11s - loss: 4.6150 - acc: 0.03 - ETA: 11s - loss: 4.6136 - acc: 0.03 - ETA: 11s - loss: 4.6170 - acc: 0.03 - ETA: 11s - loss: 4.6203 - acc: 0.03 - ETA: 11s - loss: 4.6197 - acc: 0.03 - ETA: 11s - loss: 4.6237 - acc: 0.03 - ETA: 11s - loss: 4.6217 - acc: 0.03 - ETA: 11s - loss: 4.6257 - acc: 0.03 - ETA: 11s - loss: 4.6242 - acc: 0.03 - ETA: 11s - loss: 4.6179 - acc: 0.03 - ETA: 11s - loss: 4.6189 - acc: 0.03 - ETA: 11s - loss: 4.6193 - acc: 0.03 - ETA: 10s - loss: 4.6213 - acc: 0.03 - ETA: 10s - loss: 4.6225 - acc: 0.03 - ETA: 10s - loss: 4.6206 - acc: 0.03 - ETA: 10s - loss: 4.6223 - acc: 0.03 - ETA: 10s - loss: 4.6223 - acc: 0.03 - ETA: 10s - loss: 4.6178 - acc: 0.03 - ETA: 10s - loss: 4.6183 - acc: 0.03 - ETA: 10s - loss: 4.6207 - acc: 0.03 - ETA: 10s - loss: 4.6215 - acc: 0.03 - ETA: 10s - loss: 4.6226 - acc: 0.03 - ETA: 10s - loss: 4.6217 - acc: 0.03 - ETA: 10s - loss: 4.6240 - acc: 0.03 - ETA: 10s - loss: 4.6226 - acc: 0.03 - ETA: 9s - loss: 4.6231 - acc: 0.0359 - ETA: 9s - loss: 4.6227 - acc: 0.035 - ETA: 9s - loss: 4.6230 - acc: 0.035 - ETA: 9s - loss: 4.6225 - acc: 0.035 - ETA: 9s - loss: 4.6234 - acc: 0.035 - ETA: 9s - loss: 4.6208 - acc: 0.035 - ETA: 9s - loss: 4.6193 - acc: 0.035 - ETA: 9s - loss: 4.6189 - acc: 0.036 - ETA: 9s - loss: 4.6181 - acc: 0.036 - ETA: 9s - loss: 4.6179 - acc: 0.035 - ETA: 9s - loss: 4.6181 - acc: 0.035 - ETA: 9s - loss: 4.6170 - acc: 0.035 - ETA: 9s - loss: 4.6171 - acc: 0.036 - ETA: 9s - loss: 4.6176 - acc: 0.036 - ETA: 8s - loss: 4.6169 - acc: 0.036 - ETA: 8s - loss: 4.6173 - acc: 0.037 - ETA: 8s - loss: 4.6182 - acc: 0.036 - ETA: 8s - loss: 4.6185 - acc: 0.036 - ETA: 8s - loss: 4.6215 - acc: 0.035 - ETA: 8s - loss: 4.6213 - acc: 0.035 - ETA: 8s - loss: 4.6216 - acc: 0.035 - ETA: 8s - loss: 4.6232 - acc: 0.035 - ETA: 8s - loss: 4.6242 - acc: 0.035 - ETA: 8s - loss: 4.6223 - acc: 0.034 - ETA: 8s - loss: 4.6221 - acc: 0.034 - ETA: 8s - loss: 4.6221 - acc: 0.035 - ETA: 8s - loss: 4.6224 - acc: 0.034 - ETA: 7s - loss: 4.6213 - acc: 0.034 - ETA: 7s - loss: 4.6221 - acc: 0.034 - ETA: 7s - loss: 4.6213 - acc: 0.034 - ETA: 7s - loss: 4.6200 - acc: 0.034 - ETA: 7s - loss: 4.6211 - acc: 0.034 - ETA: 7s - loss: 4.6204 - acc: 0.033 - ETA: 7s - loss: 4.6217 - acc: 0.033 - ETA: 7s - loss: 4.6240 - acc: 0.033 - ETA: 7s - loss: 4.6252 - acc: 0.032 - ETA: 7s - loss: 4.6239 - acc: 0.032 - ETA: 7s - loss: 4.6258 - acc: 0.032 - ETA: 6s - loss: 4.6255 - acc: 0.032 - ETA: 6s - loss: 4.6252 - acc: 0.032 - ETA: 6s - loss: 4.6253 - acc: 0.032 - ETA: 6s - loss: 4.6269 - acc: 0.032 - ETA: 6s - loss: 4.6267 - acc: 0.032 - ETA: 6s - loss: 4.6281 - acc: 0.032 - ETA: 6s - loss: 4.6279 - acc: 0.031 - ETA: 6s - loss: 4.6282 - acc: 0.032 - ETA: 6s - loss: 4.6275 - acc: 0.032 - ETA: 6s - loss: 4.6283 - acc: 0.032 - ETA: 6s - loss: 4.6296 - acc: 0.032 - ETA: 5s - loss: 4.6287 - acc: 0.032 - ETA: 5s - loss: 4.6277 - acc: 0.032 - ETA: 5s - loss: 4.6285 - acc: 0.032 - ETA: 5s - loss: 4.6284 - acc: 0.032 - ETA: 5s - loss: 4.6294 - acc: 0.031 - ETA: 5s - loss: 4.6304 - acc: 0.031 - ETA: 5s - loss: 4.6302 - acc: 0.031 - ETA: 5s - loss: 4.6306 - acc: 0.031 - ETA: 5s - loss: 4.6313 - acc: 0.031 - ETA: 5s - loss: 4.6319 - acc: 0.031 - ETA: 5s - loss: 4.6323 - acc: 0.031 - ETA: 5s - loss: 4.6315 - acc: 0.031 - ETA: 5s - loss: 4.6318 - acc: 0.031 - ETA: 4s - loss: 4.6338 - acc: 0.031 - ETA: 4s - loss: 4.6332 - acc: 0.031 - ETA: 4s - loss: 4.6334 - acc: 0.031 - ETA: 4s - loss: 4.6329 - acc: 0.031 - ETA: 4s - loss: 4.6325 - acc: 0.031 - ETA: 4s - loss: 4.6325 - acc: 0.031 - ETA: 4s - loss: 4.6337 - acc: 0.031 - ETA: 4s - loss: 4.6343 - acc: 0.031 - ETA: 4s - loss: 4.6327 - acc: 0.031 - ETA: 4s - loss: 4.6335 - acc: 0.031 - ETA: 4s - loss: 4.6329 - acc: 0.031 - ETA: 4s - loss: 4.6322 - acc: 0.031 - ETA: 4s - loss: 4.6319 - acc: 0.032 - ETA: 4s - loss: 4.6324 - acc: 0.032 - ETA: 3s - loss: 4.6322 - acc: 0.032 - ETA: 3s - loss: 4.6332 - acc: 0.032 - ETA: 3s - loss: 4.6323 - acc: 0.032 - ETA: 3s - loss: 4.6323 - acc: 0.032 - ETA: 3s - loss: 4.6314 - acc: 0.032 - ETA: 3s - loss: 4.6319 - acc: 0.032 - ETA: 3s - loss: 4.6320 - acc: 0.032 - ETA: 3s - loss: 4.6312 - acc: 0.032 - ETA: 3s - loss: 4.6316 - acc: 0.032 - ETA: 3s - loss: 4.6305 - acc: 0.032 - ETA: 3s - loss: 4.6312 - acc: 0.032 - ETA: 3s - loss: 4.6313 - acc: 0.032 - ETA: 2s - loss: 4.6311 - acc: 0.032 - ETA: 2s - loss: 4.6313 - acc: 0.032 - ETA: 2s - loss: 4.6302 - acc: 0.032 - ETA: 2s - loss: 4.6295 - acc: 0.032 - ETA: 2s - loss: 4.6316 - acc: 0.032 - ETA: 2s - loss: 4.6317 - acc: 0.032 - ETA: 2s - loss: 4.6316 - acc: 0.032 - ETA: 2s - loss: 4.6304 - acc: 0.032 - ETA: 2s - loss: 4.6297 - acc: 0.032 - ETA: 2s - loss: 4.6314 - acc: 0.032 - ETA: 2s - loss: 4.6305 - acc: 0.032 - ETA: 2s - loss: 4.6304 - acc: 0.032 - ETA: 1s - loss: 4.6294 - acc: 0.032 - ETA: 1s - loss: 4.6293 - acc: 0.032 - ETA: 1s - loss: 4.6279 - acc: 0.032 - ETA: 1s - loss: 4.6274 - acc: 0.032 - ETA: 1s - loss: 4.6269 - acc: 0.033 - ETA: 1s - loss: 4.6266 - acc: 0.033 - ETA: 1s - loss: 4.6260 - acc: 0.032 - ETA: 1s - loss: 4.6271 - acc: 0.033 - ETA: 1s - loss: 4.6275 - acc: 0.033 - ETA: 1s - loss: 4.6277 - acc: 0.033 - ETA: 1s - loss: 4.6277 - acc: 0.033 - ETA: 0s - loss: 4.6286 - acc: 0.032 - ETA: 0s - loss: 4.6286 - acc: 0.032 - ETA: 0s - loss: 4.6271 - acc: 0.033 - ETA: 0s - loss: 4.6264 - acc: 0.033 - ETA: 0s - loss: 4.6274 - acc: 0.032 - ETA: 0s - loss: 4.6267 - acc: 0.033 - ETA: 0s - loss: 4.6281 - acc: 0.033 - ETA: 0s - loss: 4.6283 - acc: 0.033 - ETA: 0s - loss: 4.6278 - acc: 0.033 - ETA: 0s - loss: 4.6282 - acc: 0.032 - ETA: 0s - loss: 4.6289 - acc: 0.032 - ETA: 0s - loss: 4.6293 - acc: 0.032 - ETA: 0s - loss: 4.6289 - acc: 0.033 - 17s 2ms/step - loss: 4.6280 - acc: 0.0332 - val_loss: 4.6965 - val_acc: 0.0251\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.69626\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 16s - loss: 4.3594 - acc: 0.10 - ETA: 15s - loss: 4.6692 - acc: 0.03 - ETA: 15s - loss: 4.6531 - acc: 0.05 - ETA: 15s - loss: 4.6566 - acc: 0.05 - ETA: 15s - loss: 4.6473 - acc: 0.05 - ETA: 14s - loss: 4.6201 - acc: 0.04 - ETA: 15s - loss: 4.6399 - acc: 0.03 - ETA: 15s - loss: 4.6164 - acc: 0.03 - ETA: 15s - loss: 4.6246 - acc: 0.03 - ETA: 14s - loss: 4.6459 - acc: 0.03 - ETA: 14s - loss: 4.6502 - acc: 0.03 - ETA: 14s - loss: 4.6544 - acc: 0.03 - ETA: 14s - loss: 4.6456 - acc: 0.03 - ETA: 15s - loss: 4.6391 - acc: 0.03 - ETA: 14s - loss: 4.6468 - acc: 0.03 - ETA: 14s - loss: 4.6453 - acc: 0.03 - ETA: 14s - loss: 4.6555 - acc: 0.03 - ETA: 14s - loss: 4.6541 - acc: 0.03 - ETA: 14s - loss: 4.6555 - acc: 0.03 - ETA: 14s - loss: 4.6535 - acc: 0.03 - ETA: 14s - loss: 4.6526 - acc: 0.03 - ETA: 14s - loss: 4.6528 - acc: 0.03 - ETA: 14s - loss: 4.6466 - acc: 0.03 - ETA: 14s - loss: 4.6432 - acc: 0.03 - ETA: 14s - loss: 4.6445 - acc: 0.03 - ETA: 13s - loss: 4.6434 - acc: 0.03 - ETA: 13s - loss: 4.6386 - acc: 0.03 - ETA: 13s - loss: 4.6329 - acc: 0.03 - ETA: 13s - loss: 4.6268 - acc: 0.03 - ETA: 13s - loss: 4.6266 - acc: 0.03 - ETA: 13s - loss: 4.6219 - acc: 0.03 - ETA: 13s - loss: 4.6219 - acc: 0.03 - ETA: 13s - loss: 4.6245 - acc: 0.03 - ETA: 13s - loss: 4.6231 - acc: 0.03 - ETA: 13s - loss: 4.6276 - acc: 0.03 - ETA: 13s - loss: 4.6240 - acc: 0.03 - ETA: 13s - loss: 4.6220 - acc: 0.03 - ETA: 12s - loss: 4.6251 - acc: 0.03 - ETA: 12s - loss: 4.6247 - acc: 0.03 - ETA: 12s - loss: 4.6258 - acc: 0.03 - ETA: 12s - loss: 4.6232 - acc: 0.03 - ETA: 12s - loss: 4.6200 - acc: 0.03 - ETA: 12s - loss: 4.6219 - acc: 0.03 - ETA: 12s - loss: 4.6194 - acc: 0.03 - ETA: 12s - loss: 4.6197 - acc: 0.03 - ETA: 12s - loss: 4.6188 - acc: 0.03 - ETA: 12s - loss: 4.6203 - acc: 0.03 - ETA: 12s - loss: 4.6186 - acc: 0.03 - ETA: 12s - loss: 4.6176 - acc: 0.03 - ETA: 12s - loss: 4.6179 - acc: 0.03 - ETA: 12s - loss: 4.6151 - acc: 0.03 - ETA: 12s - loss: 4.6154 - acc: 0.03 - ETA: 11s - loss: 4.6148 - acc: 0.03 - ETA: 11s - loss: 4.6118 - acc: 0.03 - ETA: 11s - loss: 4.6088 - acc: 0.03 - ETA: 11s - loss: 4.6079 - acc: 0.03 - ETA: 11s - loss: 4.6049 - acc: 0.03 - ETA: 11s - loss: 4.6066 - acc: 0.03 - ETA: 11s - loss: 4.6066 - acc: 0.03 - ETA: 11s - loss: 4.6074 - acc: 0.03 - ETA: 11s - loss: 4.6052 - acc: 0.03 - ETA: 11s - loss: 4.6050 - acc: 0.03 - ETA: 11s - loss: 4.6045 - acc: 0.03 - ETA: 11s - loss: 4.6042 - acc: 0.03 - ETA: 11s - loss: 4.6044 - acc: 0.03 - ETA: 11s - loss: 4.6027 - acc: 0.03 - ETA: 10s - loss: 4.6037 - acc: 0.03 - ETA: 10s - loss: 4.6037 - acc: 0.03 - ETA: 10s - loss: 4.6048 - acc: 0.03 - ETA: 10s - loss: 4.6030 - acc: 0.03 - ETA: 10s - loss: 4.6037 - acc: 0.03 - ETA: 10s - loss: 4.6051 - acc: 0.03 - ETA: 10s - loss: 4.6069 - acc: 0.03 - ETA: 10s - loss: 4.6085 - acc: 0.03 - ETA: 10s - loss: 4.6078 - acc: 0.03 - ETA: 10s - loss: 4.6084 - acc: 0.03 - ETA: 10s - loss: 4.6081 - acc: 0.03 - ETA: 10s - loss: 4.6087 - acc: 0.03 - ETA: 10s - loss: 4.6075 - acc: 0.03 - ETA: 9s - loss: 4.6043 - acc: 0.0366 - ETA: 9s - loss: 4.6025 - acc: 0.036 - ETA: 9s - loss: 4.5970 - acc: 0.036 - ETA: 9s - loss: 4.5986 - acc: 0.036 - ETA: 9s - loss: 4.6008 - acc: 0.035 - ETA: 9s - loss: 4.5988 - acc: 0.035 - ETA: 9s - loss: 4.5960 - acc: 0.036 - ETA: 9s - loss: 4.5963 - acc: 0.036 - ETA: 9s - loss: 4.5958 - acc: 0.036 - ETA: 9s - loss: 4.5955 - acc: 0.036 - ETA: 9s - loss: 4.5943 - acc: 0.036 - ETA: 9s - loss: 4.5942 - acc: 0.037 - ETA: 9s - loss: 4.5976 - acc: 0.037 - ETA: 9s - loss: 4.5980 - acc: 0.037 - ETA: 8s - loss: 4.5992 - acc: 0.037 - ETA: 8s - loss: 4.6011 - acc: 0.036 - ETA: 8s - loss: 4.6012 - acc: 0.036 - ETA: 8s - loss: 4.6014 - acc: 0.036 - ETA: 8s - loss: 4.6040 - acc: 0.036 - ETA: 8s - loss: 4.6026 - acc: 0.035 - ETA: 8s - loss: 4.6042 - acc: 0.035 - ETA: 8s - loss: 4.6028 - acc: 0.035 - ETA: 8s - loss: 4.6004 - acc: 0.036 - ETA: 8s - loss: 4.5998 - acc: 0.036 - ETA: 8s - loss: 4.5999 - acc: 0.035 - ETA: 8s - loss: 4.6008 - acc: 0.035 - ETA: 8s - loss: 4.6028 - acc: 0.035 - ETA: 8s - loss: 4.6012 - acc: 0.036 - ETA: 8s - loss: 4.6023 - acc: 0.036 - ETA: 7s - loss: 4.6002 - acc: 0.037 - ETA: 7s - loss: 4.6006 - acc: 0.037 - ETA: 7s - loss: 4.6012 - acc: 0.037 - ETA: 7s - loss: 4.6005 - acc: 0.037 - ETA: 7s - loss: 4.6001 - acc: 0.037 - ETA: 7s - loss: 4.6007 - acc: 0.036 - ETA: 7s - loss: 4.6010 - acc: 0.036 - ETA: 7s - loss: 4.6022 - acc: 0.037 - ETA: 7s - loss: 4.6003 - acc: 0.036 - ETA: 7s - loss: 4.6014 - acc: 0.037 - ETA: 7s - loss: 4.6013 - acc: 0.037 - ETA: 7s - loss: 4.6014 - acc: 0.037 - ETA: 7s - loss: 4.6018 - acc: 0.037 - ETA: 7s - loss: 4.6016 - acc: 0.038 - ETA: 6s - loss: 4.6006 - acc: 0.038 - ETA: 6s - loss: 4.6012 - acc: 0.037 - ETA: 6s - loss: 4.6003 - acc: 0.037 - ETA: 6s - loss: 4.5994 - acc: 0.037 - ETA: 6s - loss: 4.6012 - acc: 0.037 - ETA: 6s - loss: 4.6010 - acc: 0.037 - ETA: 6s - loss: 4.6013 - acc: 0.037 - ETA: 6s - loss: 4.6024 - acc: 0.037 - ETA: 6s - loss: 4.6018 - acc: 0.037 - ETA: 6s - loss: 4.6025 - acc: 0.037 - ETA: 6s - loss: 4.6025 - acc: 0.037 - ETA: 5s - loss: 4.6014 - acc: 0.038 - ETA: 5s - loss: 4.6024 - acc: 0.037 - ETA: 5s - loss: 4.6026 - acc: 0.037 - ETA: 5s - loss: 4.6033 - acc: 0.038 - ETA: 5s - loss: 4.6033 - acc: 0.037 - ETA: 5s - loss: 4.6051 - acc: 0.037 - ETA: 5s - loss: 4.6053 - acc: 0.037 - ETA: 5s - loss: 4.6070 - acc: 0.037 - ETA: 5s - loss: 4.6068 - acc: 0.037 - ETA: 5s - loss: 4.6064 - acc: 0.037 - ETA: 5s - loss: 4.6071 - acc: 0.037 - ETA: 5s - loss: 4.6062 - acc: 0.037 - ETA: 5s - loss: 4.6083 - acc: 0.037 - ETA: 5s - loss: 4.6081 - acc: 0.037 - ETA: 4s - loss: 4.6066 - acc: 0.038 - ETA: 4s - loss: 4.6057 - acc: 0.038 - ETA: 4s - loss: 4.6063 - acc: 0.037 - ETA: 4s - loss: 4.6061 - acc: 0.038 - ETA: 4s - loss: 4.6066 - acc: 0.037 - ETA: 4s - loss: 4.6068 - acc: 0.037 - ETA: 4s - loss: 4.6072 - acc: 0.038 - ETA: 4s - loss: 4.6068 - acc: 0.038 - ETA: 4s - loss: 4.6065 - acc: 0.038 - ETA: 4s - loss: 4.6064 - acc: 0.037 - ETA: 4s - loss: 4.6058 - acc: 0.037 - ETA: 4s - loss: 4.6065 - acc: 0.037 - ETA: 4s - loss: 4.6062 - acc: 0.037 - ETA: 4s - loss: 4.6062 - acc: 0.037 - ETA: 3s - loss: 4.6071 - acc: 0.037 - ETA: 3s - loss: 4.6087 - acc: 0.036 - ETA: 3s - loss: 4.6094 - acc: 0.037 - ETA: 3s - loss: 4.6090 - acc: 0.037 - ETA: 3s - loss: 4.6105 - acc: 0.037 - ETA: 3s - loss: 4.6108 - acc: 0.037 - ETA: 3s - loss: 4.6095 - acc: 0.037 - ETA: 3s - loss: 4.6094 - acc: 0.037 - ETA: 3s - loss: 4.6099 - acc: 0.037 - ETA: 3s - loss: 4.6098 - acc: 0.037 - ETA: 3s - loss: 4.6086 - acc: 0.037 - ETA: 3s - loss: 4.6093 - acc: 0.036 - ETA: 3s - loss: 4.6101 - acc: 0.037 - ETA: 2s - loss: 4.6100 - acc: 0.037 - ETA: 2s - loss: 4.6098 - acc: 0.037 - ETA: 2s - loss: 4.6091 - acc: 0.037 - ETA: 2s - loss: 4.6085 - acc: 0.037 - ETA: 2s - loss: 4.6088 - acc: 0.037 - ETA: 2s - loss: 4.6073 - acc: 0.038 - ETA: 2s - loss: 4.6069 - acc: 0.038 - ETA: 2s - loss: 4.6060 - acc: 0.038 - ETA: 2s - loss: 4.6056 - acc: 0.038 - ETA: 2s - loss: 4.6054 - acc: 0.038 - ETA: 2s - loss: 4.6056 - acc: 0.038 - ETA: 2s - loss: 4.6056 - acc: 0.038 - ETA: 2s - loss: 4.6056 - acc: 0.039 - ETA: 2s - loss: 4.6053 - acc: 0.038 - ETA: 2s - loss: 4.6042 - acc: 0.039 - ETA: 1s - loss: 4.6041 - acc: 0.039 - ETA: 1s - loss: 4.6043 - acc: 0.039 - ETA: 1s - loss: 4.6036 - acc: 0.039 - ETA: 1s - loss: 4.6018 - acc: 0.039 - ETA: 1s - loss: 4.6019 - acc: 0.039 - ETA: 1s - loss: 4.6030 - acc: 0.039 - ETA: 1s - loss: 4.6028 - acc: 0.038 - ETA: 1s - loss: 4.6034 - acc: 0.039 - ETA: 1s - loss: 4.6038 - acc: 0.039 - ETA: 1s - loss: 4.6027 - acc: 0.039 - ETA: 1s - loss: 4.6013 - acc: 0.039 - ETA: 1s - loss: 4.6015 - acc: 0.039 - ETA: 0s - loss: 4.6022 - acc: 0.039 - ETA: 0s - loss: 4.6022 - acc: 0.039 - ETA: 0s - loss: 4.6020 - acc: 0.039 - ETA: 0s - loss: 4.6011 - acc: 0.039 - ETA: 0s - loss: 4.6014 - acc: 0.039 - ETA: 0s - loss: 4.6018 - acc: 0.039 - ETA: 0s - loss: 4.6018 - acc: 0.039 - ETA: 0s - loss: 4.6014 - acc: 0.039 - ETA: 0s - loss: 4.6010 - acc: 0.039 - ETA: 0s - loss: 4.6014 - acc: 0.039 - ETA: 0s - loss: 4.6014 - acc: 0.039 - ETA: 0s - loss: 4.6010 - acc: 0.039 - ETA: 0s - loss: 4.6000 - acc: 0.039 - 17s 3ms/step - loss: 4.6002 - acc: 0.0394 - val_loss: 4.6618 - val_acc: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 4.69626 to 4.66183, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 15s - loss: 4.5954 - acc: 0.0000e+ - ETA: 15s - loss: 4.5100 - acc: 0.0333   - ETA: 15s - loss: 4.5387 - acc: 0.07 - ETA: 15s - loss: 4.5349 - acc: 0.08 - ETA: 14s - loss: 4.5437 - acc: 0.07 - ETA: 15s - loss: 4.5423 - acc: 0.06 - ETA: 15s - loss: 4.5427 - acc: 0.05 - ETA: 15s - loss: 4.5466 - acc: 0.05 - ETA: 15s - loss: 4.5425 - acc: 0.05 - ETA: 15s - loss: 4.5397 - acc: 0.04 - ETA: 15s - loss: 4.5403 - acc: 0.04 - ETA: 15s - loss: 4.5370 - acc: 0.04 - ETA: 15s - loss: 4.5369 - acc: 0.04 - ETA: 15s - loss: 4.5454 - acc: 0.04 - ETA: 15s - loss: 4.5406 - acc: 0.04 - ETA: 14s - loss: 4.5184 - acc: 0.04 - ETA: 14s - loss: 4.5187 - acc: 0.04 - ETA: 14s - loss: 4.5169 - acc: 0.04 - ETA: 14s - loss: 4.5231 - acc: 0.04 - ETA: 14s - loss: 4.5235 - acc: 0.04 - ETA: 14s - loss: 4.5255 - acc: 0.04 - ETA: 14s - loss: 4.5385 - acc: 0.04 - ETA: 14s - loss: 4.5334 - acc: 0.04 - ETA: 14s - loss: 4.5185 - acc: 0.04 - ETA: 14s - loss: 4.5239 - acc: 0.04 - ETA: 14s - loss: 4.5149 - acc: 0.04 - ETA: 14s - loss: 4.5171 - acc: 0.04 - ETA: 14s - loss: 4.5102 - acc: 0.04 - ETA: 14s - loss: 4.5156 - acc: 0.04 - ETA: 14s - loss: 4.5205 - acc: 0.04 - ETA: 14s - loss: 4.5181 - acc: 0.04 - ETA: 14s - loss: 4.5173 - acc: 0.04 - ETA: 14s - loss: 4.5231 - acc: 0.04 - ETA: 14s - loss: 4.5270 - acc: 0.04 - ETA: 13s - loss: 4.5249 - acc: 0.04 - ETA: 13s - loss: 4.5279 - acc: 0.04 - ETA: 13s - loss: 4.5282 - acc: 0.03 - ETA: 13s - loss: 4.5371 - acc: 0.03 - ETA: 13s - loss: 4.5358 - acc: 0.04 - ETA: 13s - loss: 4.5333 - acc: 0.03 - ETA: 13s - loss: 4.5369 - acc: 0.03 - ETA: 13s - loss: 4.5361 - acc: 0.04 - ETA: 13s - loss: 4.5393 - acc: 0.04 - ETA: 13s - loss: 4.5380 - acc: 0.04 - ETA: 13s - loss: 4.5413 - acc: 0.04 - ETA: 12s - loss: 4.5432 - acc: 0.04 - ETA: 12s - loss: 4.5381 - acc: 0.04 - ETA: 12s - loss: 4.5417 - acc: 0.04 - ETA: 12s - loss: 4.5458 - acc: 0.04 - ETA: 12s - loss: 4.5477 - acc: 0.04 - ETA: 12s - loss: 4.5480 - acc: 0.04 - ETA: 12s - loss: 4.5506 - acc: 0.04 - ETA: 12s - loss: 4.5483 - acc: 0.04 - ETA: 12s - loss: 4.5474 - acc: 0.04 - ETA: 12s - loss: 4.5433 - acc: 0.04 - ETA: 12s - loss: 4.5443 - acc: 0.04 - ETA: 12s - loss: 4.5425 - acc: 0.03 - ETA: 12s - loss: 4.5477 - acc: 0.03 - ETA: 12s - loss: 4.5511 - acc: 0.03 - ETA: 12s - loss: 4.5544 - acc: 0.03 - ETA: 12s - loss: 4.5574 - acc: 0.03 - ETA: 11s - loss: 4.5574 - acc: 0.03 - ETA: 11s - loss: 4.5571 - acc: 0.03 - ETA: 11s - loss: 4.5617 - acc: 0.03 - ETA: 11s - loss: 4.5622 - acc: 0.03 - ETA: 11s - loss: 4.5615 - acc: 0.03 - ETA: 11s - loss: 4.5641 - acc: 0.03 - ETA: 11s - loss: 4.5643 - acc: 0.03 - ETA: 11s - loss: 4.5656 - acc: 0.03 - ETA: 11s - loss: 4.5648 - acc: 0.04 - ETA: 11s - loss: 4.5660 - acc: 0.04 - ETA: 11s - loss: 4.5664 - acc: 0.03 - ETA: 11s - loss: 4.5640 - acc: 0.04 - ETA: 10s - loss: 4.5671 - acc: 0.03 - ETA: 10s - loss: 4.5680 - acc: 0.04 - ETA: 10s - loss: 4.5677 - acc: 0.04 - ETA: 10s - loss: 4.5688 - acc: 0.04 - ETA: 10s - loss: 4.5684 - acc: 0.04 - ETA: 10s - loss: 4.5734 - acc: 0.04 - ETA: 10s - loss: 4.5745 - acc: 0.04 - ETA: 10s - loss: 4.5732 - acc: 0.04 - ETA: 10s - loss: 4.5703 - acc: 0.04 - ETA: 10s - loss: 4.5701 - acc: 0.04 - ETA: 10s - loss: 4.5720 - acc: 0.04 - ETA: 10s - loss: 4.5761 - acc: 0.03 - ETA: 9s - loss: 4.5749 - acc: 0.0400 - ETA: 9s - loss: 4.5726 - acc: 0.041 - ETA: 9s - loss: 4.5736 - acc: 0.041 - ETA: 9s - loss: 4.5739 - acc: 0.040 - ETA: 9s - loss: 4.5743 - acc: 0.040 - ETA: 9s - loss: 4.5733 - acc: 0.041 - ETA: 9s - loss: 4.5739 - acc: 0.041 - ETA: 9s - loss: 4.5727 - acc: 0.043 - ETA: 9s - loss: 4.5736 - acc: 0.042 - ETA: 9s - loss: 4.5734 - acc: 0.042 - ETA: 9s - loss: 4.5717 - acc: 0.043 - ETA: 9s - loss: 4.5713 - acc: 0.043 - ETA: 9s - loss: 4.5703 - acc: 0.043 - ETA: 8s - loss: 4.5722 - acc: 0.043 - ETA: 8s - loss: 4.5707 - acc: 0.043 - ETA: 8s - loss: 4.5712 - acc: 0.044 - ETA: 8s - loss: 4.5688 - acc: 0.044 - ETA: 8s - loss: 4.5674 - acc: 0.044 - ETA: 8s - loss: 4.5664 - acc: 0.044 - ETA: 8s - loss: 4.5660 - acc: 0.043 - ETA: 8s - loss: 4.5659 - acc: 0.043 - ETA: 8s - loss: 4.5671 - acc: 0.043 - ETA: 8s - loss: 4.5653 - acc: 0.043 - ETA: 8s - loss: 4.5652 - acc: 0.043 - ETA: 8s - loss: 4.5650 - acc: 0.043 - ETA: 8s - loss: 4.5637 - acc: 0.044 - ETA: 7s - loss: 4.5643 - acc: 0.044 - ETA: 7s - loss: 4.5647 - acc: 0.043 - ETA: 7s - loss: 4.5648 - acc: 0.043 - ETA: 7s - loss: 4.5649 - acc: 0.043 - ETA: 7s - loss: 4.5645 - acc: 0.044 - ETA: 7s - loss: 4.5659 - acc: 0.044 - ETA: 7s - loss: 4.5652 - acc: 0.044 - ETA: 7s - loss: 4.5663 - acc: 0.043 - ETA: 7s - loss: 4.5650 - acc: 0.044 - ETA: 7s - loss: 4.5661 - acc: 0.043 - ETA: 6s - loss: 4.5661 - acc: 0.044 - ETA: 6s - loss: 4.5661 - acc: 0.043 - ETA: 6s - loss: 4.5673 - acc: 0.043 - ETA: 6s - loss: 4.5690 - acc: 0.043 - ETA: 6s - loss: 4.5710 - acc: 0.043 - ETA: 6s - loss: 4.5717 - acc: 0.043 - ETA: 6s - loss: 4.5702 - acc: 0.043 - ETA: 6s - loss: 4.5688 - acc: 0.043 - ETA: 6s - loss: 4.5699 - acc: 0.043 - ETA: 6s - loss: 4.5692 - acc: 0.044 - ETA: 6s - loss: 4.5692 - acc: 0.043 - ETA: 6s - loss: 4.5682 - acc: 0.043 - ETA: 6s - loss: 4.5687 - acc: 0.043 - ETA: 5s - loss: 4.5683 - acc: 0.043 - ETA: 5s - loss: 4.5698 - acc: 0.043 - ETA: 5s - loss: 4.5710 - acc: 0.043 - ETA: 5s - loss: 4.5706 - acc: 0.043 - ETA: 5s - loss: 4.5704 - acc: 0.043 - ETA: 5s - loss: 4.5701 - acc: 0.043 - ETA: 5s - loss: 4.5702 - acc: 0.044 - ETA: 5s - loss: 4.5691 - acc: 0.043 - ETA: 5s - loss: 4.5695 - acc: 0.043 - ETA: 5s - loss: 4.5700 - acc: 0.043 - ETA: 5s - loss: 4.5700 - acc: 0.043 - ETA: 5s - loss: 4.5695 - acc: 0.044 - ETA: 5s - loss: 4.5719 - acc: 0.044 - ETA: 4s - loss: 4.5717 - acc: 0.044 - ETA: 4s - loss: 4.5712 - acc: 0.043 - ETA: 4s - loss: 4.5707 - acc: 0.043 - ETA: 4s - loss: 4.5707 - acc: 0.043 - ETA: 4s - loss: 4.5709 - acc: 0.043 - ETA: 4s - loss: 4.5705 - acc: 0.043 - ETA: 4s - loss: 4.5700 - acc: 0.044 - ETA: 4s - loss: 4.5688 - acc: 0.044 - ETA: 4s - loss: 4.5692 - acc: 0.044 - ETA: 4s - loss: 4.5678 - acc: 0.044 - ETA: 4s - loss: 4.5699 - acc: 0.045 - ETA: 4s - loss: 4.5718 - acc: 0.044 - ETA: 4s - loss: 4.5728 - acc: 0.044 - ETA: 4s - loss: 4.5727 - acc: 0.044 - ETA: 3s - loss: 4.5718 - acc: 0.044 - ETA: 3s - loss: 4.5705 - acc: 0.044 - ETA: 3s - loss: 4.5707 - acc: 0.044 - ETA: 3s - loss: 4.5721 - acc: 0.043 - ETA: 3s - loss: 4.5726 - acc: 0.043 - ETA: 3s - loss: 4.5726 - acc: 0.043 - ETA: 3s - loss: 4.5727 - acc: 0.043 - ETA: 3s - loss: 4.5706 - acc: 0.044 - ETA: 3s - loss: 4.5716 - acc: 0.043 - ETA: 3s - loss: 4.5721 - acc: 0.044 - ETA: 3s - loss: 4.5708 - acc: 0.044 - ETA: 3s - loss: 4.5721 - acc: 0.044 - ETA: 2s - loss: 4.5739 - acc: 0.044 - ETA: 2s - loss: 4.5737 - acc: 0.044 - ETA: 2s - loss: 4.5729 - acc: 0.044 - ETA: 2s - loss: 4.5718 - acc: 0.044 - ETA: 2s - loss: 4.5710 - acc: 0.044 - ETA: 2s - loss: 4.5730 - acc: 0.044 - ETA: 2s - loss: 4.5737 - acc: 0.044 - ETA: 2s - loss: 4.5740 - acc: 0.043 - ETA: 2s - loss: 4.5746 - acc: 0.043 - ETA: 2s - loss: 4.5736 - acc: 0.044 - ETA: 2s - loss: 4.5743 - acc: 0.044 - ETA: 2s - loss: 4.5736 - acc: 0.044 - ETA: 2s - loss: 4.5727 - acc: 0.044 - ETA: 2s - loss: 4.5740 - acc: 0.044 - ETA: 1s - loss: 4.5740 - acc: 0.044 - ETA: 1s - loss: 4.5740 - acc: 0.044 - ETA: 1s - loss: 4.5741 - acc: 0.044 - ETA: 1s - loss: 4.5749 - acc: 0.044 - ETA: 1s - loss: 4.5749 - acc: 0.044 - ETA: 1s - loss: 4.5744 - acc: 0.044 - ETA: 1s - loss: 4.5748 - acc: 0.044 - ETA: 1s - loss: 4.5752 - acc: 0.044 - ETA: 1s - loss: 4.5744 - acc: 0.044 - ETA: 1s - loss: 4.5753 - acc: 0.045 - ETA: 1s - loss: 4.5760 - acc: 0.045 - ETA: 1s - loss: 4.5770 - acc: 0.044 - ETA: 1s - loss: 4.5772 - acc: 0.044 - ETA: 0s - loss: 4.5770 - acc: 0.044 - ETA: 0s - loss: 4.5763 - acc: 0.044 - ETA: 0s - loss: 4.5767 - acc: 0.044 - ETA: 0s - loss: 4.5776 - acc: 0.044 - ETA: 0s - loss: 4.5779 - acc: 0.044 - ETA: 0s - loss: 4.5782 - acc: 0.044 - ETA: 0s - loss: 4.5782 - acc: 0.044 - ETA: 0s - loss: 4.5779 - acc: 0.044 - ETA: 0s - loss: 4.5771 - acc: 0.044 - ETA: 0s - loss: 4.5764 - acc: 0.044 - ETA: 0s - loss: 4.5757 - acc: 0.044 - ETA: 0s - loss: 4.5761 - acc: 0.044 - ETA: 0s - loss: 4.5762 - acc: 0.044 - ETA: 0s - loss: 4.5753 - acc: 0.044 - 17s 3ms/step - loss: 4.5752 - acc: 0.0440 - val_loss: 4.7053 - val_acc: 0.0299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss did not improve from 4.66183\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 19s - loss: 4.5977 - acc: 0.05 - ETA: 18s - loss: 4.7408 - acc: 0.02 - ETA: 17s - loss: 4.6642 - acc: 0.03 - ETA: 16s - loss: 4.6798 - acc: 0.03 - ETA: 16s - loss: 4.6979 - acc: 0.02 - ETA: 16s - loss: 4.6805 - acc: 0.02 - ETA: 15s - loss: 4.6795 - acc: 0.02 - ETA: 15s - loss: 4.6420 - acc: 0.02 - ETA: 15s - loss: 4.6112 - acc: 0.03 - ETA: 15s - loss: 4.6097 - acc: 0.03 - ETA: 15s - loss: 4.6090 - acc: 0.03 - ETA: 15s - loss: 4.6068 - acc: 0.03 - ETA: 15s - loss: 4.6061 - acc: 0.03 - ETA: 14s - loss: 4.5943 - acc: 0.03 - ETA: 14s - loss: 4.5854 - acc: 0.03 - ETA: 14s - loss: 4.5785 - acc: 0.03 - ETA: 14s - loss: 4.5723 - acc: 0.03 - ETA: 14s - loss: 4.5703 - acc: 0.03 - ETA: 14s - loss: 4.5654 - acc: 0.03 - ETA: 14s - loss: 4.5696 - acc: 0.03 - ETA: 14s - loss: 4.5622 - acc: 0.03 - ETA: 14s - loss: 4.5749 - acc: 0.03 - ETA: 14s - loss: 4.5716 - acc: 0.03 - ETA: 14s - loss: 4.5767 - acc: 0.03 - ETA: 14s - loss: 4.5740 - acc: 0.03 - ETA: 14s - loss: 4.5725 - acc: 0.03 - ETA: 13s - loss: 4.5834 - acc: 0.03 - ETA: 13s - loss: 4.5855 - acc: 0.03 - ETA: 13s - loss: 4.5836 - acc: 0.03 - ETA: 13s - loss: 4.5742 - acc: 0.03 - ETA: 13s - loss: 4.5739 - acc: 0.03 - ETA: 13s - loss: 4.5755 - acc: 0.03 - ETA: 13s - loss: 4.5762 - acc: 0.03 - ETA: 13s - loss: 4.5749 - acc: 0.03 - ETA: 13s - loss: 4.5714 - acc: 0.03 - ETA: 13s - loss: 4.5713 - acc: 0.03 - ETA: 13s - loss: 4.5711 - acc: 0.03 - ETA: 13s - loss: 4.5732 - acc: 0.03 - ETA: 13s - loss: 4.5707 - acc: 0.03 - ETA: 13s - loss: 4.5715 - acc: 0.03 - ETA: 12s - loss: 4.5701 - acc: 0.03 - ETA: 12s - loss: 4.5747 - acc: 0.03 - ETA: 12s - loss: 4.5714 - acc: 0.03 - ETA: 12s - loss: 4.5670 - acc: 0.03 - ETA: 12s - loss: 4.5641 - acc: 0.03 - ETA: 12s - loss: 4.5663 - acc: 0.03 - ETA: 12s - loss: 4.5649 - acc: 0.03 - ETA: 12s - loss: 4.5638 - acc: 0.03 - ETA: 12s - loss: 4.5661 - acc: 0.03 - ETA: 12s - loss: 4.5698 - acc: 0.03 - ETA: 12s - loss: 4.5677 - acc: 0.03 - ETA: 12s - loss: 4.5680 - acc: 0.03 - ETA: 12s - loss: 4.5702 - acc: 0.03 - ETA: 12s - loss: 4.5697 - acc: 0.03 - ETA: 12s - loss: 4.5675 - acc: 0.03 - ETA: 12s - loss: 4.5642 - acc: 0.03 - ETA: 12s - loss: 4.5672 - acc: 0.03 - ETA: 11s - loss: 4.5685 - acc: 0.03 - ETA: 11s - loss: 4.5705 - acc: 0.03 - ETA: 11s - loss: 4.5732 - acc: 0.03 - ETA: 11s - loss: 4.5712 - acc: 0.03 - ETA: 11s - loss: 4.5691 - acc: 0.03 - ETA: 11s - loss: 4.5637 - acc: 0.03 - ETA: 11s - loss: 4.5671 - acc: 0.03 - ETA: 11s - loss: 4.5688 - acc: 0.03 - ETA: 11s - loss: 4.5697 - acc: 0.03 - ETA: 11s - loss: 4.5724 - acc: 0.03 - ETA: 11s - loss: 4.5728 - acc: 0.03 - ETA: 11s - loss: 4.5714 - acc: 0.03 - ETA: 11s - loss: 4.5717 - acc: 0.03 - ETA: 11s - loss: 4.5707 - acc: 0.03 - ETA: 10s - loss: 4.5684 - acc: 0.03 - ETA: 10s - loss: 4.5684 - acc: 0.03 - ETA: 10s - loss: 4.5679 - acc: 0.03 - ETA: 10s - loss: 4.5664 - acc: 0.03 - ETA: 10s - loss: 4.5673 - acc: 0.03 - ETA: 10s - loss: 4.5700 - acc: 0.03 - ETA: 10s - loss: 4.5679 - acc: 0.03 - ETA: 10s - loss: 4.5661 - acc: 0.03 - ETA: 10s - loss: 4.5655 - acc: 0.03 - ETA: 10s - loss: 4.5651 - acc: 0.03 - ETA: 10s - loss: 4.5645 - acc: 0.03 - ETA: 10s - loss: 4.5648 - acc: 0.03 - ETA: 10s - loss: 4.5634 - acc: 0.03 - ETA: 10s - loss: 4.5642 - acc: 0.03 - ETA: 9s - loss: 4.5613 - acc: 0.0376 - ETA: 9s - loss: 4.5609 - acc: 0.038 - ETA: 9s - loss: 4.5627 - acc: 0.038 - ETA: 9s - loss: 4.5634 - acc: 0.037 - ETA: 9s - loss: 4.5630 - acc: 0.037 - ETA: 9s - loss: 4.5634 - acc: 0.038 - ETA: 9s - loss: 4.5620 - acc: 0.038 - ETA: 9s - loss: 4.5629 - acc: 0.038 - ETA: 9s - loss: 4.5615 - acc: 0.038 - ETA: 9s - loss: 4.5618 - acc: 0.038 - ETA: 9s - loss: 4.5621 - acc: 0.038 - ETA: 9s - loss: 4.5613 - acc: 0.038 - ETA: 9s - loss: 4.5597 - acc: 0.039 - ETA: 9s - loss: 4.5597 - acc: 0.039 - ETA: 9s - loss: 4.5584 - acc: 0.039 - ETA: 8s - loss: 4.5556 - acc: 0.039 - ETA: 8s - loss: 4.5566 - acc: 0.039 - ETA: 8s - loss: 4.5571 - acc: 0.039 - ETA: 8s - loss: 4.5549 - acc: 0.040 - ETA: 8s - loss: 4.5537 - acc: 0.040 - ETA: 8s - loss: 4.5535 - acc: 0.040 - ETA: 8s - loss: 4.5530 - acc: 0.040 - ETA: 8s - loss: 4.5536 - acc: 0.040 - ETA: 8s - loss: 4.5523 - acc: 0.041 - ETA: 8s - loss: 4.5520 - acc: 0.041 - ETA: 8s - loss: 4.5540 - acc: 0.041 - ETA: 8s - loss: 4.5555 - acc: 0.041 - ETA: 8s - loss: 4.5550 - acc: 0.041 - ETA: 8s - loss: 4.5562 - acc: 0.041 - ETA: 7s - loss: 4.5549 - acc: 0.041 - ETA: 7s - loss: 4.5584 - acc: 0.041 - ETA: 7s - loss: 4.5573 - acc: 0.040 - ETA: 7s - loss: 4.5552 - acc: 0.040 - ETA: 7s - loss: 4.5536 - acc: 0.041 - ETA: 7s - loss: 4.5527 - acc: 0.041 - ETA: 7s - loss: 4.5527 - acc: 0.041 - ETA: 7s - loss: 4.5545 - acc: 0.040 - ETA: 7s - loss: 4.5571 - acc: 0.040 - ETA: 7s - loss: 4.5557 - acc: 0.040 - ETA: 7s - loss: 4.5545 - acc: 0.041 - ETA: 7s - loss: 4.5537 - acc: 0.040 - ETA: 7s - loss: 4.5535 - acc: 0.040 - ETA: 7s - loss: 4.5526 - acc: 0.040 - ETA: 6s - loss: 4.5511 - acc: 0.042 - ETA: 6s - loss: 4.5518 - acc: 0.042 - ETA: 6s - loss: 4.5500 - acc: 0.042 - ETA: 6s - loss: 4.5484 - acc: 0.043 - ETA: 6s - loss: 4.5486 - acc: 0.043 - ETA: 6s - loss: 4.5504 - acc: 0.043 - ETA: 6s - loss: 4.5513 - acc: 0.042 - ETA: 6s - loss: 4.5506 - acc: 0.043 - ETA: 6s - loss: 4.5507 - acc: 0.042 - ETA: 6s - loss: 4.5501 - acc: 0.042 - ETA: 6s - loss: 4.5510 - acc: 0.042 - ETA: 6s - loss: 4.5498 - acc: 0.042 - ETA: 6s - loss: 4.5503 - acc: 0.042 - ETA: 5s - loss: 4.5500 - acc: 0.042 - ETA: 5s - loss: 4.5515 - acc: 0.042 - ETA: 5s - loss: 4.5520 - acc: 0.042 - ETA: 5s - loss: 4.5523 - acc: 0.042 - ETA: 5s - loss: 4.5532 - acc: 0.042 - ETA: 5s - loss: 4.5539 - acc: 0.042 - ETA: 5s - loss: 4.5557 - acc: 0.042 - ETA: 5s - loss: 4.5548 - acc: 0.042 - ETA: 5s - loss: 4.5546 - acc: 0.042 - ETA: 5s - loss: 4.5562 - acc: 0.041 - ETA: 5s - loss: 4.5576 - acc: 0.041 - ETA: 4s - loss: 4.5579 - acc: 0.041 - ETA: 4s - loss: 4.5572 - acc: 0.041 - ETA: 4s - loss: 4.5568 - acc: 0.042 - ETA: 4s - loss: 4.5556 - acc: 0.041 - ETA: 4s - loss: 4.5566 - acc: 0.041 - ETA: 4s - loss: 4.5568 - acc: 0.041 - ETA: 4s - loss: 4.5571 - acc: 0.041 - ETA: 4s - loss: 4.5575 - acc: 0.042 - ETA: 4s - loss: 4.5567 - acc: 0.042 - ETA: 4s - loss: 4.5561 - acc: 0.041 - ETA: 4s - loss: 4.5551 - acc: 0.042 - ETA: 4s - loss: 4.5561 - acc: 0.041 - ETA: 4s - loss: 4.5552 - acc: 0.042 - ETA: 3s - loss: 4.5549 - acc: 0.042 - ETA: 3s - loss: 4.5543 - acc: 0.041 - ETA: 3s - loss: 4.5534 - acc: 0.042 - ETA: 3s - loss: 4.5528 - acc: 0.042 - ETA: 3s - loss: 4.5526 - acc: 0.042 - ETA: 3s - loss: 4.5518 - acc: 0.042 - ETA: 3s - loss: 4.5524 - acc: 0.042 - ETA: 3s - loss: 4.5518 - acc: 0.042 - ETA: 3s - loss: 4.5499 - acc: 0.042 - ETA: 3s - loss: 4.5503 - acc: 0.042 - ETA: 3s - loss: 4.5498 - acc: 0.042 - ETA: 3s - loss: 4.5493 - acc: 0.042 - ETA: 2s - loss: 4.5491 - acc: 0.042 - ETA: 2s - loss: 4.5484 - acc: 0.042 - ETA: 2s - loss: 4.5486 - acc: 0.042 - ETA: 2s - loss: 4.5484 - acc: 0.042 - ETA: 2s - loss: 4.5492 - acc: 0.042 - ETA: 2s - loss: 4.5488 - acc: 0.042 - ETA: 2s - loss: 4.5493 - acc: 0.042 - ETA: 2s - loss: 4.5492 - acc: 0.042 - ETA: 2s - loss: 4.5490 - acc: 0.042 - ETA: 2s - loss: 4.5488 - acc: 0.042 - ETA: 2s - loss: 4.5470 - acc: 0.042 - ETA: 2s - loss: 4.5476 - acc: 0.043 - ETA: 2s - loss: 4.5474 - acc: 0.042 - ETA: 2s - loss: 4.5477 - acc: 0.042 - ETA: 1s - loss: 4.5479 - acc: 0.042 - ETA: 1s - loss: 4.5474 - acc: 0.042 - ETA: 1s - loss: 4.5470 - acc: 0.042 - ETA: 1s - loss: 4.5469 - acc: 0.043 - ETA: 1s - loss: 4.5466 - acc: 0.043 - ETA: 1s - loss: 4.5470 - acc: 0.042 - ETA: 1s - loss: 4.5455 - acc: 0.043 - ETA: 1s - loss: 4.5465 - acc: 0.042 - ETA: 1s - loss: 4.5463 - acc: 0.042 - ETA: 1s - loss: 4.5467 - acc: 0.042 - ETA: 1s - loss: 4.5474 - acc: 0.042 - ETA: 1s - loss: 4.5471 - acc: 0.042 - ETA: 1s - loss: 4.5464 - acc: 0.042 - ETA: 0s - loss: 4.5473 - acc: 0.042 - ETA: 0s - loss: 4.5468 - acc: 0.042 - ETA: 0s - loss: 4.5471 - acc: 0.042 - ETA: 0s - loss: 4.5468 - acc: 0.042 - ETA: 0s - loss: 4.5454 - acc: 0.042 - ETA: 0s - loss: 4.5457 - acc: 0.042 - ETA: 0s - loss: 4.5461 - acc: 0.042 - ETA: 0s - loss: 4.5458 - acc: 0.042 - ETA: 0s - loss: 4.5461 - acc: 0.0425\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.5469 - acc: 0.042 - ETA: 0s - loss: 4.5467 - acc: 0.042 - ETA: 0s - loss: 4.5468 - acc: 0.042 - 17s 3ms/step - loss: 4.5459 - acc: 0.0427 - val_loss: 4.6295 - val_acc: 0.0395\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.66183 to 4.62954, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300/6680 [======================>.......] - ETA: 14s - loss: 4.3049 - acc: 0.0000e+ - ETA: 15s - loss: 4.3998 - acc: 0.0333   - ETA: 15s - loss: 4.4297 - acc: 0.05 - ETA: 15s - loss: 4.4493 - acc: 0.05 - ETA: 15s - loss: 4.4330 - acc: 0.05 - ETA: 15s - loss: 4.4261 - acc: 0.05 - ETA: 15s - loss: 4.4456 - acc: 0.05 - ETA: 15s - loss: 4.4525 - acc: 0.05 - ETA: 14s - loss: 4.4524 - acc: 0.04 - ETA: 14s - loss: 4.4573 - acc: 0.04 - ETA: 14s - loss: 4.4665 - acc: 0.04 - ETA: 14s - loss: 4.4773 - acc: 0.04 - ETA: 14s - loss: 4.4780 - acc: 0.05 - ETA: 14s - loss: 4.4827 - acc: 0.04 - ETA: 14s - loss: 4.4836 - acc: 0.04 - ETA: 14s - loss: 4.4844 - acc: 0.05 - ETA: 14s - loss: 4.4884 - acc: 0.05 - ETA: 14s - loss: 4.4857 - acc: 0.05 - ETA: 14s - loss: 4.4873 - acc: 0.05 - ETA: 14s - loss: 4.4895 - acc: 0.04 - ETA: 14s - loss: 4.4871 - acc: 0.04 - ETA: 14s - loss: 4.4841 - acc: 0.05 - ETA: 14s - loss: 4.4941 - acc: 0.04 - ETA: 14s - loss: 4.4981 - acc: 0.04 - ETA: 14s - loss: 4.4980 - acc: 0.04 - ETA: 13s - loss: 4.4958 - acc: 0.05 - ETA: 13s - loss: 4.4896 - acc: 0.05 - ETA: 13s - loss: 4.4878 - acc: 0.04 - ETA: 13s - loss: 4.4875 - acc: 0.04 - ETA: 13s - loss: 4.4941 - acc: 0.04 - ETA: 13s - loss: 4.4884 - acc: 0.04 - ETA: 13s - loss: 4.4929 - acc: 0.04 - ETA: 13s - loss: 4.4991 - acc: 0.04 - ETA: 13s - loss: 4.5012 - acc: 0.04 - ETA: 13s - loss: 4.4986 - acc: 0.05 - ETA: 13s - loss: 4.4913 - acc: 0.05 - ETA: 13s - loss: 4.4947 - acc: 0.05 - ETA: 13s - loss: 4.4981 - acc: 0.05 - ETA: 13s - loss: 4.4984 - acc: 0.05 - ETA: 13s - loss: 4.5042 - acc: 0.05 - ETA: 13s - loss: 4.5027 - acc: 0.05 - ETA: 13s - loss: 4.5022 - acc: 0.05 - ETA: 13s - loss: 4.5067 - acc: 0.05 - ETA: 13s - loss: 4.5051 - acc: 0.04 - ETA: 13s - loss: 4.5043 - acc: 0.05 - ETA: 13s - loss: 4.5078 - acc: 0.05 - ETA: 13s - loss: 4.5087 - acc: 0.05 - ETA: 13s - loss: 4.5054 - acc: 0.05 - ETA: 13s - loss: 4.5073 - acc: 0.05 - ETA: 13s - loss: 4.5042 - acc: 0.05 - ETA: 13s - loss: 4.5034 - acc: 0.05 - ETA: 13s - loss: 4.5053 - acc: 0.05 - ETA: 13s - loss: 4.5019 - acc: 0.05 - ETA: 13s - loss: 4.5080 - acc: 0.05 - ETA: 13s - loss: 4.5089 - acc: 0.05 - ETA: 13s - loss: 4.5108 - acc: 0.05 - ETA: 12s - loss: 4.5140 - acc: 0.05 - ETA: 12s - loss: 4.5157 - acc: 0.05 - ETA: 12s - loss: 4.5155 - acc: 0.05 - ETA: 12s - loss: 4.5187 - acc: 0.04 - ETA: 12s - loss: 4.5231 - acc: 0.04 - ETA: 12s - loss: 4.5217 - acc: 0.04 - ETA: 12s - loss: 4.5200 - acc: 0.04 - ETA: 12s - loss: 4.5162 - acc: 0.04 - ETA: 12s - loss: 4.5160 - acc: 0.04 - ETA: 12s - loss: 4.5150 - acc: 0.04 - ETA: 12s - loss: 4.5169 - acc: 0.04 - ETA: 12s - loss: 4.5179 - acc: 0.04 - ETA: 12s - loss: 4.5169 - acc: 0.04 - ETA: 12s - loss: 4.5178 - acc: 0.04 - ETA: 12s - loss: 4.5151 - acc: 0.05 - ETA: 12s - loss: 4.5177 - acc: 0.04 - ETA: 12s - loss: 4.5182 - acc: 0.04 - ETA: 12s - loss: 4.5185 - acc: 0.05 - ETA: 12s - loss: 4.5177 - acc: 0.05 - ETA: 12s - loss: 4.5151 - acc: 0.05 - ETA: 12s - loss: 4.5174 - acc: 0.05 - ETA: 12s - loss: 4.5173 - acc: 0.05 - ETA: 12s - loss: 4.5153 - acc: 0.05 - ETA: 12s - loss: 4.5162 - acc: 0.05 - ETA: 12s - loss: 4.5147 - acc: 0.05 - ETA: 12s - loss: 4.5158 - acc: 0.05 - ETA: 12s - loss: 4.5158 - acc: 0.05 - ETA: 12s - loss: 4.5163 - acc: 0.05 - ETA: 12s - loss: 4.5170 - acc: 0.05 - ETA: 12s - loss: 4.5172 - acc: 0.05 - ETA: 12s - loss: 4.5200 - acc: 0.05 - ETA: 11s - loss: 4.5193 - acc: 0.05 - ETA: 11s - loss: 4.5171 - acc: 0.05 - ETA: 12s - loss: 4.5161 - acc: 0.05 - ETA: 12s - loss: 4.5156 - acc: 0.05 - ETA: 11s - loss: 4.5169 - acc: 0.05 - ETA: 11s - loss: 4.5190 - acc: 0.05 - ETA: 11s - loss: 4.5214 - acc: 0.05 - ETA: 11s - loss: 4.5215 - acc: 0.05 - ETA: 11s - loss: 4.5219 - acc: 0.05 - ETA: 11s - loss: 4.5228 - acc: 0.05 - ETA: 11s - loss: 4.5231 - acc: 0.05 - ETA: 11s - loss: 4.5252 - acc: 0.05 - ETA: 11s - loss: 4.5266 - acc: 0.05 - ETA: 11s - loss: 4.5247 - acc: 0.05 - ETA: 11s - loss: 4.5236 - acc: 0.05 - ETA: 11s - loss: 4.5220 - acc: 0.05 - ETA: 11s - loss: 4.5229 - acc: 0.05 - ETA: 11s - loss: 4.5216 - acc: 0.05 - ETA: 11s - loss: 4.5195 - acc: 0.05 - ETA: 11s - loss: 4.5194 - acc: 0.05 - ETA: 11s - loss: 4.5175 - acc: 0.05 - ETA: 11s - loss: 4.5182 - acc: 0.05 - ETA: 11s - loss: 4.5158 - acc: 0.05 - ETA: 11s - loss: 4.5171 - acc: 0.05 - ETA: 11s - loss: 4.5164 - acc: 0.05 - ETA: 11s - loss: 4.5167 - acc: 0.05 - ETA: 10s - loss: 4.5156 - acc: 0.05 - ETA: 10s - loss: 4.5146 - acc: 0.05 - ETA: 10s - loss: 4.5126 - acc: 0.05 - ETA: 10s - loss: 4.5130 - acc: 0.05 - ETA: 10s - loss: 4.5149 - acc: 0.05 - ETA: 10s - loss: 4.5166 - acc: 0.05 - ETA: 10s - loss: 4.5177 - acc: 0.05 - ETA: 10s - loss: 4.5195 - acc: 0.05 - ETA: 10s - loss: 4.5194 - acc: 0.05 - ETA: 10s - loss: 4.5199 - acc: 0.05 - ETA: 10s - loss: 4.5197 - acc: 0.05 - ETA: 10s - loss: 4.5203 - acc: 0.05 - ETA: 10s - loss: 4.5225 - acc: 0.05 - ETA: 10s - loss: 4.5199 - acc: 0.05 - ETA: 10s - loss: 4.5208 - acc: 0.05 - ETA: 10s - loss: 4.5206 - acc: 0.05 - ETA: 9s - loss: 4.5213 - acc: 0.0519 - ETA: 9s - loss: 4.5196 - acc: 0.053 - ETA: 9s - loss: 4.5187 - acc: 0.053 - ETA: 9s - loss: 4.5190 - acc: 0.053 - ETA: 9s - loss: 4.5199 - acc: 0.053 - ETA: 9s - loss: 4.5214 - acc: 0.053 - ETA: 9s - loss: 4.5225 - acc: 0.053 - ETA: 9s - loss: 4.5217 - acc: 0.054 - ETA: 9s - loss: 4.5220 - acc: 0.054 - ETA: 9s - loss: 4.5221 - acc: 0.053 - ETA: 9s - loss: 4.5214 - acc: 0.054 - ETA: 9s - loss: 4.5229 - acc: 0.054 - ETA: 9s - loss: 4.5219 - acc: 0.054 - ETA: 9s - loss: 4.5207 - acc: 0.054 - ETA: 9s - loss: 4.5219 - acc: 0.054 - ETA: 9s - loss: 4.5198 - acc: 0.054 - ETA: 8s - loss: 4.5192 - acc: 0.054 - ETA: 8s - loss: 4.5200 - acc: 0.053 - ETA: 8s - loss: 4.5221 - acc: 0.053 - ETA: 8s - loss: 4.5237 - acc: 0.053 - ETA: 8s - loss: 4.5235 - acc: 0.052 - ETA: 8s - loss: 4.5251 - acc: 0.052 - ETA: 8s - loss: 4.5250 - acc: 0.053 - ETA: 8s - loss: 4.5236 - acc: 0.054 - ETA: 8s - loss: 4.5237 - acc: 0.054 - ETA: 8s - loss: 4.5225 - acc: 0.054 - ETA: 8s - loss: 4.5217 - acc: 0.054 - ETA: 8s - loss: 4.5216 - acc: 0.054 - ETA: 8s - loss: 4.5216 - acc: 0.054 - ETA: 8s - loss: 4.5214 - acc: 0.053 - ETA: 7s - loss: 4.5215 - acc: 0.053 - ETA: 7s - loss: 4.5207 - acc: 0.053 - ETA: 7s - loss: 4.5211 - acc: 0.054 - ETA: 7s - loss: 4.5207 - acc: 0.053 - ETA: 7s - loss: 4.5210 - acc: 0.053 - ETA: 7s - loss: 4.5210 - acc: 0.053 - ETA: 7s - loss: 4.5190 - acc: 0.053 - ETA: 7s - loss: 4.5200 - acc: 0.053 - ETA: 7s - loss: 4.5195 - acc: 0.053 - ETA: 7s - loss: 4.5197 - acc: 0.053 - ETA: 7s - loss: 4.5176 - acc: 0.053 - ETA: 7s - loss: 4.5172 - acc: 0.053 - ETA: 7s - loss: 4.5195 - acc: 0.054 - ETA: 7s - loss: 4.5197 - acc: 0.053 - ETA: 7s - loss: 4.5184 - acc: 0.053 - ETA: 7s - loss: 4.5191 - acc: 0.053 - ETA: 7s - loss: 4.5172 - acc: 0.053 - ETA: 7s - loss: 4.5172 - acc: 0.053 - ETA: 7s - loss: 4.5175 - acc: 0.053 - ETA: 6s - loss: 4.5184 - acc: 0.053 - ETA: 6s - loss: 4.5185 - acc: 0.052 - ETA: 6s - loss: 4.5193 - acc: 0.052 - ETA: 6s - loss: 4.5196 - acc: 0.052 - ETA: 6s - loss: 4.5186 - acc: 0.052 - ETA: 6s - loss: 4.5193 - acc: 0.052 - ETA: 6s - loss: 4.5188 - acc: 0.052 - ETA: 6s - loss: 4.5184 - acc: 0.052 - ETA: 6s - loss: 4.5177 - acc: 0.052 - ETA: 6s - loss: 4.5173 - acc: 0.052 - ETA: 6s - loss: 4.5171 - acc: 0.052 - ETA: 6s - loss: 4.5167 - acc: 0.052 - ETA: 5s - loss: 4.5163 - acc: 0.052 - ETA: 5s - loss: 4.5155 - acc: 0.052 - ETA: 5s - loss: 4.5142 - acc: 0.051 - ETA: 5s - loss: 4.5136 - acc: 0.051 - ETA: 5s - loss: 4.5128 - acc: 0.051 - ETA: 5s - loss: 4.5130 - acc: 0.051 - ETA: 5s - loss: 4.5130 - acc: 0.051 - ETA: 5s - loss: 4.5137 - acc: 0.051 - ETA: 5s - loss: 4.5135 - acc: 0.052 - ETA: 5s - loss: 4.5147 - acc: 0.051 - ETA: 5s - loss: 4.5139 - acc: 0.051 - ETA: 4s - loss: 4.5133 - acc: 0.051 - ETA: 4s - loss: 4.5123 - acc: 0.052 - ETA: 4s - loss: 4.5124 - acc: 0.052 - ETA: 4s - loss: 4.5130 - acc: 0.052 - ETA: 4s - loss: 4.5128 - acc: 0.052 - ETA: 4s - loss: 4.5119 - acc: 0.052 - ETA: 4s - loss: 4.5120 - acc: 0.052 - ETA: 4s - loss: 4.5118 - acc: 0.052 - ETA: 4s - loss: 4.5127 - acc: 0.051 - ETA: 4s - loss: 4.5134 - acc: 0.051 - ETA: 4s - loss: 4.5122 - acc: 0.051 - ETA: 3s - loss: 4.5137 - acc: 0.051 - ETA: 3s - loss: 4.5142 - acc: 0.051 - ETA: 3s - loss: 4.5146 - acc: 0.0515"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 4.5131 - acc: 0.051 - ETA: 3s - loss: 4.5137 - acc: 0.051 - ETA: 3s - loss: 4.5138 - acc: 0.051 - ETA: 3s - loss: 4.5149 - acc: 0.051 - ETA: 3s - loss: 4.5155 - acc: 0.052 - ETA: 3s - loss: 4.5136 - acc: 0.052 - ETA: 3s - loss: 4.5141 - acc: 0.052 - ETA: 2s - loss: 4.5150 - acc: 0.051 - ETA: 2s - loss: 4.5151 - acc: 0.051 - ETA: 2s - loss: 4.5158 - acc: 0.051 - ETA: 2s - loss: 4.5166 - acc: 0.051 - ETA: 2s - loss: 4.5166 - acc: 0.051 - ETA: 2s - loss: 4.5172 - acc: 0.050 - ETA: 2s - loss: 4.5178 - acc: 0.050 - ETA: 2s - loss: 4.5176 - acc: 0.050 - ETA: 2s - loss: 4.5174 - acc: 0.051 - ETA: 2s - loss: 4.5178 - acc: 0.050 - ETA: 2s - loss: 4.5173 - acc: 0.050 - ETA: 1s - loss: 4.5185 - acc: 0.051 - ETA: 1s - loss: 4.5207 - acc: 0.050 - ETA: 1s - loss: 4.5213 - acc: 0.050 - ETA: 1s - loss: 4.5223 - acc: 0.050 - ETA: 1s - loss: 4.5224 - acc: 0.050 - ETA: 1s - loss: 4.5210 - acc: 0.051 - ETA: 1s - loss: 4.5202 - acc: 0.051 - ETA: 1s - loss: 4.5200 - acc: 0.051 - ETA: 1s - loss: 4.5203 - acc: 0.051 - ETA: 1s - loss: 4.5198 - acc: 0.051 - ETA: 1s - loss: 4.5192 - acc: 0.051 - ETA: 1s - loss: 4.5191 - acc: 0.051 - ETA: 0s - loss: 4.5186 - acc: 0.051 - ETA: 0s - loss: 4.5192 - acc: 0.051 - ETA: 0s - loss: 4.5196 - acc: 0.051 - ETA: 0s - loss: 4.5193 - acc: 0.051 - ETA: 0s - loss: 4.5192 - acc: 0.051 - ETA: 0s - loss: 4.5195 - acc: 0.051 - ETA: 0s - loss: 4.5183 - acc: 0.051 - ETA: 0s - loss: 4.5188 - acc: 0.051 - ETA: 0s - loss: 4.5184 - acc: 0.051 - ETA: 0s - loss: 4.5191 - acc: 0.051 - 19s 3ms/step - loss: 4.5188 - acc: 0.0516 - val_loss: 4.6127 - val_acc: 0.0335\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.62954 to 4.61275, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 15s - loss: 4.4754 - acc: 0.10 - ETA: 15s - loss: 4.4323 - acc: 0.05 - ETA: 14s - loss: 4.4492 - acc: 0.03 - ETA: 14s - loss: 4.5260 - acc: 0.02 - ETA: 14s - loss: 4.5489 - acc: 0.02 - ETA: 14s - loss: 4.5961 - acc: 0.01 - ETA: 14s - loss: 4.5675 - acc: 0.01 - ETA: 14s - loss: 4.5529 - acc: 0.02 - ETA: 14s - loss: 4.5357 - acc: 0.02 - ETA: 14s - loss: 4.5118 - acc: 0.03 - ETA: 14s - loss: 4.5201 - acc: 0.04 - ETA: 14s - loss: 4.5122 - acc: 0.04 - ETA: 13s - loss: 4.5011 - acc: 0.04 - ETA: 13s - loss: 4.4872 - acc: 0.04 - ETA: 13s - loss: 4.4906 - acc: 0.05 - ETA: 13s - loss: 4.5034 - acc: 0.05 - ETA: 13s - loss: 4.4981 - acc: 0.05 - ETA: 13s - loss: 4.4942 - acc: 0.05 - ETA: 13s - loss: 4.4799 - acc: 0.05 - ETA: 13s - loss: 4.4704 - acc: 0.05 - ETA: 13s - loss: 4.4592 - acc: 0.05 - ETA: 13s - loss: 4.4604 - acc: 0.05 - ETA: 12s - loss: 4.4615 - acc: 0.05 - ETA: 12s - loss: 4.4598 - acc: 0.05 - ETA: 12s - loss: 4.4749 - acc: 0.04 - ETA: 12s - loss: 4.4724 - acc: 0.05 - ETA: 12s - loss: 4.4807 - acc: 0.04 - ETA: 12s - loss: 4.4785 - acc: 0.04 - ETA: 12s - loss: 4.4845 - acc: 0.04 - ETA: 12s - loss: 4.4813 - acc: 0.04 - ETA: 12s - loss: 4.4805 - acc: 0.04 - ETA: 12s - loss: 4.4844 - acc: 0.05 - ETA: 12s - loss: 4.4872 - acc: 0.05 - ETA: 12s - loss: 4.4845 - acc: 0.05 - ETA: 12s - loss: 4.4871 - acc: 0.04 - ETA: 11s - loss: 4.4878 - acc: 0.04 - ETA: 11s - loss: 4.4856 - acc: 0.04 - ETA: 11s - loss: 4.4804 - acc: 0.04 - ETA: 11s - loss: 4.4802 - acc: 0.04 - ETA: 11s - loss: 4.4795 - acc: 0.04 - ETA: 11s - loss: 4.4836 - acc: 0.04 - ETA: 11s - loss: 4.4854 - acc: 0.04 - ETA: 11s - loss: 4.4867 - acc: 0.04 - ETA: 11s - loss: 4.4823 - acc: 0.05 - ETA: 11s - loss: 4.4814 - acc: 0.05 - ETA: 11s - loss: 4.4834 - acc: 0.05 - ETA: 11s - loss: 4.4877 - acc: 0.05 - ETA: 11s - loss: 4.4875 - acc: 0.05 - ETA: 11s - loss: 4.4862 - acc: 0.05 - ETA: 11s - loss: 4.4874 - acc: 0.05 - ETA: 11s - loss: 4.4918 - acc: 0.05 - ETA: 10s - loss: 4.4912 - acc: 0.05 - ETA: 10s - loss: 4.4930 - acc: 0.05 - ETA: 10s - loss: 4.4916 - acc: 0.04 - ETA: 10s - loss: 4.4911 - acc: 0.04 - ETA: 10s - loss: 4.4922 - acc: 0.04 - ETA: 10s - loss: 4.4906 - acc: 0.04 - ETA: 10s - loss: 4.4905 - acc: 0.04 - ETA: 10s - loss: 4.4940 - acc: 0.04 - ETA: 10s - loss: 4.4932 - acc: 0.04 - ETA: 10s - loss: 4.4945 - acc: 0.04 - ETA: 10s - loss: 4.4955 - acc: 0.04 - ETA: 10s - loss: 4.4961 - acc: 0.04 - ETA: 9s - loss: 4.4938 - acc: 0.0474 - ETA: 9s - loss: 4.4934 - acc: 0.047 - ETA: 9s - loss: 4.4936 - acc: 0.047 - ETA: 9s - loss: 4.4939 - acc: 0.047 - ETA: 9s - loss: 4.4961 - acc: 0.046 - ETA: 9s - loss: 4.4958 - acc: 0.046 - ETA: 9s - loss: 4.4945 - acc: 0.046 - ETA: 9s - loss: 4.4974 - acc: 0.047 - ETA: 9s - loss: 4.4972 - acc: 0.047 - ETA: 9s - loss: 4.4984 - acc: 0.047 - ETA: 9s - loss: 4.4974 - acc: 0.047 - ETA: 9s - loss: 4.4975 - acc: 0.047 - ETA: 9s - loss: 4.4992 - acc: 0.047 - ETA: 9s - loss: 4.4996 - acc: 0.048 - ETA: 8s - loss: 4.4972 - acc: 0.048 - ETA: 8s - loss: 4.4973 - acc: 0.048 - ETA: 8s - loss: 4.4997 - acc: 0.048 - ETA: 8s - loss: 4.4992 - acc: 0.049 - ETA: 8s - loss: 4.4998 - acc: 0.048 - ETA: 8s - loss: 4.5019 - acc: 0.048 - ETA: 8s - loss: 4.5038 - acc: 0.048 - ETA: 8s - loss: 4.5028 - acc: 0.048 - ETA: 8s - loss: 4.5022 - acc: 0.049 - ETA: 8s - loss: 4.5010 - acc: 0.049 - ETA: 8s - loss: 4.5011 - acc: 0.048 - ETA: 8s - loss: 4.5014 - acc: 0.048 - ETA: 8s - loss: 4.5003 - acc: 0.049 - ETA: 8s - loss: 4.5026 - acc: 0.048 - ETA: 8s - loss: 4.5022 - acc: 0.048 - ETA: 8s - loss: 4.5011 - acc: 0.048 - ETA: 8s - loss: 4.4995 - acc: 0.048 - ETA: 8s - loss: 4.4992 - acc: 0.047 - ETA: 8s - loss: 4.4987 - acc: 0.048 - ETA: 7s - loss: 4.4960 - acc: 0.048 - ETA: 7s - loss: 4.4955 - acc: 0.049 - ETA: 7s - loss: 4.4981 - acc: 0.049 - ETA: 7s - loss: 4.4975 - acc: 0.049 - ETA: 7s - loss: 4.4982 - acc: 0.049 - ETA: 7s - loss: 4.4972 - acc: 0.049 - ETA: 7s - loss: 4.4998 - acc: 0.049 - ETA: 7s - loss: 4.4976 - acc: 0.050 - ETA: 7s - loss: 4.4996 - acc: 0.050 - ETA: 7s - loss: 4.4991 - acc: 0.050 - ETA: 7s - loss: 4.4985 - acc: 0.049 - ETA: 7s - loss: 4.5013 - acc: 0.049 - ETA: 7s - loss: 4.5009 - acc: 0.049 - ETA: 7s - loss: 4.4998 - acc: 0.049 - ETA: 6s - loss: 4.4977 - acc: 0.050 - ETA: 6s - loss: 4.4951 - acc: 0.050 - ETA: 6s - loss: 4.4944 - acc: 0.050 - ETA: 6s - loss: 4.4942 - acc: 0.050 - ETA: 6s - loss: 4.4952 - acc: 0.050 - ETA: 6s - loss: 4.4952 - acc: 0.050 - ETA: 6s - loss: 4.4950 - acc: 0.050 - ETA: 6s - loss: 4.4944 - acc: 0.050 - ETA: 6s - loss: 4.4941 - acc: 0.050 - ETA: 6s - loss: 4.4928 - acc: 0.049 - ETA: 5s - loss: 4.4925 - acc: 0.049 - ETA: 5s - loss: 4.4934 - acc: 0.049 - ETA: 5s - loss: 4.4906 - acc: 0.048 - ETA: 5s - loss: 4.4910 - acc: 0.049 - ETA: 5s - loss: 4.4906 - acc: 0.049 - ETA: 5s - loss: 4.4908 - acc: 0.050 - ETA: 5s - loss: 4.4903 - acc: 0.050 - ETA: 5s - loss: 4.4901 - acc: 0.050 - ETA: 5s - loss: 4.4885 - acc: 0.050 - ETA: 5s - loss: 4.4882 - acc: 0.050 - ETA: 5s - loss: 4.4882 - acc: 0.050 - ETA: 5s - loss: 4.4887 - acc: 0.050 - ETA: 4s - loss: 4.4894 - acc: 0.051 - ETA: 4s - loss: 4.4888 - acc: 0.051 - ETA: 4s - loss: 4.4908 - acc: 0.051 - ETA: 4s - loss: 4.4918 - acc: 0.051 - ETA: 4s - loss: 4.4919 - acc: 0.051 - ETA: 4s - loss: 4.4928 - acc: 0.050 - ETA: 4s - loss: 4.4919 - acc: 0.050 - ETA: 4s - loss: 4.4923 - acc: 0.050 - ETA: 4s - loss: 4.4911 - acc: 0.051 - ETA: 4s - loss: 4.4910 - acc: 0.050 - ETA: 4s - loss: 4.4925 - acc: 0.050 - ETA: 4s - loss: 4.4931 - acc: 0.050 - ETA: 3s - loss: 4.4935 - acc: 0.050 - ETA: 3s - loss: 4.4932 - acc: 0.050 - ETA: 3s - loss: 4.4936 - acc: 0.050 - ETA: 3s - loss: 4.4928 - acc: 0.051 - ETA: 3s - loss: 4.4924 - acc: 0.051 - ETA: 3s - loss: 4.4914 - acc: 0.051 - ETA: 3s - loss: 4.4887 - acc: 0.051 - ETA: 3s - loss: 4.4901 - acc: 0.051 - ETA: 3s - loss: 4.4891 - acc: 0.051 - ETA: 3s - loss: 4.4887 - acc: 0.051 - ETA: 3s - loss: 4.4885 - acc: 0.051 - ETA: 3s - loss: 4.4866 - acc: 0.051 - ETA: 2s - loss: 4.4865 - acc: 0.051 - ETA: 2s - loss: 4.4877 - acc: 0.051 - ETA: 2s - loss: 4.4877 - acc: 0.051 - ETA: 2s - loss: 4.4879 - acc: 0.051 - ETA: 2s - loss: 4.4877 - acc: 0.050 - ETA: 2s - loss: 4.4868 - acc: 0.051 - ETA: 2s - loss: 4.4879 - acc: 0.051 - ETA: 2s - loss: 4.4884 - acc: 0.051 - ETA: 2s - loss: 4.4875 - acc: 0.051 - ETA: 2s - loss: 4.4867 - acc: 0.051 - ETA: 2s - loss: 4.4864 - acc: 0.050 - ETA: 2s - loss: 4.4866 - acc: 0.050 - ETA: 2s - loss: 4.4858 - acc: 0.050 - ETA: 2s - loss: 4.4881 - acc: 0.050 - ETA: 1s - loss: 4.4871 - acc: 0.050 - ETA: 1s - loss: 4.4859 - acc: 0.050 - ETA: 1s - loss: 4.4856 - acc: 0.050 - ETA: 1s - loss: 4.4851 - acc: 0.050 - ETA: 1s - loss: 4.4853 - acc: 0.051 - ETA: 1s - loss: 4.4859 - acc: 0.051 - ETA: 1s - loss: 4.4864 - acc: 0.051 - ETA: 1s - loss: 4.4861 - acc: 0.051 - ETA: 1s - loss: 4.4863 - acc: 0.050 - ETA: 1s - loss: 4.4878 - acc: 0.050 - ETA: 1s - loss: 4.4883 - acc: 0.050 - ETA: 1s - loss: 4.4885 - acc: 0.050 - ETA: 1s - loss: 4.4890 - acc: 0.050 - ETA: 1s - loss: 4.4889 - acc: 0.049 - ETA: 0s - loss: 4.4888 - acc: 0.050 - ETA: 0s - loss: 4.4897 - acc: 0.050 - ETA: 0s - loss: 4.4887 - acc: 0.050 - ETA: 0s - loss: 4.4889 - acc: 0.050 - ETA: 0s - loss: 4.4888 - acc: 0.050 - ETA: 0s - loss: 4.4893 - acc: 0.050 - ETA: 0s - loss: 4.4897 - acc: 0.050 - ETA: 0s - loss: 4.4898 - acc: 0.050 - ETA: 0s - loss: 4.4898 - acc: 0.050 - ETA: 0s - loss: 4.4903 - acc: 0.050 - ETA: 0s - loss: 4.4892 - acc: 0.050 - ETA: 0s - loss: 4.4900 - acc: 0.050 - ETA: 0s - loss: 4.4906 - acc: 0.051 - ETA: 0s - loss: 4.4900 - acc: 0.051 - ETA: 0s - loss: 4.4889 - acc: 0.051 - 17s 3ms/step - loss: 4.4896 - acc: 0.0512 - val_loss: 4.5846 - val_acc: 0.0395\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.61275 to 4.58462, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6680 [============================>.] - ETA: 14s - loss: 4.3709 - acc: 0.05 - ETA: 14s - loss: 4.2867 - acc: 0.03 - ETA: 14s - loss: 4.3109 - acc: 0.06 - ETA: 14s - loss: 4.3928 - acc: 0.06 - ETA: 14s - loss: 4.4169 - acc: 0.06 - ETA: 14s - loss: 4.4540 - acc: 0.05 - ETA: 14s - loss: 4.4568 - acc: 0.05 - ETA: 14s - loss: 4.4581 - acc: 0.05 - ETA: 14s - loss: 4.4613 - acc: 0.05 - ETA: 14s - loss: 4.4643 - acc: 0.04 - ETA: 14s - loss: 4.4430 - acc: 0.05 - ETA: 14s - loss: 4.4295 - acc: 0.05 - ETA: 14s - loss: 4.4349 - acc: 0.05 - ETA: 14s - loss: 4.4284 - acc: 0.05 - ETA: 14s - loss: 4.4361 - acc: 0.05 - ETA: 13s - loss: 4.4445 - acc: 0.05 - ETA: 13s - loss: 4.4468 - acc: 0.05 - ETA: 13s - loss: 4.4570 - acc: 0.05 - ETA: 13s - loss: 4.4462 - acc: 0.05 - ETA: 13s - loss: 4.4442 - acc: 0.05 - ETA: 13s - loss: 4.4489 - acc: 0.05 - ETA: 13s - loss: 4.4439 - acc: 0.05 - ETA: 13s - loss: 4.4515 - acc: 0.05 - ETA: 13s - loss: 4.4596 - acc: 0.05 - ETA: 13s - loss: 4.4408 - acc: 0.05 - ETA: 13s - loss: 4.4438 - acc: 0.05 - ETA: 13s - loss: 4.4480 - acc: 0.05 - ETA: 13s - loss: 4.4490 - acc: 0.05 - ETA: 13s - loss: 4.4465 - acc: 0.05 - ETA: 13s - loss: 4.4431 - acc: 0.05 - ETA: 13s - loss: 4.4480 - acc: 0.06 - ETA: 13s - loss: 4.4453 - acc: 0.06 - ETA: 13s - loss: 4.4458 - acc: 0.05 - ETA: 13s - loss: 4.4462 - acc: 0.06 - ETA: 13s - loss: 4.4483 - acc: 0.06 - ETA: 13s - loss: 4.4467 - acc: 0.06 - ETA: 13s - loss: 4.4518 - acc: 0.06 - ETA: 12s - loss: 4.4545 - acc: 0.06 - ETA: 12s - loss: 4.4559 - acc: 0.06 - ETA: 12s - loss: 4.4479 - acc: 0.06 - ETA: 12s - loss: 4.4495 - acc: 0.06 - ETA: 12s - loss: 4.4540 - acc: 0.06 - ETA: 12s - loss: 4.4534 - acc: 0.06 - ETA: 12s - loss: 4.4493 - acc: 0.06 - ETA: 12s - loss: 4.4481 - acc: 0.06 - ETA: 12s - loss: 4.4470 - acc: 0.06 - ETA: 12s - loss: 4.4494 - acc: 0.06 - ETA: 12s - loss: 4.4546 - acc: 0.06 - ETA: 12s - loss: 4.4594 - acc: 0.06 - ETA: 12s - loss: 4.4645 - acc: 0.06 - ETA: 12s - loss: 4.4633 - acc: 0.06 - ETA: 12s - loss: 4.4626 - acc: 0.06 - ETA: 12s - loss: 4.4637 - acc: 0.06 - ETA: 12s - loss: 4.4600 - acc: 0.06 - ETA: 11s - loss: 4.4686 - acc: 0.06 - ETA: 11s - loss: 4.4718 - acc: 0.05 - ETA: 11s - loss: 4.4714 - acc: 0.05 - ETA: 11s - loss: 4.4745 - acc: 0.05 - ETA: 11s - loss: 4.4716 - acc: 0.05 - ETA: 11s - loss: 4.4672 - acc: 0.05 - ETA: 11s - loss: 4.4655 - acc: 0.05 - ETA: 11s - loss: 4.4684 - acc: 0.05 - ETA: 11s - loss: 4.4679 - acc: 0.05 - ETA: 11s - loss: 4.4673 - acc: 0.05 - ETA: 11s - loss: 4.4672 - acc: 0.05 - ETA: 11s - loss: 4.4639 - acc: 0.05 - ETA: 11s - loss: 4.4632 - acc: 0.05 - ETA: 11s - loss: 4.4624 - acc: 0.05 - ETA: 10s - loss: 4.4650 - acc: 0.05 - ETA: 10s - loss: 4.4669 - acc: 0.05 - ETA: 10s - loss: 4.4707 - acc: 0.05 - ETA: 10s - loss: 4.4702 - acc: 0.05 - ETA: 10s - loss: 4.4696 - acc: 0.05 - ETA: 10s - loss: 4.4666 - acc: 0.05 - ETA: 10s - loss: 4.4678 - acc: 0.05 - ETA: 10s - loss: 4.4687 - acc: 0.05 - ETA: 10s - loss: 4.4683 - acc: 0.05 - ETA: 10s - loss: 4.4635 - acc: 0.05 - ETA: 10s - loss: 4.4648 - acc: 0.05 - ETA: 10s - loss: 4.4675 - acc: 0.05 - ETA: 10s - loss: 4.4701 - acc: 0.05 - ETA: 10s - loss: 4.4731 - acc: 0.05 - ETA: 9s - loss: 4.4715 - acc: 0.0576 - ETA: 9s - loss: 4.4720 - acc: 0.057 - ETA: 9s - loss: 4.4715 - acc: 0.057 - ETA: 9s - loss: 4.4709 - acc: 0.057 - ETA: 9s - loss: 4.4726 - acc: 0.057 - ETA: 9s - loss: 4.4705 - acc: 0.057 - ETA: 9s - loss: 4.4700 - acc: 0.057 - ETA: 9s - loss: 4.4698 - acc: 0.057 - ETA: 9s - loss: 4.4674 - acc: 0.057 - ETA: 9s - loss: 4.4668 - acc: 0.056 - ETA: 9s - loss: 4.4672 - acc: 0.057 - ETA: 9s - loss: 4.4678 - acc: 0.056 - ETA: 9s - loss: 4.4678 - acc: 0.057 - ETA: 9s - loss: 4.4677 - acc: 0.058 - ETA: 9s - loss: 4.4700 - acc: 0.057 - ETA: 9s - loss: 4.4686 - acc: 0.057 - ETA: 8s - loss: 4.4695 - acc: 0.056 - ETA: 8s - loss: 4.4711 - acc: 0.056 - ETA: 8s - loss: 4.4702 - acc: 0.057 - ETA: 8s - loss: 4.4709 - acc: 0.056 - ETA: 8s - loss: 4.4719 - acc: 0.056 - ETA: 8s - loss: 4.4729 - acc: 0.055 - ETA: 8s - loss: 4.4716 - acc: 0.056 - ETA: 8s - loss: 4.4704 - acc: 0.056 - ETA: 8s - loss: 4.4707 - acc: 0.056 - ETA: 8s - loss: 4.4714 - acc: 0.056 - ETA: 8s - loss: 4.4707 - acc: 0.056 - ETA: 8s - loss: 4.4680 - acc: 0.057 - ETA: 8s - loss: 4.4669 - acc: 0.057 - ETA: 8s - loss: 4.4691 - acc: 0.057 - ETA: 8s - loss: 4.4694 - acc: 0.057 - ETA: 8s - loss: 4.4697 - acc: 0.057 - ETA: 7s - loss: 4.4690 - acc: 0.057 - ETA: 7s - loss: 4.4704 - acc: 0.057 - ETA: 7s - loss: 4.4698 - acc: 0.057 - ETA: 7s - loss: 4.4681 - acc: 0.057 - ETA: 7s - loss: 4.4686 - acc: 0.057 - ETA: 7s - loss: 4.4695 - acc: 0.057 - ETA: 7s - loss: 4.4676 - acc: 0.057 - ETA: 7s - loss: 4.4658 - acc: 0.058 - ETA: 7s - loss: 4.4656 - acc: 0.058 - ETA: 7s - loss: 4.4639 - acc: 0.058 - ETA: 7s - loss: 4.4633 - acc: 0.058 - ETA: 7s - loss: 4.4628 - acc: 0.058 - ETA: 7s - loss: 4.4631 - acc: 0.057 - ETA: 7s - loss: 4.4633 - acc: 0.057 - ETA: 7s - loss: 4.4632 - acc: 0.057 - ETA: 6s - loss: 4.4622 - acc: 0.057 - ETA: 6s - loss: 4.4625 - acc: 0.057 - ETA: 6s - loss: 4.4626 - acc: 0.057 - ETA: 6s - loss: 4.4611 - acc: 0.057 - ETA: 6s - loss: 4.4606 - acc: 0.057 - ETA: 6s - loss: 4.4619 - acc: 0.057 - ETA: 6s - loss: 4.4621 - acc: 0.057 - ETA: 6s - loss: 4.4648 - acc: 0.057 - ETA: 6s - loss: 4.4646 - acc: 0.057 - ETA: 6s - loss: 4.4635 - acc: 0.056 - ETA: 6s - loss: 4.4625 - acc: 0.057 - ETA: 6s - loss: 4.4596 - acc: 0.056 - ETA: 6s - loss: 4.4615 - acc: 0.056 - ETA: 5s - loss: 4.4632 - acc: 0.056 - ETA: 5s - loss: 4.4625 - acc: 0.056 - ETA: 5s - loss: 4.4654 - acc: 0.056 - ETA: 5s - loss: 4.4657 - acc: 0.056 - ETA: 5s - loss: 4.4651 - acc: 0.056 - ETA: 5s - loss: 4.4651 - acc: 0.056 - ETA: 5s - loss: 4.4646 - acc: 0.056 - ETA: 5s - loss: 4.4673 - acc: 0.056 - ETA: 5s - loss: 4.4675 - acc: 0.056 - ETA: 5s - loss: 4.4672 - acc: 0.056 - ETA: 5s - loss: 4.4692 - acc: 0.057 - ETA: 5s - loss: 4.4689 - acc: 0.057 - ETA: 5s - loss: 4.4686 - acc: 0.057 - ETA: 4s - loss: 4.4675 - acc: 0.057 - ETA: 4s - loss: 4.4688 - acc: 0.057 - ETA: 4s - loss: 4.4689 - acc: 0.057 - ETA: 4s - loss: 4.4677 - acc: 0.057 - ETA: 4s - loss: 4.4666 - acc: 0.058 - ETA: 4s - loss: 4.4668 - acc: 0.057 - ETA: 4s - loss: 4.4660 - acc: 0.057 - ETA: 4s - loss: 4.4666 - acc: 0.057 - ETA: 4s - loss: 4.4671 - acc: 0.057 - ETA: 4s - loss: 4.4671 - acc: 0.057 - ETA: 4s - loss: 4.4675 - acc: 0.057 - ETA: 4s - loss: 4.4661 - acc: 0.058 - ETA: 4s - loss: 4.4660 - acc: 0.058 - ETA: 4s - loss: 4.4677 - acc: 0.058 - ETA: 3s - loss: 4.4675 - acc: 0.058 - ETA: 3s - loss: 4.4670 - acc: 0.058 - ETA: 3s - loss: 4.4674 - acc: 0.058 - ETA: 3s - loss: 4.4688 - acc: 0.057 - ETA: 3s - loss: 4.4679 - acc: 0.057 - ETA: 3s - loss: 4.4679 - acc: 0.057 - ETA: 3s - loss: 4.4677 - acc: 0.058 - ETA: 3s - loss: 4.4686 - acc: 0.058 - ETA: 3s - loss: 4.4673 - acc: 0.058 - ETA: 3s - loss: 4.4672 - acc: 0.058 - ETA: 3s - loss: 4.4662 - acc: 0.058 - ETA: 3s - loss: 4.4655 - acc: 0.058 - ETA: 3s - loss: 4.4672 - acc: 0.058 - ETA: 2s - loss: 4.4668 - acc: 0.059 - ETA: 2s - loss: 4.4676 - acc: 0.059 - ETA: 2s - loss: 4.4658 - acc: 0.059 - ETA: 2s - loss: 4.4656 - acc: 0.059 - ETA: 2s - loss: 4.4648 - acc: 0.060 - ETA: 2s - loss: 4.4635 - acc: 0.059 - ETA: 2s - loss: 4.4638 - acc: 0.060 - ETA: 2s - loss: 4.4636 - acc: 0.059 - ETA: 2s - loss: 4.4637 - acc: 0.060 - ETA: 2s - loss: 4.4645 - acc: 0.060 - ETA: 2s - loss: 4.4657 - acc: 0.060 - ETA: 1s - loss: 4.4656 - acc: 0.059 - ETA: 1s - loss: 4.4666 - acc: 0.059 - ETA: 1s - loss: 4.4664 - acc: 0.059 - ETA: 1s - loss: 4.4663 - acc: 0.059 - ETA: 1s - loss: 4.4667 - acc: 0.059 - ETA: 1s - loss: 4.4667 - acc: 0.059 - ETA: 1s - loss: 4.4674 - acc: 0.059 - ETA: 1s - loss: 4.4682 - acc: 0.059 - ETA: 1s - loss: 4.4669 - acc: 0.059 - ETA: 1s - loss: 4.4665 - acc: 0.059 - ETA: 1s - loss: 4.4663 - acc: 0.058 - ETA: 1s - loss: 4.4668 - acc: 0.058 - ETA: 0s - loss: 4.4665 - acc: 0.058 - ETA: 0s - loss: 4.4666 - acc: 0.058 - ETA: 0s - loss: 4.4666 - acc: 0.058 - ETA: 0s - loss: 4.4656 - acc: 0.058 - ETA: 0s - loss: 4.4652 - acc: 0.057 - ETA: 0s - loss: 4.4662 - acc: 0.057 - ETA: 0s - loss: 4.4669 - acc: 0.057 - ETA: 0s - loss: 4.4664 - acc: 0.057 - ETA: 0s - loss: 4.4661 - acc: 0.058 - ETA: 0s - loss: 4.4666 - acc: 0.0578"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.4663 - acc: 0.057 - ETA: 0s - loss: 4.4663 - acc: 0.058 - ETA: 0s - loss: 4.4675 - acc: 0.057 - ETA: 0s - loss: 4.4678 - acc: 0.057 - ETA: 0s - loss: 4.4676 - acc: 0.057 - ETA: 0s - loss: 4.4674 - acc: 0.057 - 17s 3ms/step - loss: 4.4664 - acc: 0.0576 - val_loss: 4.5656 - val_acc: 0.0407\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.58462 to 4.56557, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 15s - loss: 4.4936 - acc: 0.10 - ETA: 15s - loss: 4.3955 - acc: 0.03 - ETA: 15s - loss: 4.2834 - acc: 0.04 - ETA: 15s - loss: 4.3865 - acc: 0.02 - ETA: 14s - loss: 4.4584 - acc: 0.03 - ETA: 15s - loss: 4.4466 - acc: 0.04 - ETA: 14s - loss: 4.4468 - acc: 0.04 - ETA: 14s - loss: 4.4287 - acc: 0.04 - ETA: 14s - loss: 4.3973 - acc: 0.05 - ETA: 14s - loss: 4.3995 - acc: 0.05 - ETA: 14s - loss: 4.3821 - acc: 0.05 - ETA: 14s - loss: 4.3790 - acc: 0.05 - ETA: 14s - loss: 4.3731 - acc: 0.05 - ETA: 14s - loss: 4.3684 - acc: 0.05 - ETA: 14s - loss: 4.3759 - acc: 0.05 - ETA: 14s - loss: 4.3826 - acc: 0.05 - ETA: 14s - loss: 4.3836 - acc: 0.05 - ETA: 14s - loss: 4.4047 - acc: 0.05 - ETA: 13s - loss: 4.4091 - acc: 0.05 - ETA: 13s - loss: 4.4146 - acc: 0.05 - ETA: 13s - loss: 4.4193 - acc: 0.05 - ETA: 13s - loss: 4.4176 - acc: 0.05 - ETA: 13s - loss: 4.4146 - acc: 0.05 - ETA: 13s - loss: 4.4075 - acc: 0.05 - ETA: 13s - loss: 4.4062 - acc: 0.05 - ETA: 13s - loss: 4.4186 - acc: 0.05 - ETA: 13s - loss: 4.4206 - acc: 0.05 - ETA: 13s - loss: 4.4197 - acc: 0.05 - ETA: 13s - loss: 4.4233 - acc: 0.05 - ETA: 13s - loss: 4.4317 - acc: 0.05 - ETA: 13s - loss: 4.4249 - acc: 0.05 - ETA: 12s - loss: 4.4288 - acc: 0.05 - ETA: 12s - loss: 4.4319 - acc: 0.05 - ETA: 12s - loss: 4.4310 - acc: 0.05 - ETA: 12s - loss: 4.4359 - acc: 0.05 - ETA: 12s - loss: 4.4376 - acc: 0.05 - ETA: 12s - loss: 4.4378 - acc: 0.05 - ETA: 12s - loss: 4.4390 - acc: 0.05 - ETA: 12s - loss: 4.4374 - acc: 0.05 - ETA: 12s - loss: 4.4323 - acc: 0.05 - ETA: 12s - loss: 4.4343 - acc: 0.05 - ETA: 12s - loss: 4.4369 - acc: 0.05 - ETA: 12s - loss: 4.4359 - acc: 0.05 - ETA: 12s - loss: 4.4366 - acc: 0.05 - ETA: 12s - loss: 4.4372 - acc: 0.05 - ETA: 12s - loss: 4.4376 - acc: 0.05 - ETA: 11s - loss: 4.4387 - acc: 0.05 - ETA: 11s - loss: 4.4410 - acc: 0.05 - ETA: 11s - loss: 4.4364 - acc: 0.05 - ETA: 11s - loss: 4.4381 - acc: 0.05 - ETA: 11s - loss: 4.4299 - acc: 0.05 - ETA: 11s - loss: 4.4302 - acc: 0.05 - ETA: 11s - loss: 4.4282 - acc: 0.05 - ETA: 11s - loss: 4.4291 - acc: 0.05 - ETA: 11s - loss: 4.4320 - acc: 0.05 - ETA: 11s - loss: 4.4314 - acc: 0.05 - ETA: 11s - loss: 4.4334 - acc: 0.05 - ETA: 11s - loss: 4.4363 - acc: 0.05 - ETA: 10s - loss: 4.4375 - acc: 0.05 - ETA: 10s - loss: 4.4338 - acc: 0.05 - ETA: 10s - loss: 4.4382 - acc: 0.05 - ETA: 10s - loss: 4.4376 - acc: 0.05 - ETA: 10s - loss: 4.4391 - acc: 0.05 - ETA: 10s - loss: 4.4400 - acc: 0.05 - ETA: 10s - loss: 4.4355 - acc: 0.05 - ETA: 10s - loss: 4.4357 - acc: 0.05 - ETA: 10s - loss: 4.4380 - acc: 0.05 - ETA: 10s - loss: 4.4365 - acc: 0.05 - ETA: 10s - loss: 4.4355 - acc: 0.05 - ETA: 10s - loss: 4.4374 - acc: 0.05 - ETA: 10s - loss: 4.4353 - acc: 0.05 - ETA: 10s - loss: 4.4344 - acc: 0.05 - ETA: 9s - loss: 4.4330 - acc: 0.0566 - ETA: 9s - loss: 4.4310 - acc: 0.056 - ETA: 9s - loss: 4.4274 - acc: 0.055 - ETA: 9s - loss: 4.4284 - acc: 0.056 - ETA: 9s - loss: 4.4274 - acc: 0.056 - ETA: 9s - loss: 4.4287 - acc: 0.055 - ETA: 9s - loss: 4.4281 - acc: 0.055 - ETA: 9s - loss: 4.4293 - acc: 0.055 - ETA: 9s - loss: 4.4294 - acc: 0.055 - ETA: 9s - loss: 4.4300 - acc: 0.055 - ETA: 9s - loss: 4.4329 - acc: 0.055 - ETA: 9s - loss: 4.4306 - acc: 0.055 - ETA: 8s - loss: 4.4340 - acc: 0.054 - ETA: 8s - loss: 4.4349 - acc: 0.054 - ETA: 8s - loss: 4.4363 - acc: 0.053 - ETA: 8s - loss: 4.4374 - acc: 0.053 - ETA: 8s - loss: 4.4352 - acc: 0.054 - ETA: 8s - loss: 4.4347 - acc: 0.054 - ETA: 8s - loss: 4.4366 - acc: 0.054 - ETA: 8s - loss: 4.4369 - acc: 0.054 - ETA: 8s - loss: 4.4378 - acc: 0.053 - ETA: 8s - loss: 4.4368 - acc: 0.053 - ETA: 8s - loss: 4.4351 - acc: 0.053 - ETA: 8s - loss: 4.4350 - acc: 0.054 - ETA: 8s - loss: 4.4362 - acc: 0.054 - ETA: 7s - loss: 4.4345 - acc: 0.055 - ETA: 7s - loss: 4.4337 - acc: 0.055 - ETA: 7s - loss: 4.4343 - acc: 0.055 - ETA: 7s - loss: 4.4340 - acc: 0.055 - ETA: 7s - loss: 4.4350 - acc: 0.055 - ETA: 7s - loss: 4.4359 - acc: 0.055 - ETA: 7s - loss: 4.4372 - acc: 0.054 - ETA: 7s - loss: 4.4353 - acc: 0.055 - ETA: 7s - loss: 4.4360 - acc: 0.055 - ETA: 7s - loss: 4.4351 - acc: 0.055 - ETA: 7s - loss: 4.4358 - acc: 0.054 - ETA: 7s - loss: 4.4383 - acc: 0.054 - ETA: 7s - loss: 4.4369 - acc: 0.055 - ETA: 7s - loss: 4.4354 - acc: 0.055 - ETA: 7s - loss: 4.4341 - acc: 0.055 - ETA: 6s - loss: 4.4358 - acc: 0.055 - ETA: 6s - loss: 4.4367 - acc: 0.055 - ETA: 6s - loss: 4.4366 - acc: 0.055 - ETA: 6s - loss: 4.4383 - acc: 0.055 - ETA: 6s - loss: 4.4401 - acc: 0.056 - ETA: 6s - loss: 4.4421 - acc: 0.055 - ETA: 6s - loss: 4.4431 - acc: 0.055 - ETA: 6s - loss: 4.4422 - acc: 0.055 - ETA: 6s - loss: 4.4419 - acc: 0.055 - ETA: 6s - loss: 4.4445 - acc: 0.055 - ETA: 6s - loss: 4.4455 - acc: 0.055 - ETA: 6s - loss: 4.4461 - acc: 0.055 - ETA: 5s - loss: 4.4469 - acc: 0.055 - ETA: 5s - loss: 4.4455 - acc: 0.056 - ETA: 5s - loss: 4.4452 - acc: 0.056 - ETA: 5s - loss: 4.4463 - acc: 0.056 - ETA: 5s - loss: 4.4466 - acc: 0.056 - ETA: 5s - loss: 4.4457 - acc: 0.056 - ETA: 5s - loss: 4.4448 - acc: 0.056 - ETA: 5s - loss: 4.4467 - acc: 0.057 - ETA: 5s - loss: 4.4501 - acc: 0.057 - ETA: 5s - loss: 4.4507 - acc: 0.056 - ETA: 5s - loss: 4.4517 - acc: 0.056 - ETA: 4s - loss: 4.4505 - acc: 0.056 - ETA: 4s - loss: 4.4490 - acc: 0.056 - ETA: 4s - loss: 4.4491 - acc: 0.056 - ETA: 4s - loss: 4.4499 - acc: 0.056 - ETA: 4s - loss: 4.4494 - acc: 0.056 - ETA: 4s - loss: 4.4480 - acc: 0.056 - ETA: 4s - loss: 4.4475 - acc: 0.056 - ETA: 4s - loss: 4.4479 - acc: 0.056 - ETA: 4s - loss: 4.4489 - acc: 0.056 - ETA: 4s - loss: 4.4481 - acc: 0.056 - ETA: 4s - loss: 4.4471 - acc: 0.056 - ETA: 4s - loss: 4.4487 - acc: 0.057 - ETA: 4s - loss: 4.4489 - acc: 0.057 - ETA: 4s - loss: 4.4517 - acc: 0.057 - ETA: 3s - loss: 4.4518 - acc: 0.057 - ETA: 3s - loss: 4.4501 - acc: 0.057 - ETA: 3s - loss: 4.4501 - acc: 0.057 - ETA: 3s - loss: 4.4490 - acc: 0.057 - ETA: 3s - loss: 4.4462 - acc: 0.057 - ETA: 3s - loss: 4.4452 - acc: 0.058 - ETA: 3s - loss: 4.4448 - acc: 0.058 - ETA: 3s - loss: 4.4443 - acc: 0.057 - ETA: 3s - loss: 4.4427 - acc: 0.057 - ETA: 3s - loss: 4.4428 - acc: 0.057 - ETA: 3s - loss: 4.4429 - acc: 0.057 - ETA: 3s - loss: 4.4436 - acc: 0.057 - ETA: 3s - loss: 4.4444 - acc: 0.057 - ETA: 2s - loss: 4.4421 - acc: 0.058 - ETA: 2s - loss: 4.4410 - acc: 0.058 - ETA: 2s - loss: 4.4427 - acc: 0.058 - ETA: 2s - loss: 4.4432 - acc: 0.058 - ETA: 2s - loss: 4.4428 - acc: 0.058 - ETA: 2s - loss: 4.4424 - acc: 0.058 - ETA: 2s - loss: 4.4440 - acc: 0.058 - ETA: 2s - loss: 4.4440 - acc: 0.058 - ETA: 2s - loss: 4.4440 - acc: 0.058 - ETA: 2s - loss: 4.4431 - acc: 0.058 - ETA: 2s - loss: 4.4428 - acc: 0.058 - ETA: 2s - loss: 4.4422 - acc: 0.058 - ETA: 1s - loss: 4.4407 - acc: 0.058 - ETA: 1s - loss: 4.4426 - acc: 0.059 - ETA: 1s - loss: 4.4415 - acc: 0.059 - ETA: 1s - loss: 4.4427 - acc: 0.059 - ETA: 1s - loss: 4.4426 - acc: 0.059 - ETA: 1s - loss: 4.4424 - acc: 0.059 - ETA: 1s - loss: 4.4418 - acc: 0.058 - ETA: 1s - loss: 4.4431 - acc: 0.058 - ETA: 1s - loss: 4.4427 - acc: 0.058 - ETA: 1s - loss: 4.4443 - acc: 0.058 - ETA: 1s - loss: 4.4437 - acc: 0.058 - ETA: 1s - loss: 4.4444 - acc: 0.058 - ETA: 1s - loss: 4.4449 - acc: 0.058 - ETA: 0s - loss: 4.4447 - acc: 0.058 - ETA: 0s - loss: 4.4460 - acc: 0.058 - ETA: 0s - loss: 4.4450 - acc: 0.058 - ETA: 0s - loss: 4.4450 - acc: 0.058 - ETA: 0s - loss: 4.4440 - acc: 0.058 - ETA: 0s - loss: 4.4452 - acc: 0.059 - ETA: 0s - loss: 4.4456 - acc: 0.059 - ETA: 0s - loss: 4.4456 - acc: 0.058 - ETA: 0s - loss: 4.4462 - acc: 0.059 - ETA: 0s - loss: 4.4457 - acc: 0.059 - ETA: 0s - loss: 4.4450 - acc: 0.059 - ETA: 0s - loss: 4.4456 - acc: 0.059 - 17s 2ms/step - loss: 4.4446 - acc: 0.0593 - val_loss: 4.6899 - val_acc: 0.0383\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: val_loss did not improve from 4.56557\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6520/6680 [============================>.] - ETA: 17s - loss: 4.7934 - acc: 0.05 - ETA: 15s - loss: 4.5141 - acc: 0.06 - ETA: 14s - loss: 4.4640 - acc: 0.06 - ETA: 15s - loss: 4.3847 - acc: 0.07 - ETA: 15s - loss: 4.4117 - acc: 0.07 - ETA: 15s - loss: 4.4407 - acc: 0.06 - ETA: 15s - loss: 4.4568 - acc: 0.06 - ETA: 15s - loss: 4.4353 - acc: 0.05 - ETA: 15s - loss: 4.4274 - acc: 0.05 - ETA: 15s - loss: 4.4206 - acc: 0.06 - ETA: 15s - loss: 4.4025 - acc: 0.06 - ETA: 15s - loss: 4.3917 - acc: 0.06 - ETA: 14s - loss: 4.3981 - acc: 0.06 - ETA: 14s - loss: 4.3783 - acc: 0.06 - ETA: 14s - loss: 4.3829 - acc: 0.06 - ETA: 14s - loss: 4.3906 - acc: 0.06 - ETA: 14s - loss: 4.3956 - acc: 0.06 - ETA: 14s - loss: 4.3778 - acc: 0.06 - ETA: 14s - loss: 4.3857 - acc: 0.06 - ETA: 14s - loss: 4.3803 - acc: 0.06 - ETA: 14s - loss: 4.3801 - acc: 0.06 - ETA: 14s - loss: 4.3849 - acc: 0.06 - ETA: 14s - loss: 4.3850 - acc: 0.06 - ETA: 14s - loss: 4.3747 - acc: 0.06 - ETA: 13s - loss: 4.3628 - acc: 0.06 - ETA: 13s - loss: 4.3669 - acc: 0.06 - ETA: 13s - loss: 4.3688 - acc: 0.06 - ETA: 13s - loss: 4.3737 - acc: 0.06 - ETA: 13s - loss: 4.3735 - acc: 0.06 - ETA: 13s - loss: 4.3742 - acc: 0.06 - ETA: 13s - loss: 4.3776 - acc: 0.06 - ETA: 13s - loss: 4.3828 - acc: 0.06 - ETA: 13s - loss: 4.3872 - acc: 0.06 - ETA: 13s - loss: 4.3909 - acc: 0.06 - ETA: 13s - loss: 4.3887 - acc: 0.06 - ETA: 13s - loss: 4.3882 - acc: 0.06 - ETA: 13s - loss: 4.3814 - acc: 0.06 - ETA: 13s - loss: 4.3936 - acc: 0.06 - ETA: 13s - loss: 4.3893 - acc: 0.06 - ETA: 13s - loss: 4.3851 - acc: 0.06 - ETA: 13s - loss: 4.3852 - acc: 0.06 - ETA: 13s - loss: 4.3904 - acc: 0.06 - ETA: 12s - loss: 4.3966 - acc: 0.06 - ETA: 12s - loss: 4.3891 - acc: 0.06 - ETA: 12s - loss: 4.3873 - acc: 0.06 - ETA: 12s - loss: 4.3880 - acc: 0.06 - ETA: 12s - loss: 4.3875 - acc: 0.06 - ETA: 12s - loss: 4.3886 - acc: 0.06 - ETA: 12s - loss: 4.3832 - acc: 0.07 - ETA: 12s - loss: 4.3825 - acc: 0.07 - ETA: 12s - loss: 4.3841 - acc: 0.07 - ETA: 12s - loss: 4.3902 - acc: 0.07 - ETA: 12s - loss: 4.3904 - acc: 0.07 - ETA: 12s - loss: 4.3879 - acc: 0.07 - ETA: 12s - loss: 4.3889 - acc: 0.07 - ETA: 12s - loss: 4.3934 - acc: 0.06 - ETA: 11s - loss: 4.3947 - acc: 0.06 - ETA: 11s - loss: 4.3961 - acc: 0.06 - ETA: 11s - loss: 4.3967 - acc: 0.06 - ETA: 11s - loss: 4.3929 - acc: 0.07 - ETA: 11s - loss: 4.3938 - acc: 0.07 - ETA: 11s - loss: 4.3974 - acc: 0.06 - ETA: 11s - loss: 4.3972 - acc: 0.06 - ETA: 11s - loss: 4.3967 - acc: 0.06 - ETA: 11s - loss: 4.3933 - acc: 0.06 - ETA: 11s - loss: 4.3933 - acc: 0.06 - ETA: 11s - loss: 4.3978 - acc: 0.06 - ETA: 11s - loss: 4.3950 - acc: 0.06 - ETA: 11s - loss: 4.3971 - acc: 0.06 - ETA: 10s - loss: 4.3987 - acc: 0.06 - ETA: 10s - loss: 4.3970 - acc: 0.06 - ETA: 10s - loss: 4.3971 - acc: 0.06 - ETA: 10s - loss: 4.3990 - acc: 0.06 - ETA: 10s - loss: 4.3993 - acc: 0.06 - ETA: 10s - loss: 4.3995 - acc: 0.06 - ETA: 10s - loss: 4.3997 - acc: 0.06 - ETA: 10s - loss: 4.3992 - acc: 0.06 - ETA: 10s - loss: 4.4027 - acc: 0.06 - ETA: 10s - loss: 4.4045 - acc: 0.06 - ETA: 10s - loss: 4.4035 - acc: 0.06 - ETA: 10s - loss: 4.4043 - acc: 0.06 - ETA: 10s - loss: 4.4011 - acc: 0.06 - ETA: 10s - loss: 4.4024 - acc: 0.06 - ETA: 10s - loss: 4.4001 - acc: 0.06 - ETA: 9s - loss: 4.4009 - acc: 0.0631 - ETA: 9s - loss: 4.4012 - acc: 0.063 - ETA: 9s - loss: 4.4024 - acc: 0.063 - ETA: 9s - loss: 4.4013 - acc: 0.062 - ETA: 9s - loss: 4.4034 - acc: 0.062 - ETA: 9s - loss: 4.4008 - acc: 0.062 - ETA: 9s - loss: 4.4015 - acc: 0.061 - ETA: 9s - loss: 4.4040 - acc: 0.061 - ETA: 9s - loss: 4.4023 - acc: 0.060 - ETA: 9s - loss: 4.4023 - acc: 0.060 - ETA: 9s - loss: 4.4014 - acc: 0.060 - ETA: 9s - loss: 4.4023 - acc: 0.060 - ETA: 8s - loss: 4.4056 - acc: 0.059 - ETA: 8s - loss: 4.4046 - acc: 0.059 - ETA: 8s - loss: 4.4061 - acc: 0.060 - ETA: 8s - loss: 4.4089 - acc: 0.059 - ETA: 8s - loss: 4.4088 - acc: 0.059 - ETA: 8s - loss: 4.4070 - acc: 0.060 - ETA: 8s - loss: 4.4098 - acc: 0.060 - ETA: 8s - loss: 4.4096 - acc: 0.061 - ETA: 8s - loss: 4.4084 - acc: 0.061 - ETA: 8s - loss: 4.4077 - acc: 0.061 - ETA: 8s - loss: 4.4112 - acc: 0.060 - ETA: 8s - loss: 4.4109 - acc: 0.060 - ETA: 8s - loss: 4.4107 - acc: 0.060 - ETA: 8s - loss: 4.4091 - acc: 0.060 - ETA: 8s - loss: 4.4095 - acc: 0.059 - ETA: 8s - loss: 4.4096 - acc: 0.059 - ETA: 7s - loss: 4.4074 - acc: 0.059 - ETA: 7s - loss: 4.4052 - acc: 0.059 - ETA: 7s - loss: 4.4034 - acc: 0.059 - ETA: 7s - loss: 4.4064 - acc: 0.059 - ETA: 7s - loss: 4.4061 - acc: 0.059 - ETA: 7s - loss: 4.4060 - acc: 0.059 - ETA: 7s - loss: 4.4058 - acc: 0.059 - ETA: 7s - loss: 4.4055 - acc: 0.059 - ETA: 7s - loss: 4.4059 - acc: 0.059 - ETA: 7s - loss: 4.4058 - acc: 0.059 - ETA: 7s - loss: 4.4058 - acc: 0.059 - ETA: 7s - loss: 4.4054 - acc: 0.060 - ETA: 6s - loss: 4.4084 - acc: 0.059 - ETA: 6s - loss: 4.4079 - acc: 0.060 - ETA: 6s - loss: 4.4074 - acc: 0.060 - ETA: 6s - loss: 4.4067 - acc: 0.060 - ETA: 6s - loss: 4.4071 - acc: 0.060 - ETA: 6s - loss: 4.4045 - acc: 0.061 - ETA: 6s - loss: 4.4043 - acc: 0.060 - ETA: 6s - loss: 4.4044 - acc: 0.060 - ETA: 6s - loss: 4.4054 - acc: 0.060 - ETA: 6s - loss: 4.4064 - acc: 0.060 - ETA: 6s - loss: 4.4064 - acc: 0.060 - ETA: 6s - loss: 4.4042 - acc: 0.060 - ETA: 6s - loss: 4.4046 - acc: 0.060 - ETA: 6s - loss: 4.4039 - acc: 0.059 - ETA: 5s - loss: 4.4044 - acc: 0.060 - ETA: 5s - loss: 4.4058 - acc: 0.060 - ETA: 5s - loss: 4.4051 - acc: 0.060 - ETA: 5s - loss: 4.4040 - acc: 0.059 - ETA: 5s - loss: 4.4028 - acc: 0.060 - ETA: 5s - loss: 4.4043 - acc: 0.059 - ETA: 5s - loss: 4.4036 - acc: 0.060 - ETA: 5s - loss: 4.4049 - acc: 0.060 - ETA: 5s - loss: 4.4027 - acc: 0.060 - ETA: 5s - loss: 4.4040 - acc: 0.060 - ETA: 5s - loss: 4.4050 - acc: 0.060 - ETA: 5s - loss: 4.4052 - acc: 0.060 - ETA: 4s - loss: 4.4060 - acc: 0.060 - ETA: 4s - loss: 4.4085 - acc: 0.060 - ETA: 4s - loss: 4.4076 - acc: 0.060 - ETA: 4s - loss: 4.4073 - acc: 0.060 - ETA: 4s - loss: 4.4067 - acc: 0.061 - ETA: 4s - loss: 4.4068 - acc: 0.061 - ETA: 4s - loss: 4.4070 - acc: 0.061 - ETA: 4s - loss: 4.4079 - acc: 0.061 - ETA: 4s - loss: 4.4096 - acc: 0.061 - ETA: 4s - loss: 4.4096 - acc: 0.061 - ETA: 4s - loss: 4.4095 - acc: 0.061 - ETA: 4s - loss: 4.4088 - acc: 0.061 - ETA: 4s - loss: 4.4085 - acc: 0.061 - ETA: 4s - loss: 4.4088 - acc: 0.061 - ETA: 4s - loss: 4.4096 - acc: 0.061 - ETA: 3s - loss: 4.4077 - acc: 0.061 - ETA: 3s - loss: 4.4081 - acc: 0.060 - ETA: 3s - loss: 4.4075 - acc: 0.060 - ETA: 3s - loss: 4.4081 - acc: 0.060 - ETA: 3s - loss: 4.4083 - acc: 0.060 - ETA: 3s - loss: 4.4101 - acc: 0.060 - ETA: 3s - loss: 4.4104 - acc: 0.060 - ETA: 3s - loss: 4.4118 - acc: 0.059 - ETA: 3s - loss: 4.4120 - acc: 0.059 - ETA: 3s - loss: 4.4133 - acc: 0.059 - ETA: 3s - loss: 4.4141 - acc: 0.059 - ETA: 3s - loss: 4.4139 - acc: 0.058 - ETA: 3s - loss: 4.4143 - acc: 0.059 - ETA: 2s - loss: 4.4148 - acc: 0.059 - ETA: 2s - loss: 4.4140 - acc: 0.059 - ETA: 2s - loss: 4.4147 - acc: 0.059 - ETA: 2s - loss: 4.4127 - acc: 0.059 - ETA: 2s - loss: 4.4156 - acc: 0.059 - ETA: 2s - loss: 4.4143 - acc: 0.060 - ETA: 2s - loss: 4.4144 - acc: 0.060 - ETA: 2s - loss: 4.4143 - acc: 0.060 - ETA: 2s - loss: 4.4154 - acc: 0.060 - ETA: 2s - loss: 4.4153 - acc: 0.060 - ETA: 2s - loss: 4.4155 - acc: 0.060 - ETA: 2s - loss: 4.4158 - acc: 0.060 - ETA: 2s - loss: 4.4153 - acc: 0.060 - ETA: 1s - loss: 4.4159 - acc: 0.060 - ETA: 1s - loss: 4.4165 - acc: 0.060 - ETA: 1s - loss: 4.4167 - acc: 0.060 - ETA: 1s - loss: 4.4160 - acc: 0.060 - ETA: 1s - loss: 4.4175 - acc: 0.060 - ETA: 1s - loss: 4.4165 - acc: 0.060 - ETA: 1s - loss: 4.4148 - acc: 0.060 - ETA: 1s - loss: 4.4141 - acc: 0.060 - ETA: 1s - loss: 4.4142 - acc: 0.060 - ETA: 1s - loss: 4.4140 - acc: 0.061 - ETA: 1s - loss: 4.4119 - acc: 0.061 - ETA: 0s - loss: 4.4117 - acc: 0.061 - ETA: 0s - loss: 4.4105 - acc: 0.061 - ETA: 0s - loss: 4.4113 - acc: 0.061 - ETA: 0s - loss: 4.4116 - acc: 0.061 - ETA: 0s - loss: 4.4121 - acc: 0.061 - ETA: 0s - loss: 4.4128 - acc: 0.061 - ETA: 0s - loss: 4.4143 - acc: 0.061 - ETA: 0s - loss: 4.4140 - acc: 0.061 - ETA: 0s - loss: 4.4149 - acc: 0.061 - ETA: 0s - loss: 4.4149 - acc: 0.061 - ETA: 0s - loss: 4.4145 - acc: 0.0610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 0s - loss: 4.4148 - acc: 0.061 - ETA: 0s - loss: 4.4159 - acc: 0.061 - ETA: 0s - loss: 4.4160 - acc: 0.061 - ETA: 0s - loss: 4.4152 - acc: 0.061 - 17s 3ms/step - loss: 4.4152 - acc: 0.0611 - val_loss: 4.5464 - val_acc: 0.0467\n",
      "\n",
      "Epoch 00018: val_loss improved from 4.56557 to 4.54636, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 15s - loss: 4.2707 - acc: 0.15 - ETA: 15s - loss: 4.3076 - acc: 0.11 - ETA: 15s - loss: 4.3815 - acc: 0.08 - ETA: 15s - loss: 4.3905 - acc: 0.10 - ETA: 15s - loss: 4.3856 - acc: 0.08 - ETA: 15s - loss: 4.3900 - acc: 0.08 - ETA: 15s - loss: 4.3365 - acc: 0.08 - ETA: 15s - loss: 4.3078 - acc: 0.08 - ETA: 14s - loss: 4.2973 - acc: 0.08 - ETA: 14s - loss: 4.2939 - acc: 0.08 - ETA: 14s - loss: 4.2951 - acc: 0.08 - ETA: 14s - loss: 4.2855 - acc: 0.09 - ETA: 14s - loss: 4.2848 - acc: 0.09 - ETA: 14s - loss: 4.2883 - acc: 0.08 - ETA: 14s - loss: 4.2917 - acc: 0.08 - ETA: 14s - loss: 4.3019 - acc: 0.08 - ETA: 14s - loss: 4.2999 - acc: 0.07 - ETA: 14s - loss: 4.3008 - acc: 0.08 - ETA: 14s - loss: 4.3070 - acc: 0.07 - ETA: 14s - loss: 4.3048 - acc: 0.08 - ETA: 14s - loss: 4.3052 - acc: 0.08 - ETA: 14s - loss: 4.3007 - acc: 0.08 - ETA: 13s - loss: 4.3095 - acc: 0.07 - ETA: 13s - loss: 4.3230 - acc: 0.07 - ETA: 13s - loss: 4.3223 - acc: 0.07 - ETA: 13s - loss: 4.3277 - acc: 0.07 - ETA: 13s - loss: 4.3207 - acc: 0.07 - ETA: 13s - loss: 4.3301 - acc: 0.07 - ETA: 13s - loss: 4.3367 - acc: 0.07 - ETA: 13s - loss: 4.3345 - acc: 0.07 - ETA: 13s - loss: 4.3295 - acc: 0.07 - ETA: 13s - loss: 4.3302 - acc: 0.07 - ETA: 13s - loss: 4.3278 - acc: 0.07 - ETA: 13s - loss: 4.3247 - acc: 0.07 - ETA: 13s - loss: 4.3243 - acc: 0.07 - ETA: 13s - loss: 4.3288 - acc: 0.07 - ETA: 13s - loss: 4.3314 - acc: 0.07 - ETA: 13s - loss: 4.3422 - acc: 0.07 - ETA: 13s - loss: 4.3471 - acc: 0.06 - ETA: 12s - loss: 4.3494 - acc: 0.06 - ETA: 12s - loss: 4.3516 - acc: 0.06 - ETA: 12s - loss: 4.3560 - acc: 0.06 - ETA: 12s - loss: 4.3581 - acc: 0.06 - ETA: 12s - loss: 4.3697 - acc: 0.06 - ETA: 12s - loss: 4.3680 - acc: 0.06 - ETA: 12s - loss: 4.3717 - acc: 0.06 - ETA: 12s - loss: 4.3769 - acc: 0.06 - ETA: 12s - loss: 4.3865 - acc: 0.06 - ETA: 12s - loss: 4.3866 - acc: 0.05 - ETA: 12s - loss: 4.3911 - acc: 0.06 - ETA: 12s - loss: 4.3935 - acc: 0.05 - ETA: 12s - loss: 4.3923 - acc: 0.05 - ETA: 12s - loss: 4.3940 - acc: 0.05 - ETA: 12s - loss: 4.3944 - acc: 0.05 - ETA: 11s - loss: 4.3873 - acc: 0.06 - ETA: 11s - loss: 4.3847 - acc: 0.05 - ETA: 11s - loss: 4.3838 - acc: 0.05 - ETA: 11s - loss: 4.3847 - acc: 0.05 - ETA: 11s - loss: 4.3849 - acc: 0.05 - ETA: 11s - loss: 4.3863 - acc: 0.05 - ETA: 11s - loss: 4.3888 - acc: 0.05 - ETA: 11s - loss: 4.3883 - acc: 0.05 - ETA: 11s - loss: 4.3858 - acc: 0.05 - ETA: 11s - loss: 4.3854 - acc: 0.05 - ETA: 11s - loss: 4.3854 - acc: 0.05 - ETA: 11s - loss: 4.3809 - acc: 0.05 - ETA: 11s - loss: 4.3867 - acc: 0.05 - ETA: 10s - loss: 4.3854 - acc: 0.05 - ETA: 10s - loss: 4.3895 - acc: 0.05 - ETA: 10s - loss: 4.3878 - acc: 0.06 - ETA: 10s - loss: 4.3903 - acc: 0.06 - ETA: 10s - loss: 4.3906 - acc: 0.06 - ETA: 10s - loss: 4.3914 - acc: 0.06 - ETA: 10s - loss: 4.3899 - acc: 0.06 - ETA: 10s - loss: 4.3876 - acc: 0.06 - ETA: 10s - loss: 4.3912 - acc: 0.06 - ETA: 10s - loss: 4.3924 - acc: 0.06 - ETA: 10s - loss: 4.3927 - acc: 0.06 - ETA: 10s - loss: 4.3910 - acc: 0.06 - ETA: 10s - loss: 4.3910 - acc: 0.06 - ETA: 10s - loss: 4.3914 - acc: 0.06 - ETA: 9s - loss: 4.3919 - acc: 0.0606 - ETA: 9s - loss: 4.3900 - acc: 0.060 - ETA: 9s - loss: 4.3884 - acc: 0.061 - ETA: 9s - loss: 4.3866 - acc: 0.061 - ETA: 9s - loss: 4.3895 - acc: 0.062 - ETA: 9s - loss: 4.3899 - acc: 0.063 - ETA: 9s - loss: 4.3940 - acc: 0.062 - ETA: 9s - loss: 4.3927 - acc: 0.062 - ETA: 9s - loss: 4.3935 - acc: 0.063 - ETA: 9s - loss: 4.3935 - acc: 0.062 - ETA: 9s - loss: 4.3896 - acc: 0.063 - ETA: 9s - loss: 4.3920 - acc: 0.063 - ETA: 9s - loss: 4.3938 - acc: 0.063 - ETA: 8s - loss: 4.3977 - acc: 0.063 - ETA: 8s - loss: 4.3980 - acc: 0.063 - ETA: 8s - loss: 4.3993 - acc: 0.063 - ETA: 8s - loss: 4.3965 - acc: 0.064 - ETA: 8s - loss: 4.3957 - acc: 0.064 - ETA: 8s - loss: 4.3913 - acc: 0.065 - ETA: 8s - loss: 4.3942 - acc: 0.065 - ETA: 8s - loss: 4.4009 - acc: 0.065 - ETA: 8s - loss: 4.4008 - acc: 0.065 - ETA: 8s - loss: 4.4006 - acc: 0.065 - ETA: 8s - loss: 4.4012 - acc: 0.064 - ETA: 8s - loss: 4.4007 - acc: 0.065 - ETA: 8s - loss: 4.4017 - acc: 0.065 - ETA: 8s - loss: 4.4004 - acc: 0.065 - ETA: 8s - loss: 4.3994 - acc: 0.065 - ETA: 7s - loss: 4.4014 - acc: 0.065 - ETA: 7s - loss: 4.3996 - acc: 0.065 - ETA: 7s - loss: 4.3975 - acc: 0.066 - ETA: 7s - loss: 4.3970 - acc: 0.066 - ETA: 7s - loss: 4.3941 - acc: 0.066 - ETA: 7s - loss: 4.3946 - acc: 0.066 - ETA: 7s - loss: 4.3942 - acc: 0.066 - ETA: 7s - loss: 4.3940 - acc: 0.066 - ETA: 7s - loss: 4.3945 - acc: 0.065 - ETA: 7s - loss: 4.3948 - acc: 0.066 - ETA: 7s - loss: 4.3950 - acc: 0.066 - ETA: 7s - loss: 4.3977 - acc: 0.066 - ETA: 6s - loss: 4.3964 - acc: 0.066 - ETA: 6s - loss: 4.3980 - acc: 0.066 - ETA: 6s - loss: 4.3978 - acc: 0.065 - ETA: 6s - loss: 4.4006 - acc: 0.065 - ETA: 6s - loss: 4.4012 - acc: 0.065 - ETA: 6s - loss: 4.4006 - acc: 0.065 - ETA: 6s - loss: 4.4021 - acc: 0.065 - ETA: 6s - loss: 4.4012 - acc: 0.065 - ETA: 6s - loss: 4.4033 - acc: 0.065 - ETA: 6s - loss: 4.4032 - acc: 0.065 - ETA: 6s - loss: 4.4035 - acc: 0.065 - ETA: 6s - loss: 4.4036 - acc: 0.065 - ETA: 5s - loss: 4.4036 - acc: 0.065 - ETA: 5s - loss: 4.4022 - acc: 0.065 - ETA: 5s - loss: 4.4017 - acc: 0.064 - ETA: 5s - loss: 4.4021 - acc: 0.065 - ETA: 5s - loss: 4.4026 - acc: 0.064 - ETA: 5s - loss: 4.4034 - acc: 0.064 - ETA: 5s - loss: 4.4029 - acc: 0.064 - ETA: 5s - loss: 4.4006 - acc: 0.064 - ETA: 5s - loss: 4.4002 - acc: 0.064 - ETA: 5s - loss: 4.3991 - acc: 0.064 - ETA: 5s - loss: 4.4006 - acc: 0.064 - ETA: 5s - loss: 4.4017 - acc: 0.064 - ETA: 5s - loss: 4.4015 - acc: 0.064 - ETA: 5s - loss: 4.4012 - acc: 0.064 - ETA: 5s - loss: 4.4011 - acc: 0.065 - ETA: 4s - loss: 4.4001 - acc: 0.065 - ETA: 4s - loss: 4.3987 - acc: 0.066 - ETA: 4s - loss: 4.3964 - acc: 0.066 - ETA: 4s - loss: 4.3973 - acc: 0.066 - ETA: 4s - loss: 4.3968 - acc: 0.067 - ETA: 4s - loss: 4.3965 - acc: 0.066 - ETA: 4s - loss: 4.3968 - acc: 0.066 - ETA: 4s - loss: 4.3972 - acc: 0.066 - ETA: 4s - loss: 4.3973 - acc: 0.066 - ETA: 4s - loss: 4.3971 - acc: 0.066 - ETA: 4s - loss: 4.3972 - acc: 0.066 - ETA: 4s - loss: 4.3980 - acc: 0.066 - ETA: 4s - loss: 4.3965 - acc: 0.066 - ETA: 3s - loss: 4.3965 - acc: 0.066 - ETA: 3s - loss: 4.3958 - acc: 0.066 - ETA: 3s - loss: 4.3937 - acc: 0.066 - ETA: 3s - loss: 4.3958 - acc: 0.066 - ETA: 3s - loss: 4.3977 - acc: 0.066 - ETA: 3s - loss: 4.3979 - acc: 0.066 - ETA: 3s - loss: 4.3981 - acc: 0.066 - ETA: 3s - loss: 4.3968 - acc: 0.066 - ETA: 3s - loss: 4.3967 - acc: 0.065 - ETA: 3s - loss: 4.3977 - acc: 0.065 - ETA: 3s - loss: 4.3980 - acc: 0.065 - ETA: 3s - loss: 4.3986 - acc: 0.065 - ETA: 2s - loss: 4.3966 - acc: 0.066 - ETA: 2s - loss: 4.3956 - acc: 0.065 - ETA: 2s - loss: 4.3953 - acc: 0.066 - ETA: 2s - loss: 4.3948 - acc: 0.065 - ETA: 2s - loss: 4.3961 - acc: 0.065 - ETA: 2s - loss: 4.3965 - acc: 0.065 - ETA: 2s - loss: 4.3957 - acc: 0.065 - ETA: 2s - loss: 4.3950 - acc: 0.065 - ETA: 2s - loss: 4.3940 - acc: 0.066 - ETA: 2s - loss: 4.3941 - acc: 0.066 - ETA: 2s - loss: 4.3946 - acc: 0.065 - ETA: 2s - loss: 4.3949 - acc: 0.065 - ETA: 2s - loss: 4.3939 - acc: 0.065 - ETA: 1s - loss: 4.3931 - acc: 0.065 - ETA: 1s - loss: 4.3921 - acc: 0.065 - ETA: 1s - loss: 4.3934 - acc: 0.065 - ETA: 1s - loss: 4.3939 - acc: 0.064 - ETA: 1s - loss: 4.3926 - acc: 0.065 - ETA: 1s - loss: 4.3937 - acc: 0.064 - ETA: 1s - loss: 4.3929 - acc: 0.064 - ETA: 1s - loss: 4.3927 - acc: 0.064 - ETA: 1s - loss: 4.3936 - acc: 0.064 - ETA: 1s - loss: 4.3923 - acc: 0.064 - ETA: 1s - loss: 4.3920 - acc: 0.064 - ETA: 0s - loss: 4.3920 - acc: 0.064 - ETA: 0s - loss: 4.3925 - acc: 0.064 - ETA: 0s - loss: 4.3911 - acc: 0.064 - ETA: 0s - loss: 4.3905 - acc: 0.064 - ETA: 0s - loss: 4.3902 - acc: 0.065 - ETA: 0s - loss: 4.3914 - acc: 0.064 - ETA: 0s - loss: 4.3911 - acc: 0.064 - ETA: 0s - loss: 4.3910 - acc: 0.064 - ETA: 0s - loss: 4.3925 - acc: 0.064 - ETA: 0s - loss: 4.3929 - acc: 0.064 - ETA: 0s - loss: 4.3923 - acc: 0.064 - ETA: 0s - loss: 4.3930 - acc: 0.064 - 17s 2ms/step - loss: 4.3932 - acc: 0.0642 - val_loss: 4.5393 - val_acc: 0.0443\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.54636 to 4.53934, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 14s - loss: 4.0087 - acc: 0.05 - ETA: 13s - loss: 4.3463 - acc: 0.03 - ETA: 15s - loss: 4.3690 - acc: 0.03 - ETA: 15s - loss: 4.3253 - acc: 0.05 - ETA: 15s - loss: 4.2832 - acc: 0.06 - ETA: 14s - loss: 4.2970 - acc: 0.07 - ETA: 15s - loss: 4.2964 - acc: 0.07 - ETA: 14s - loss: 4.3236 - acc: 0.05 - ETA: 14s - loss: 4.3019 - acc: 0.06 - ETA: 14s - loss: 4.3215 - acc: 0.05 - ETA: 14s - loss: 4.3204 - acc: 0.05 - ETA: 14s - loss: 4.3280 - acc: 0.05 - ETA: 14s - loss: 4.3225 - acc: 0.05 - ETA: 14s - loss: 4.3160 - acc: 0.05 - ETA: 14s - loss: 4.2973 - acc: 0.06 - ETA: 14s - loss: 4.3109 - acc: 0.06 - ETA: 14s - loss: 4.3031 - acc: 0.06 - ETA: 14s - loss: 4.3159 - acc: 0.06 - ETA: 14s - loss: 4.3164 - acc: 0.06 - ETA: 14s - loss: 4.3345 - acc: 0.06 - ETA: 14s - loss: 4.3335 - acc: 0.06 - ETA: 14s - loss: 4.3352 - acc: 0.06 - ETA: 14s - loss: 4.3286 - acc: 0.06 - ETA: 14s - loss: 4.3288 - acc: 0.06 - ETA: 13s - loss: 4.3428 - acc: 0.06 - ETA: 13s - loss: 4.3470 - acc: 0.06 - ETA: 13s - loss: 4.3470 - acc: 0.06 - ETA: 13s - loss: 4.3359 - acc: 0.06 - ETA: 13s - loss: 4.3422 - acc: 0.06 - ETA: 13s - loss: 4.3357 - acc: 0.06 - ETA: 13s - loss: 4.3394 - acc: 0.06 - ETA: 13s - loss: 4.3414 - acc: 0.06 - ETA: 13s - loss: 4.3421 - acc: 0.06 - ETA: 13s - loss: 4.3385 - acc: 0.06 - ETA: 13s - loss: 4.3454 - acc: 0.06 - ETA: 13s - loss: 4.3466 - acc: 0.06 - ETA: 12s - loss: 4.3519 - acc: 0.06 - ETA: 12s - loss: 4.3550 - acc: 0.06 - ETA: 12s - loss: 4.3549 - acc: 0.06 - ETA: 12s - loss: 4.3550 - acc: 0.06 - ETA: 12s - loss: 4.3510 - acc: 0.06 - ETA: 12s - loss: 4.3495 - acc: 0.06 - ETA: 12s - loss: 4.3497 - acc: 0.06 - ETA: 12s - loss: 4.3546 - acc: 0.06 - ETA: 12s - loss: 4.3605 - acc: 0.06 - ETA: 12s - loss: 4.3642 - acc: 0.06 - ETA: 12s - loss: 4.3651 - acc: 0.06 - ETA: 12s - loss: 4.3656 - acc: 0.06 - ETA: 12s - loss: 4.3648 - acc: 0.06 - ETA: 12s - loss: 4.3632 - acc: 0.06 - ETA: 12s - loss: 4.3582 - acc: 0.06 - ETA: 12s - loss: 4.3614 - acc: 0.06 - ETA: 11s - loss: 4.3529 - acc: 0.06 - ETA: 11s - loss: 4.3457 - acc: 0.06 - ETA: 11s - loss: 4.3437 - acc: 0.06 - ETA: 11s - loss: 4.3425 - acc: 0.06 - ETA: 11s - loss: 4.3450 - acc: 0.06 - ETA: 11s - loss: 4.3425 - acc: 0.06 - ETA: 11s - loss: 4.3390 - acc: 0.06 - ETA: 11s - loss: 4.3385 - acc: 0.06 - ETA: 11s - loss: 4.3408 - acc: 0.06 - ETA: 11s - loss: 4.3448 - acc: 0.06 - ETA: 10s - loss: 4.3417 - acc: 0.06 - ETA: 10s - loss: 4.3420 - acc: 0.06 - ETA: 10s - loss: 4.3385 - acc: 0.06 - ETA: 10s - loss: 4.3396 - acc: 0.06 - ETA: 10s - loss: 4.3424 - acc: 0.06 - ETA: 10s - loss: 4.3378 - acc: 0.06 - ETA: 10s - loss: 4.3420 - acc: 0.06 - ETA: 10s - loss: 4.3441 - acc: 0.06 - ETA: 10s - loss: 4.3419 - acc: 0.06 - ETA: 10s - loss: 4.3417 - acc: 0.06 - ETA: 10s - loss: 4.3451 - acc: 0.06 - ETA: 10s - loss: 4.3450 - acc: 0.06 - ETA: 10s - loss: 4.3491 - acc: 0.06 - ETA: 9s - loss: 4.3519 - acc: 0.0636 - ETA: 9s - loss: 4.3529 - acc: 0.063 - ETA: 9s - loss: 4.3523 - acc: 0.063 - ETA: 9s - loss: 4.3504 - acc: 0.064 - ETA: 9s - loss: 4.3505 - acc: 0.064 - ETA: 9s - loss: 4.3513 - acc: 0.064 - ETA: 9s - loss: 4.3531 - acc: 0.063 - ETA: 9s - loss: 4.3500 - acc: 0.064 - ETA: 9s - loss: 4.3557 - acc: 0.064 - ETA: 9s - loss: 4.3587 - acc: 0.064 - ETA: 9s - loss: 4.3613 - acc: 0.063 - ETA: 9s - loss: 4.3614 - acc: 0.064 - ETA: 9s - loss: 4.3613 - acc: 0.063 - ETA: 9s - loss: 4.3613 - acc: 0.064 - ETA: 8s - loss: 4.3585 - acc: 0.064 - ETA: 8s - loss: 4.3587 - acc: 0.063 - ETA: 8s - loss: 4.3590 - acc: 0.063 - ETA: 8s - loss: 4.3604 - acc: 0.063 - ETA: 8s - loss: 4.3613 - acc: 0.063 - ETA: 8s - loss: 4.3632 - acc: 0.062 - ETA: 8s - loss: 4.3659 - acc: 0.062 - ETA: 8s - loss: 4.3645 - acc: 0.063 - ETA: 8s - loss: 4.3650 - acc: 0.062 - ETA: 8s - loss: 4.3619 - acc: 0.063 - ETA: 8s - loss: 4.3628 - acc: 0.063 - ETA: 8s - loss: 4.3662 - acc: 0.063 - ETA: 8s - loss: 4.3676 - acc: 0.063 - ETA: 8s - loss: 4.3673 - acc: 0.062 - ETA: 7s - loss: 4.3662 - acc: 0.062 - ETA: 7s - loss: 4.3647 - acc: 0.062 - ETA: 7s - loss: 4.3635 - acc: 0.062 - ETA: 7s - loss: 4.3664 - acc: 0.062 - ETA: 7s - loss: 4.3665 - acc: 0.062 - ETA: 7s - loss: 4.3660 - acc: 0.062 - ETA: 7s - loss: 4.3691 - acc: 0.062 - ETA: 7s - loss: 4.3697 - acc: 0.062 - ETA: 7s - loss: 4.3704 - acc: 0.062 - ETA: 7s - loss: 4.3700 - acc: 0.063 - ETA: 7s - loss: 4.3699 - acc: 0.062 - ETA: 7s - loss: 4.3705 - acc: 0.062 - ETA: 7s - loss: 4.3714 - acc: 0.062 - ETA: 7s - loss: 4.3698 - acc: 0.063 - ETA: 6s - loss: 4.3710 - acc: 0.064 - ETA: 6s - loss: 4.3724 - acc: 0.063 - ETA: 6s - loss: 4.3729 - acc: 0.063 - ETA: 6s - loss: 4.3744 - acc: 0.062 - ETA: 6s - loss: 4.3753 - acc: 0.062 - ETA: 6s - loss: 4.3748 - acc: 0.062 - ETA: 6s - loss: 4.3740 - acc: 0.063 - ETA: 6s - loss: 4.3733 - acc: 0.063 - ETA: 6s - loss: 4.3741 - acc: 0.063 - ETA: 6s - loss: 4.3738 - acc: 0.063 - ETA: 6s - loss: 4.3736 - acc: 0.063 - ETA: 6s - loss: 4.3709 - acc: 0.063 - ETA: 6s - loss: 4.3698 - acc: 0.063 - ETA: 5s - loss: 4.3704 - acc: 0.063 - ETA: 5s - loss: 4.3695 - acc: 0.062 - ETA: 5s - loss: 4.3693 - acc: 0.062 - ETA: 5s - loss: 4.3700 - acc: 0.063 - ETA: 5s - loss: 4.3697 - acc: 0.062 - ETA: 5s - loss: 4.3708 - acc: 0.062 - ETA: 5s - loss: 4.3719 - acc: 0.062 - ETA: 5s - loss: 4.3710 - acc: 0.062 - ETA: 5s - loss: 4.3704 - acc: 0.062 - ETA: 5s - loss: 4.3702 - acc: 0.062 - ETA: 5s - loss: 4.3704 - acc: 0.062 - ETA: 5s - loss: 4.3713 - acc: 0.062 - ETA: 5s - loss: 4.3703 - acc: 0.062 - ETA: 5s - loss: 4.3677 - acc: 0.062 - ETA: 4s - loss: 4.3645 - acc: 0.063 - ETA: 4s - loss: 4.3649 - acc: 0.063 - ETA: 4s - loss: 4.3672 - acc: 0.063 - ETA: 4s - loss: 4.3665 - acc: 0.063 - ETA: 4s - loss: 4.3676 - acc: 0.063 - ETA: 4s - loss: 4.3677 - acc: 0.063 - ETA: 4s - loss: 4.3687 - acc: 0.063 - ETA: 4s - loss: 4.3686 - acc: 0.063 - ETA: 4s - loss: 4.3697 - acc: 0.063 - ETA: 4s - loss: 4.3708 - acc: 0.063 - ETA: 3s - loss: 4.3720 - acc: 0.063 - ETA: 3s - loss: 4.3721 - acc: 0.063 - ETA: 3s - loss: 4.3722 - acc: 0.063 - ETA: 3s - loss: 4.3717 - acc: 0.063 - ETA: 3s - loss: 4.3712 - acc: 0.063 - ETA: 3s - loss: 4.3707 - acc: 0.063 - ETA: 3s - loss: 4.3694 - acc: 0.064 - ETA: 3s - loss: 4.3711 - acc: 0.064 - ETA: 3s - loss: 4.3746 - acc: 0.064 - ETA: 3s - loss: 4.3746 - acc: 0.064 - ETA: 3s - loss: 4.3737 - acc: 0.065 - ETA: 3s - loss: 4.3730 - acc: 0.065 - ETA: 3s - loss: 4.3728 - acc: 0.065 - ETA: 3s - loss: 4.3704 - acc: 0.065 - ETA: 2s - loss: 4.3706 - acc: 0.065 - ETA: 2s - loss: 4.3697 - acc: 0.065 - ETA: 2s - loss: 4.3699 - acc: 0.066 - ETA: 2s - loss: 4.3690 - acc: 0.066 - ETA: 2s - loss: 4.3689 - acc: 0.066 - ETA: 2s - loss: 4.3696 - acc: 0.066 - ETA: 2s - loss: 4.3699 - acc: 0.066 - ETA: 2s - loss: 4.3696 - acc: 0.066 - ETA: 2s - loss: 4.3689 - acc: 0.066 - ETA: 2s - loss: 4.3694 - acc: 0.066 - ETA: 2s - loss: 4.3688 - acc: 0.066 - ETA: 1s - loss: 4.3661 - acc: 0.066 - ETA: 1s - loss: 4.3653 - acc: 0.066 - ETA: 1s - loss: 4.3685 - acc: 0.066 - ETA: 1s - loss: 4.3681 - acc: 0.066 - ETA: 1s - loss: 4.3680 - acc: 0.067 - ETA: 1s - loss: 4.3685 - acc: 0.067 - ETA: 1s - loss: 4.3692 - acc: 0.066 - ETA: 1s - loss: 4.3687 - acc: 0.067 - ETA: 1s - loss: 4.3695 - acc: 0.067 - ETA: 1s - loss: 4.3683 - acc: 0.067 - ETA: 1s - loss: 4.3689 - acc: 0.067 - ETA: 1s - loss: 4.3686 - acc: 0.067 - ETA: 0s - loss: 4.3688 - acc: 0.067 - ETA: 0s - loss: 4.3695 - acc: 0.067 - ETA: 0s - loss: 4.3691 - acc: 0.066 - ETA: 0s - loss: 4.3700 - acc: 0.066 - ETA: 0s - loss: 4.3702 - acc: 0.066 - ETA: 0s - loss: 4.3704 - acc: 0.066 - ETA: 0s - loss: 4.3696 - acc: 0.066 - ETA: 0s - loss: 4.3690 - acc: 0.066 - ETA: 0s - loss: 4.3686 - acc: 0.066 - ETA: 0s - loss: 4.3688 - acc: 0.066 - ETA: 0s - loss: 4.3682 - acc: 0.066 - ETA: 0s - loss: 4.3676 - acc: 0.066 - 17s 2ms/step - loss: 4.3677 - acc: 0.0665 - val_loss: 4.4841 - val_acc: 0.0563\n",
      "\n",
      "Epoch 00020: val_loss improved from 4.53934 to 4.48413, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x54e5550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: 设置训练模型的epochs的数量\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### 不要修改下方代码\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载具有最好验证loss的模型\n",
    "\n",
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型\n",
    "\n",
    "在狗图像的测试数据集上试用你的模型。确保测试准确率大于1%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 6.6986%\n"
     ]
    }
   ],
   "source": [
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## 步骤 4: 使用一个CNN来区分狗的品种\n",
    "\n",
    "\n",
    "使用 迁移学习（Transfer Learning）的方法，能帮助我们在不损失准确率的情况下大大减少训练时间。在以下步骤中，你可以尝试使用迁移学习来训练你自己的CNN。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到从图像中提取的特征向量（Bottleneck Features）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型架构\n",
    "\n",
    "该模型使用预训练的 VGG-16 模型作为固定的图像特征提取器，其中 VGG-16 最后一层卷积层的输出被直接输入到我们的模型。我们只需要添加一个全局平均池化层以及一个全连接层，其中全连接层使用 softmax 激活函数，对每一个狗的种类都包含一个节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 编译模型\n",
    "\n",
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - ETA: 3:52 - loss: 14.4054 - acc: 0.0000e+ - ETA: 49s - loss: 14.6878 - acc: 0.0100     - ETA: 29s - loss: 14.6924 - acc: 0.022 - ETA: 21s - loss: 14.6038 - acc: 0.026 - ETA: 17s - loss: 14.6014 - acc: 0.032 - ETA: 14s - loss: 14.6162 - acc: 0.035 - ETA: 12s - loss: 14.4644 - acc: 0.036 - ETA: 10s - loss: 14.2327 - acc: 0.045 - ETA: 9s - loss: 14.3266 - acc: 0.041 - ETA: 9s - loss: 14.3623 - acc: 0.03 - ETA: 8s - loss: 14.2866 - acc: 0.03 - ETA: 7s - loss: 14.2365 - acc: 0.03 - ETA: 7s - loss: 14.2423 - acc: 0.03 - ETA: 6s - loss: 14.1812 - acc: 0.03 - ETA: 6s - loss: 14.0288 - acc: 0.04 - ETA: 6s - loss: 13.9249 - acc: 0.04 - ETA: 5s - loss: 13.7938 - acc: 0.04 - ETA: 5s - loss: 13.7488 - acc: 0.04 - ETA: 5s - loss: 13.7445 - acc: 0.04 - ETA: 5s - loss: 13.6907 - acc: 0.04 - ETA: 4s - loss: 13.6511 - acc: 0.04 - ETA: 4s - loss: 13.6320 - acc: 0.04 - ETA: 4s - loss: 13.6362 - acc: 0.04 - ETA: 4s - loss: 13.6202 - acc: 0.05 - ETA: 4s - loss: 13.5988 - acc: 0.05 - ETA: 3s - loss: 13.5647 - acc: 0.05 - ETA: 3s - loss: 13.4924 - acc: 0.06 - ETA: 3s - loss: 13.4338 - acc: 0.06 - ETA: 3s - loss: 13.3697 - acc: 0.06 - ETA: 3s - loss: 13.3269 - acc: 0.06 - ETA: 3s - loss: 13.3038 - acc: 0.06 - ETA: 3s - loss: 13.2789 - acc: 0.06 - ETA: 3s - loss: 13.2108 - acc: 0.07 - ETA: 2s - loss: 13.1736 - acc: 0.07 - ETA: 2s - loss: 13.1237 - acc: 0.07 - ETA: 2s - loss: 13.1116 - acc: 0.07 - ETA: 2s - loss: 13.0727 - acc: 0.07 - ETA: 2s - loss: 13.0669 - acc: 0.07 - ETA: 2s - loss: 13.0039 - acc: 0.08 - ETA: 2s - loss: 12.9603 - acc: 0.08 - ETA: 2s - loss: 12.9237 - acc: 0.08 - ETA: 2s - loss: 12.8808 - acc: 0.08 - ETA: 2s - loss: 12.8708 - acc: 0.08 - ETA: 2s - loss: 12.8588 - acc: 0.08 - ETA: 1s - loss: 12.8069 - acc: 0.09 - ETA: 1s - loss: 12.7821 - acc: 0.09 - ETA: 1s - loss: 12.7514 - acc: 0.09 - ETA: 1s - loss: 12.7521 - acc: 0.09 - ETA: 1s - loss: 12.7104 - acc: 0.09 - ETA: 1s - loss: 12.6775 - acc: 0.09 - ETA: 1s - loss: 12.6263 - acc: 0.10 - ETA: 1s - loss: 12.5998 - acc: 0.10 - ETA: 1s - loss: 12.5827 - acc: 0.10 - ETA: 1s - loss: 12.5568 - acc: 0.10 - ETA: 1s - loss: 12.4907 - acc: 0.11 - ETA: 1s - loss: 12.4790 - acc: 0.11 - ETA: 1s - loss: 12.4305 - acc: 0.11 - ETA: 0s - loss: 12.3809 - acc: 0.11 - ETA: 0s - loss: 12.3574 - acc: 0.11 - ETA: 0s - loss: 12.3303 - acc: 0.11 - ETA: 0s - loss: 12.2802 - acc: 0.12 - ETA: 0s - loss: 12.2481 - acc: 0.12 - ETA: 0s - loss: 12.2451 - acc: 0.12 - ETA: 0s - loss: 12.2229 - acc: 0.12 - ETA: 0s - loss: 12.1990 - acc: 0.12 - ETA: 0s - loss: 12.1656 - acc: 0.12 - ETA: 0s - loss: 12.1442 - acc: 0.12 - ETA: 0s - loss: 12.1265 - acc: 0.12 - ETA: 0s - loss: 12.1107 - acc: 0.13 - ETA: 0s - loss: 12.0755 - acc: 0.13 - ETA: 0s - loss: 12.0569 - acc: 0.13 - ETA: 0s - loss: 12.0297 - acc: 0.13 - 5s 752us/step - loss: 11.9905 - acc: 0.1376 - val_loss: 10.3647 - val_acc: 0.2371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.36473, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - ETA: 0s - loss: 10.8091 - acc: 0.15 - ETA: 5s - loss: 10.2468 - acc: 0.23 - ETA: 4s - loss: 10.1216 - acc: 0.24 - ETA: 4s - loss: 10.1416 - acc: 0.25 - ETA: 4s - loss: 10.3314 - acc: 0.24 - ETA: 3s - loss: 10.3886 - acc: 0.25 - ETA: 3s - loss: 10.2856 - acc: 0.26 - ETA: 3s - loss: 10.2218 - acc: 0.26 - ETA: 3s - loss: 10.1686 - acc: 0.26 - ETA: 3s - loss: 10.1934 - acc: 0.26 - ETA: 3s - loss: 10.1434 - acc: 0.26 - ETA: 3s - loss: 10.0377 - acc: 0.27 - ETA: 3s - loss: 10.0763 - acc: 0.26 - ETA: 3s - loss: 9.9825 - acc: 0.2684 - ETA: 3s - loss: 9.9803 - acc: 0.270 - ETA: 3s - loss: 9.9016 - acc: 0.273 - ETA: 3s - loss: 9.8675 - acc: 0.271 - ETA: 3s - loss: 9.8631 - acc: 0.273 - ETA: 3s - loss: 9.8457 - acc: 0.277 - ETA: 3s - loss: 9.7769 - acc: 0.280 - ETA: 3s - loss: 9.7846 - acc: 0.281 - ETA: 3s - loss: 9.7197 - acc: 0.284 - ETA: 3s - loss: 9.6586 - acc: 0.289 - ETA: 2s - loss: 9.6355 - acc: 0.291 - ETA: 2s - loss: 9.6173 - acc: 0.293 - ETA: 2s - loss: 9.6528 - acc: 0.293 - ETA: 2s - loss: 9.6770 - acc: 0.292 - ETA: 2s - loss: 9.7080 - acc: 0.291 - ETA: 2s - loss: 9.7414 - acc: 0.290 - ETA: 2s - loss: 9.7593 - acc: 0.289 - ETA: 2s - loss: 9.7869 - acc: 0.287 - ETA: 2s - loss: 9.8163 - acc: 0.284 - ETA: 2s - loss: 9.7798 - acc: 0.287 - ETA: 2s - loss: 9.7757 - acc: 0.288 - ETA: 2s - loss: 9.7649 - acc: 0.288 - ETA: 2s - loss: 9.7470 - acc: 0.290 - ETA: 2s - loss: 9.7137 - acc: 0.291 - ETA: 2s - loss: 9.6823 - acc: 0.293 - ETA: 2s - loss: 9.7070 - acc: 0.292 - ETA: 1s - loss: 9.7073 - acc: 0.293 - ETA: 1s - loss: 9.7254 - acc: 0.293 - ETA: 1s - loss: 9.7314 - acc: 0.293 - ETA: 1s - loss: 9.7425 - acc: 0.292 - ETA: 1s - loss: 9.7339 - acc: 0.293 - ETA: 1s - loss: 9.7412 - acc: 0.292 - ETA: 1s - loss: 9.7490 - acc: 0.292 - ETA: 1s - loss: 9.7357 - acc: 0.293 - ETA: 1s - loss: 9.7667 - acc: 0.291 - ETA: 1s - loss: 9.7363 - acc: 0.292 - ETA: 1s - loss: 9.7512 - acc: 0.292 - ETA: 1s - loss: 9.7544 - acc: 0.292 - ETA: 1s - loss: 9.7240 - acc: 0.293 - ETA: 1s - loss: 9.7271 - acc: 0.293 - ETA: 1s - loss: 9.7272 - acc: 0.293 - ETA: 1s - loss: 9.7146 - acc: 0.294 - ETA: 1s - loss: 9.7144 - acc: 0.294 - ETA: 0s - loss: 9.7040 - acc: 0.295 - ETA: 0s - loss: 9.7229 - acc: 0.294 - ETA: 0s - loss: 9.7053 - acc: 0.295 - ETA: 0s - loss: 9.7006 - acc: 0.296 - ETA: 0s - loss: 9.7061 - acc: 0.296 - ETA: 0s - loss: 9.7090 - acc: 0.296 - ETA: 0s - loss: 9.6947 - acc: 0.297 - ETA: 0s - loss: 9.6875 - acc: 0.297 - ETA: 0s - loss: 9.6842 - acc: 0.298 - ETA: 0s - loss: 9.6660 - acc: 0.299 - ETA: 0s - loss: 9.6567 - acc: 0.300 - ETA: 0s - loss: 9.6537 - acc: 0.301 - ETA: 0s - loss: 9.6317 - acc: 0.301 - ETA: 0s - loss: 9.6368 - acc: 0.302 - ETA: 0s - loss: 9.6226 - acc: 0.303 - ETA: 0s - loss: 9.6166 - acc: 0.304 - ETA: 0s - loss: 9.6191 - acc: 0.304 - ETA: 0s - loss: 9.6026 - acc: 0.306 - 4s 646us/step - loss: 9.6028 - acc: 0.3063 - val_loss: 9.4944 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.36473 to 9.49436, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 10.8599 - acc: 0.25 - ETA: 3s - loss: 10.1718 - acc: 0.32 - ETA: 3s - loss: 10.1804 - acc: 0.32 - ETA: 3s - loss: 10.1261 - acc: 0.33 - ETA: 3s - loss: 9.5445 - acc: 0.3722 - ETA: 3s - loss: 9.5128 - acc: 0.365 - ETA: 3s - loss: 9.4761 - acc: 0.364 - ETA: 3s - loss: 9.3847 - acc: 0.369 - ETA: 3s - loss: 9.4155 - acc: 0.367 - ETA: 3s - loss: 9.4821 - acc: 0.361 - ETA: 3s - loss: 9.4576 - acc: 0.359 - ETA: 3s - loss: 9.4419 - acc: 0.362 - ETA: 3s - loss: 9.2747 - acc: 0.371 - ETA: 3s - loss: 9.1598 - acc: 0.377 - ETA: 3s - loss: 9.1364 - acc: 0.378 - ETA: 3s - loss: 9.0851 - acc: 0.378 - ETA: 3s - loss: 9.0166 - acc: 0.380 - ETA: 3s - loss: 9.0872 - acc: 0.376 - ETA: 3s - loss: 9.0823 - acc: 0.378 - ETA: 2s - loss: 9.0545 - acc: 0.381 - ETA: 2s - loss: 9.0776 - acc: 0.378 - ETA: 2s - loss: 9.1450 - acc: 0.373 - ETA: 2s - loss: 9.1673 - acc: 0.372 - ETA: 2s - loss: 9.2221 - acc: 0.369 - ETA: 2s - loss: 9.2029 - acc: 0.369 - ETA: 2s - loss: 9.2142 - acc: 0.368 - ETA: 2s - loss: 9.1417 - acc: 0.372 - ETA: 2s - loss: 9.1192 - acc: 0.372 - ETA: 2s - loss: 9.1326 - acc: 0.372 - ETA: 2s - loss: 9.1294 - acc: 0.372 - ETA: 2s - loss: 9.0585 - acc: 0.376 - ETA: 2s - loss: 9.0866 - acc: 0.375 - ETA: 2s - loss: 9.0506 - acc: 0.377 - ETA: 2s - loss: 9.1061 - acc: 0.375 - ETA: 2s - loss: 9.0884 - acc: 0.376 - ETA: 2s - loss: 9.1191 - acc: 0.374 - ETA: 2s - loss: 9.1354 - acc: 0.373 - ETA: 1s - loss: 9.1038 - acc: 0.375 - ETA: 1s - loss: 9.0665 - acc: 0.377 - ETA: 1s - loss: 9.0836 - acc: 0.376 - ETA: 1s - loss: 9.0929 - acc: 0.376 - ETA: 1s - loss: 9.0652 - acc: 0.378 - ETA: 1s - loss: 9.0507 - acc: 0.378 - ETA: 1s - loss: 9.0644 - acc: 0.378 - ETA: 1s - loss: 9.0838 - acc: 0.377 - ETA: 1s - loss: 9.0954 - acc: 0.377 - ETA: 1s - loss: 9.0710 - acc: 0.378 - ETA: 1s - loss: 9.0529 - acc: 0.380 - ETA: 1s - loss: 9.0396 - acc: 0.381 - ETA: 1s - loss: 9.0386 - acc: 0.381 - ETA: 1s - loss: 9.0252 - acc: 0.382 - ETA: 1s - loss: 9.0221 - acc: 0.381 - ETA: 1s - loss: 9.0440 - acc: 0.380 - ETA: 1s - loss: 9.0376 - acc: 0.380 - ETA: 1s - loss: 9.0509 - acc: 0.380 - ETA: 0s - loss: 9.0755 - acc: 0.378 - ETA: 0s - loss: 9.0667 - acc: 0.378 - ETA: 0s - loss: 9.0803 - acc: 0.377 - ETA: 0s - loss: 9.0619 - acc: 0.378 - ETA: 0s - loss: 9.0830 - acc: 0.376 - ETA: 0s - loss: 9.0736 - acc: 0.377 - ETA: 0s - loss: 9.0680 - acc: 0.377 - ETA: 0s - loss: 9.0641 - acc: 0.377 - ETA: 0s - loss: 9.0618 - acc: 0.377 - ETA: 0s - loss: 9.0584 - acc: 0.376 - ETA: 0s - loss: 9.0568 - acc: 0.376 - ETA: 0s - loss: 9.0367 - acc: 0.378 - ETA: 0s - loss: 9.0424 - acc: 0.378 - ETA: 0s - loss: 9.0228 - acc: 0.379 - ETA: 0s - loss: 9.0300 - acc: 0.379 - ETA: 0s - loss: 9.0322 - acc: 0.378 - ETA: 0s - loss: 9.0345 - acc: 0.379 - ETA: 0s - loss: 9.0377 - acc: 0.378 - 4s 636us/step - loss: 9.0320 - acc: 0.3787 - val_loss: 9.2183 - val_acc: 0.3389\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.49436 to 9.21826, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 9.6968 - acc: 0.400 - ETA: 4s - loss: 9.3964 - acc: 0.375 - ETA: 4s - loss: 9.0833 - acc: 0.400 - ETA: 4s - loss: 8.6849 - acc: 0.417 - ETA: 4s - loss: 8.7784 - acc: 0.411 - ETA: 3s - loss: 8.6247 - acc: 0.428 - ETA: 3s - loss: 8.8233 - acc: 0.416 - ETA: 3s - loss: 8.6630 - acc: 0.426 - ETA: 3s - loss: 8.6153 - acc: 0.429 - ETA: 3s - loss: 8.4443 - acc: 0.435 - ETA: 3s - loss: 8.4419 - acc: 0.435 - ETA: 3s - loss: 8.4677 - acc: 0.430 - ETA: 3s - loss: 8.4661 - acc: 0.428 - ETA: 3s - loss: 8.5136 - acc: 0.424 - ETA: 3s - loss: 8.6451 - acc: 0.416 - ETA: 3s - loss: 8.6420 - acc: 0.415 - ETA: 3s - loss: 8.5647 - acc: 0.422 - ETA: 3s - loss: 8.6321 - acc: 0.416 - ETA: 3s - loss: 8.6255 - acc: 0.416 - ETA: 3s - loss: 8.6705 - acc: 0.413 - ETA: 2s - loss: 8.6934 - acc: 0.413 - ETA: 2s - loss: 8.7288 - acc: 0.412 - ETA: 2s - loss: 8.7901 - acc: 0.408 - ETA: 2s - loss: 8.7698 - acc: 0.408 - ETA: 2s - loss: 8.7685 - acc: 0.408 - ETA: 2s - loss: 8.7737 - acc: 0.409 - ETA: 2s - loss: 8.7606 - acc: 0.410 - ETA: 2s - loss: 8.7721 - acc: 0.410 - ETA: 2s - loss: 8.8223 - acc: 0.407 - ETA: 2s - loss: 8.8640 - acc: 0.404 - ETA: 2s - loss: 8.8079 - acc: 0.407 - ETA: 2s - loss: 8.7963 - acc: 0.409 - ETA: 2s - loss: 8.8215 - acc: 0.408 - ETA: 2s - loss: 8.8781 - acc: 0.405 - ETA: 2s - loss: 8.8861 - acc: 0.405 - ETA: 2s - loss: 8.8786 - acc: 0.405 - ETA: 2s - loss: 8.8636 - acc: 0.406 - ETA: 1s - loss: 8.8461 - acc: 0.406 - ETA: 1s - loss: 8.8274 - acc: 0.407 - ETA: 1s - loss: 8.8087 - acc: 0.409 - ETA: 1s - loss: 8.8200 - acc: 0.408 - ETA: 1s - loss: 8.8248 - acc: 0.408 - ETA: 1s - loss: 8.8427 - acc: 0.407 - ETA: 1s - loss: 8.8126 - acc: 0.409 - ETA: 1s - loss: 8.8212 - acc: 0.408 - ETA: 1s - loss: 8.8300 - acc: 0.408 - ETA: 1s - loss: 8.8265 - acc: 0.408 - ETA: 1s - loss: 8.8260 - acc: 0.407 - ETA: 1s - loss: 8.8235 - acc: 0.407 - ETA: 1s - loss: 8.8103 - acc: 0.408 - ETA: 1s - loss: 8.8264 - acc: 0.407 - ETA: 1s - loss: 8.8258 - acc: 0.407 - ETA: 1s - loss: 8.8054 - acc: 0.409 - ETA: 1s - loss: 8.8012 - acc: 0.409 - ETA: 0s - loss: 8.8053 - acc: 0.408 - ETA: 0s - loss: 8.8174 - acc: 0.408 - ETA: 0s - loss: 8.8142 - acc: 0.408 - ETA: 0s - loss: 8.7987 - acc: 0.409 - ETA: 0s - loss: 8.7914 - acc: 0.409 - ETA: 0s - loss: 8.7635 - acc: 0.411 - ETA: 0s - loss: 8.8041 - acc: 0.408 - ETA: 0s - loss: 8.8114 - acc: 0.408 - ETA: 0s - loss: 8.8158 - acc: 0.408 - ETA: 0s - loss: 8.8197 - acc: 0.408 - ETA: 0s - loss: 8.8241 - acc: 0.408 - ETA: 0s - loss: 8.8083 - acc: 0.409 - ETA: 0s - loss: 8.8083 - acc: 0.408 - ETA: 0s - loss: 8.7871 - acc: 0.410 - ETA: 0s - loss: 8.7827 - acc: 0.410 - ETA: 0s - loss: 8.8057 - acc: 0.408 - ETA: 0s - loss: 8.8047 - acc: 0.408 - ETA: 0s - loss: 8.7977 - acc: 0.408 - 4s 636us/step - loss: 8.7874 - acc: 0.4096 - val_loss: 9.1610 - val_acc: 0.3497\n",
      "\n",
      "Epoch 00004: val_loss improved from 9.21826 to 9.16100, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 9.7229 - acc: 0.300 - ETA: 3s - loss: 9.0238 - acc: 0.416 - ETA: 3s - loss: 8.7641 - acc: 0.436 - ETA: 3s - loss: 8.7729 - acc: 0.428 - ETA: 3s - loss: 8.6800 - acc: 0.426 - ETA: 3s - loss: 8.9362 - acc: 0.412 - ETA: 3s - loss: 9.0240 - acc: 0.405 - ETA: 3s - loss: 8.7052 - acc: 0.426 - ETA: 3s - loss: 8.8129 - acc: 0.423 - ETA: 3s - loss: 8.7819 - acc: 0.423 - ETA: 3s - loss: 8.8883 - acc: 0.417 - ETA: 3s - loss: 8.8709 - acc: 0.417 - ETA: 3s - loss: 8.8436 - acc: 0.416 - ETA: 3s - loss: 8.8109 - acc: 0.418 - ETA: 3s - loss: 8.7554 - acc: 0.420 - ETA: 3s - loss: 8.7361 - acc: 0.419 - ETA: 3s - loss: 8.7609 - acc: 0.418 - ETA: 3s - loss: 8.7519 - acc: 0.420 - ETA: 2s - loss: 8.8184 - acc: 0.416 - ETA: 2s - loss: 8.8397 - acc: 0.416 - ETA: 2s - loss: 8.8171 - acc: 0.417 - ETA: 2s - loss: 8.8269 - acc: 0.415 - ETA: 2s - loss: 8.7860 - acc: 0.417 - ETA: 2s - loss: 8.7547 - acc: 0.417 - ETA: 2s - loss: 8.7942 - acc: 0.416 - ETA: 2s - loss: 8.7537 - acc: 0.419 - ETA: 2s - loss: 8.7421 - acc: 0.419 - ETA: 2s - loss: 8.6718 - acc: 0.423 - ETA: 2s - loss: 8.6738 - acc: 0.422 - ETA: 2s - loss: 8.6854 - acc: 0.420 - ETA: 2s - loss: 8.6487 - acc: 0.423 - ETA: 2s - loss: 8.6344 - acc: 0.424 - ETA: 2s - loss: 8.6948 - acc: 0.420 - ETA: 2s - loss: 8.6788 - acc: 0.420 - ETA: 2s - loss: 8.6388 - acc: 0.421 - ETA: 1s - loss: 8.6309 - acc: 0.422 - ETA: 1s - loss: 8.5919 - acc: 0.424 - ETA: 1s - loss: 8.5831 - acc: 0.425 - ETA: 1s - loss: 8.5701 - acc: 0.426 - ETA: 1s - loss: 8.5610 - acc: 0.426 - ETA: 1s - loss: 8.5760 - acc: 0.425 - ETA: 1s - loss: 8.5542 - acc: 0.426 - ETA: 1s - loss: 8.5580 - acc: 0.426 - ETA: 1s - loss: 8.5622 - acc: 0.426 - ETA: 1s - loss: 8.5495 - acc: 0.427 - ETA: 1s - loss: 8.5636 - acc: 0.426 - ETA: 1s - loss: 8.5533 - acc: 0.427 - ETA: 1s - loss: 8.5395 - acc: 0.427 - ETA: 1s - loss: 8.5564 - acc: 0.426 - ETA: 1s - loss: 8.5475 - acc: 0.427 - ETA: 1s - loss: 8.5110 - acc: 0.429 - ETA: 1s - loss: 8.5236 - acc: 0.428 - ETA: 0s - loss: 8.5197 - acc: 0.429 - ETA: 0s - loss: 8.4983 - acc: 0.431 - ETA: 0s - loss: 8.5071 - acc: 0.431 - ETA: 0s - loss: 8.4849 - acc: 0.432 - ETA: 0s - loss: 8.4999 - acc: 0.431 - ETA: 0s - loss: 8.4973 - acc: 0.431 - ETA: 0s - loss: 8.4800 - acc: 0.431 - ETA: 0s - loss: 8.5111 - acc: 0.430 - ETA: 0s - loss: 8.5109 - acc: 0.430 - ETA: 0s - loss: 8.5010 - acc: 0.431 - ETA: 0s - loss: 8.5126 - acc: 0.430 - ETA: 0s - loss: 8.4992 - acc: 0.431 - ETA: 0s - loss: 8.5024 - acc: 0.430 - ETA: 0s - loss: 8.5192 - acc: 0.429 - ETA: 0s - loss: 8.5138 - acc: 0.429 - ETA: 0s - loss: 8.5141 - acc: 0.429 - ETA: 0s - loss: 8.5140 - acc: 0.429 - ETA: 0s - loss: 8.5078 - acc: 0.430 - 4s 624us/step - loss: 8.4968 - acc: 0.4302 - val_loss: 8.6793 - val_acc: 0.3820\n",
      "\n",
      "Epoch 00005: val_loss improved from 9.16100 to 8.67934, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 8.8652 - acc: 0.450 - ETA: 3s - loss: 9.2004 - acc: 0.416 - ETA: 3s - loss: 9.0106 - acc: 0.422 - ETA: 3s - loss: 8.5203 - acc: 0.446 - ETA: 3s - loss: 8.4868 - acc: 0.450 - ETA: 3s - loss: 8.3177 - acc: 0.462 - ETA: 3s - loss: 8.2366 - acc: 0.461 - ETA: 3s - loss: 8.1976 - acc: 0.462 - ETA: 3s - loss: 8.2463 - acc: 0.461 - ETA: 3s - loss: 8.2303 - acc: 0.461 - ETA: 3s - loss: 8.3914 - acc: 0.452 - ETA: 3s - loss: 8.3195 - acc: 0.457 - ETA: 3s - loss: 8.2625 - acc: 0.461 - ETA: 3s - loss: 8.2191 - acc: 0.464 - ETA: 3s - loss: 8.2010 - acc: 0.466 - ETA: 3s - loss: 8.2636 - acc: 0.463 - ETA: 2s - loss: 8.2532 - acc: 0.464 - ETA: 2s - loss: 8.2154 - acc: 0.466 - ETA: 2s - loss: 8.1719 - acc: 0.468 - ETA: 2s - loss: 8.1601 - acc: 0.468 - ETA: 2s - loss: 8.1403 - acc: 0.469 - ETA: 2s - loss: 8.1733 - acc: 0.466 - ETA: 2s - loss: 8.1830 - acc: 0.465 - ETA: 2s - loss: 8.1861 - acc: 0.464 - ETA: 2s - loss: 8.1768 - acc: 0.465 - ETA: 2s - loss: 8.2574 - acc: 0.461 - ETA: 2s - loss: 8.2770 - acc: 0.459 - ETA: 2s - loss: 8.2722 - acc: 0.460 - ETA: 2s - loss: 8.2977 - acc: 0.458 - ETA: 2s - loss: 8.3095 - acc: 0.458 - ETA: 2s - loss: 8.3085 - acc: 0.456 - ETA: 2s - loss: 8.2824 - acc: 0.457 - ETA: 2s - loss: 8.2855 - acc: 0.457 - ETA: 2s - loss: 8.2969 - acc: 0.456 - ETA: 1s - loss: 8.2963 - acc: 0.457 - ETA: 1s - loss: 8.2965 - acc: 0.457 - ETA: 1s - loss: 8.3103 - acc: 0.456 - ETA: 1s - loss: 8.2989 - acc: 0.457 - ETA: 1s - loss: 8.3165 - acc: 0.456 - ETA: 1s - loss: 8.3064 - acc: 0.457 - ETA: 1s - loss: 8.3140 - acc: 0.457 - ETA: 1s - loss: 8.3132 - acc: 0.456 - ETA: 1s - loss: 8.3181 - acc: 0.456 - ETA: 1s - loss: 8.3290 - acc: 0.455 - ETA: 1s - loss: 8.3275 - acc: 0.455 - ETA: 1s - loss: 8.3177 - acc: 0.456 - ETA: 1s - loss: 8.3290 - acc: 0.455 - ETA: 1s - loss: 8.3531 - acc: 0.454 - ETA: 1s - loss: 8.3377 - acc: 0.454 - ETA: 1s - loss: 8.3469 - acc: 0.454 - ETA: 1s - loss: 8.3425 - acc: 0.454 - ETA: 1s - loss: 8.3298 - acc: 0.454 - ETA: 0s - loss: 8.3367 - acc: 0.454 - ETA: 0s - loss: 8.3101 - acc: 0.455 - ETA: 0s - loss: 8.2979 - acc: 0.456 - ETA: 0s - loss: 8.2866 - acc: 0.457 - ETA: 0s - loss: 8.2848 - acc: 0.457 - ETA: 0s - loss: 8.2803 - acc: 0.457 - ETA: 0s - loss: 8.2813 - acc: 0.458 - ETA: 0s - loss: 8.2752 - acc: 0.458 - ETA: 0s - loss: 8.2498 - acc: 0.459 - ETA: 0s - loss: 8.2501 - acc: 0.459 - ETA: 0s - loss: 8.2578 - acc: 0.459 - ETA: 0s - loss: 8.2559 - acc: 0.458 - ETA: 0s - loss: 8.2558 - acc: 0.458 - ETA: 0s - loss: 8.2732 - acc: 0.457 - ETA: 0s - loss: 8.2735 - acc: 0.457 - ETA: 0s - loss: 8.2691 - acc: 0.458 - ETA: 0s - loss: 8.2642 - acc: 0.458 - ETA: 0s - loss: 8.2573 - acc: 0.459 - 4s 624us/step - loss: 8.2608 - acc: 0.4591 - val_loss: 8.6673 - val_acc: 0.3904\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.67934 to 8.66729, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 9s - loss: 7.3387 - acc: 0.500 - ETA: 4s - loss: 8.2254 - acc: 0.475 - ETA: 4s - loss: 8.6264 - acc: 0.440 - ETA: 4s - loss: 8.6212 - acc: 0.443 - ETA: 3s - loss: 8.2818 - acc: 0.467 - ETA: 3s - loss: 8.3863 - acc: 0.462 - ETA: 3s - loss: 8.1557 - acc: 0.476 - ETA: 3s - loss: 8.1834 - acc: 0.474 - ETA: 3s - loss: 8.0475 - acc: 0.482 - ETA: 3s - loss: 7.8604 - acc: 0.493 - ETA: 3s - loss: 7.9071 - acc: 0.491 - ETA: 3s - loss: 7.9172 - acc: 0.488 - ETA: 3s - loss: 7.9979 - acc: 0.485 - ETA: 3s - loss: 8.0463 - acc: 0.481 - ETA: 3s - loss: 7.9374 - acc: 0.487 - ETA: 3s - loss: 7.9956 - acc: 0.484 - ETA: 3s - loss: 7.9828 - acc: 0.485 - ETA: 2s - loss: 7.9257 - acc: 0.488 - ETA: 2s - loss: 7.9695 - acc: 0.485 - ETA: 2s - loss: 7.9215 - acc: 0.487 - ETA: 2s - loss: 7.9450 - acc: 0.487 - ETA: 2s - loss: 7.9829 - acc: 0.484 - ETA: 2s - loss: 7.9682 - acc: 0.484 - ETA: 2s - loss: 7.9947 - acc: 0.483 - ETA: 2s - loss: 7.9757 - acc: 0.484 - ETA: 2s - loss: 7.9731 - acc: 0.484 - ETA: 2s - loss: 7.9837 - acc: 0.484 - ETA: 2s - loss: 7.9948 - acc: 0.483 - ETA: 2s - loss: 8.0073 - acc: 0.483 - ETA: 2s - loss: 7.9933 - acc: 0.484 - ETA: 2s - loss: 7.9465 - acc: 0.486 - ETA: 2s - loss: 7.9349 - acc: 0.487 - ETA: 2s - loss: 7.9391 - acc: 0.488 - ETA: 2s - loss: 7.9022 - acc: 0.489 - ETA: 1s - loss: 7.9036 - acc: 0.490 - ETA: 1s - loss: 7.8994 - acc: 0.490 - ETA: 1s - loss: 7.9511 - acc: 0.487 - ETA: 1s - loss: 7.9954 - acc: 0.484 - ETA: 1s - loss: 7.9882 - acc: 0.484 - ETA: 1s - loss: 7.9895 - acc: 0.485 - ETA: 1s - loss: 8.0004 - acc: 0.484 - ETA: 1s - loss: 8.0121 - acc: 0.482 - ETA: 1s - loss: 8.0067 - acc: 0.483 - ETA: 1s - loss: 7.9930 - acc: 0.484 - ETA: 1s - loss: 7.9961 - acc: 0.484 - ETA: 1s - loss: 7.9654 - acc: 0.485 - ETA: 1s - loss: 7.9615 - acc: 0.486 - ETA: 1s - loss: 7.9925 - acc: 0.483 - ETA: 1s - loss: 8.0095 - acc: 0.482 - ETA: 1s - loss: 8.0184 - acc: 0.481 - ETA: 1s - loss: 8.0183 - acc: 0.481 - ETA: 0s - loss: 8.0421 - acc: 0.480 - ETA: 0s - loss: 8.0582 - acc: 0.479 - ETA: 0s - loss: 8.0939 - acc: 0.477 - ETA: 0s - loss: 8.0977 - acc: 0.477 - ETA: 0s - loss: 8.1110 - acc: 0.476 - ETA: 0s - loss: 8.1181 - acc: 0.476 - ETA: 0s - loss: 8.1184 - acc: 0.475 - ETA: 0s - loss: 8.1328 - acc: 0.474 - ETA: 0s - loss: 8.1280 - acc: 0.475 - ETA: 0s - loss: 8.1281 - acc: 0.474 - ETA: 0s - loss: 8.1595 - acc: 0.472 - ETA: 0s - loss: 8.1775 - acc: 0.471 - ETA: 0s - loss: 8.1741 - acc: 0.471 - ETA: 0s - loss: 8.1726 - acc: 0.471 - ETA: 0s - loss: 8.1997 - acc: 0.469 - ETA: 0s - loss: 8.1685 - acc: 0.471 - ETA: 0s - loss: 8.1631 - acc: 0.470 - ETA: 0s - loss: 8.1627 - acc: 0.470 - 4s 620us/step - loss: 8.1554 - acc: 0.4707 - val_loss: 8.4835 - val_acc: 0.4072\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.66729 to 8.48354, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 6.3441 - acc: 0.500 - ETA: 3s - loss: 6.8394 - acc: 0.558 - ETA: 3s - loss: 7.3105 - acc: 0.522 - ETA: 3s - loss: 7.6487 - acc: 0.509 - ETA: 3s - loss: 7.6911 - acc: 0.502 - ETA: 3s - loss: 7.8250 - acc: 0.498 - ETA: 3s - loss: 7.6859 - acc: 0.506 - ETA: 3s - loss: 7.7200 - acc: 0.505 - ETA: 3s - loss: 7.8436 - acc: 0.496 - ETA: 3s - loss: 7.8162 - acc: 0.500 - ETA: 3s - loss: 7.8066 - acc: 0.498 - ETA: 3s - loss: 7.7967 - acc: 0.499 - ETA: 3s - loss: 7.9411 - acc: 0.489 - ETA: 3s - loss: 7.8115 - acc: 0.498 - ETA: 3s - loss: 7.8069 - acc: 0.496 - ETA: 3s - loss: 7.8830 - acc: 0.491 - ETA: 3s - loss: 7.8843 - acc: 0.490 - ETA: 2s - loss: 7.9383 - acc: 0.487 - ETA: 2s - loss: 7.9451 - acc: 0.487 - ETA: 2s - loss: 7.9326 - acc: 0.487 - ETA: 2s - loss: 7.9025 - acc: 0.489 - ETA: 2s - loss: 7.8683 - acc: 0.491 - ETA: 2s - loss: 7.8074 - acc: 0.493 - ETA: 2s - loss: 7.9100 - acc: 0.486 - ETA: 2s - loss: 7.9131 - acc: 0.486 - ETA: 2s - loss: 7.8994 - acc: 0.488 - ETA: 2s - loss: 7.8809 - acc: 0.490 - ETA: 2s - loss: 7.8688 - acc: 0.490 - ETA: 2s - loss: 7.8595 - acc: 0.490 - ETA: 2s - loss: 7.8779 - acc: 0.488 - ETA: 2s - loss: 7.8834 - acc: 0.487 - ETA: 2s - loss: 7.8924 - acc: 0.487 - ETA: 2s - loss: 7.8920 - acc: 0.487 - ETA: 2s - loss: 7.9384 - acc: 0.484 - ETA: 2s - loss: 7.9509 - acc: 0.483 - ETA: 1s - loss: 7.9763 - acc: 0.481 - ETA: 1s - loss: 7.9980 - acc: 0.479 - ETA: 1s - loss: 8.0285 - acc: 0.478 - ETA: 1s - loss: 7.9977 - acc: 0.480 - ETA: 1s - loss: 8.0005 - acc: 0.480 - ETA: 1s - loss: 8.0057 - acc: 0.479 - ETA: 1s - loss: 7.9897 - acc: 0.479 - ETA: 1s - loss: 7.9857 - acc: 0.479 - ETA: 1s - loss: 7.9724 - acc: 0.480 - ETA: 1s - loss: 7.9769 - acc: 0.480 - ETA: 1s - loss: 7.9851 - acc: 0.479 - ETA: 1s - loss: 7.9722 - acc: 0.479 - ETA: 1s - loss: 7.9662 - acc: 0.479 - ETA: 1s - loss: 7.9487 - acc: 0.480 - ETA: 1s - loss: 7.9331 - acc: 0.481 - ETA: 1s - loss: 7.9418 - acc: 0.480 - ETA: 1s - loss: 7.9260 - acc: 0.480 - ETA: 1s - loss: 7.9001 - acc: 0.481 - ETA: 0s - loss: 7.9028 - acc: 0.480 - ETA: 0s - loss: 7.9227 - acc: 0.478 - ETA: 0s - loss: 7.9252 - acc: 0.478 - ETA: 0s - loss: 7.9326 - acc: 0.477 - ETA: 0s - loss: 7.9470 - acc: 0.476 - ETA: 0s - loss: 7.9407 - acc: 0.476 - ETA: 0s - loss: 7.9540 - acc: 0.476 - ETA: 0s - loss: 7.9393 - acc: 0.477 - ETA: 0s - loss: 7.9224 - acc: 0.478 - ETA: 0s - loss: 7.9051 - acc: 0.479 - ETA: 0s - loss: 7.9172 - acc: 0.478 - ETA: 0s - loss: 7.9255 - acc: 0.477 - ETA: 0s - loss: 7.9240 - acc: 0.478 - ETA: 0s - loss: 7.9029 - acc: 0.479 - ETA: 0s - loss: 7.8877 - acc: 0.480 - ETA: 0s - loss: 7.8742 - acc: 0.481 - ETA: 0s - loss: 7.8613 - acc: 0.482 - 4s 625us/step - loss: 7.8503 - acc: 0.4829 - val_loss: 8.1852 - val_acc: 0.4192\n",
      "\n",
      "Epoch 00008: val_loss improved from 8.48354 to 8.18524, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 8.8973 - acc: 0.450 - ETA: 3s - loss: 7.6904 - acc: 0.500 - ETA: 3s - loss: 7.3992 - acc: 0.518 - ETA: 3s - loss: 7.3638 - acc: 0.526 - ETA: 3s - loss: 7.5652 - acc: 0.510 - ETA: 3s - loss: 7.9858 - acc: 0.485 - ETA: 3s - loss: 7.9682 - acc: 0.482 - ETA: 3s - loss: 7.9900 - acc: 0.477 - ETA: 3s - loss: 7.8336 - acc: 0.484 - ETA: 3s - loss: 7.8299 - acc: 0.481 - ETA: 3s - loss: 7.9174 - acc: 0.471 - ETA: 3s - loss: 7.7881 - acc: 0.477 - ETA: 3s - loss: 7.7388 - acc: 0.483 - ETA: 3s - loss: 7.7069 - acc: 0.485 - ETA: 3s - loss: 7.5866 - acc: 0.492 - ETA: 3s - loss: 7.5821 - acc: 0.493 - ETA: 3s - loss: 7.5850 - acc: 0.491 - ETA: 3s - loss: 7.6070 - acc: 0.491 - ETA: 3s - loss: 7.6165 - acc: 0.490 - ETA: 2s - loss: 7.6249 - acc: 0.489 - ETA: 2s - loss: 7.6210 - acc: 0.490 - ETA: 2s - loss: 7.6372 - acc: 0.489 - ETA: 2s - loss: 7.6500 - acc: 0.489 - ETA: 2s - loss: 7.6259 - acc: 0.490 - ETA: 2s - loss: 7.6240 - acc: 0.490 - ETA: 2s - loss: 7.5830 - acc: 0.493 - ETA: 2s - loss: 7.5128 - acc: 0.497 - ETA: 2s - loss: 7.4905 - acc: 0.498 - ETA: 2s - loss: 7.4559 - acc: 0.501 - ETA: 2s - loss: 7.4389 - acc: 0.501 - ETA: 2s - loss: 7.4561 - acc: 0.500 - ETA: 2s - loss: 7.4529 - acc: 0.500 - ETA: 2s - loss: 7.4265 - acc: 0.501 - ETA: 2s - loss: 7.4635 - acc: 0.498 - ETA: 2s - loss: 7.4533 - acc: 0.497 - ETA: 2s - loss: 7.4478 - acc: 0.497 - ETA: 1s - loss: 7.4428 - acc: 0.497 - ETA: 1s - loss: 7.4332 - acc: 0.498 - ETA: 1s - loss: 7.4486 - acc: 0.497 - ETA: 1s - loss: 7.4451 - acc: 0.497 - ETA: 1s - loss: 7.4605 - acc: 0.496 - ETA: 1s - loss: 7.4327 - acc: 0.498 - ETA: 1s - loss: 7.4212 - acc: 0.499 - ETA: 1s - loss: 7.4150 - acc: 0.500 - ETA: 1s - loss: 7.4353 - acc: 0.498 - ETA: 1s - loss: 7.4141 - acc: 0.499 - ETA: 1s - loss: 7.4127 - acc: 0.499 - ETA: 1s - loss: 7.4128 - acc: 0.500 - ETA: 1s - loss: 7.3940 - acc: 0.501 - ETA: 1s - loss: 7.3757 - acc: 0.502 - ETA: 1s - loss: 7.3859 - acc: 0.502 - ETA: 1s - loss: 7.3780 - acc: 0.503 - ETA: 1s - loss: 7.3799 - acc: 0.503 - ETA: 0s - loss: 7.3480 - acc: 0.505 - ETA: 0s - loss: 7.3683 - acc: 0.504 - ETA: 0s - loss: 7.3625 - acc: 0.504 - ETA: 0s - loss: 7.3671 - acc: 0.504 - ETA: 0s - loss: 7.3645 - acc: 0.504 - ETA: 0s - loss: 7.3633 - acc: 0.504 - ETA: 0s - loss: 7.3643 - acc: 0.504 - ETA: 0s - loss: 7.3890 - acc: 0.502 - ETA: 0s - loss: 7.3631 - acc: 0.504 - ETA: 0s - loss: 7.3648 - acc: 0.504 - ETA: 0s - loss: 7.3548 - acc: 0.505 - ETA: 0s - loss: 7.3376 - acc: 0.506 - ETA: 0s - loss: 7.3472 - acc: 0.505 - ETA: 0s - loss: 7.3585 - acc: 0.505 - ETA: 0s - loss: 7.3619 - acc: 0.504 - ETA: 0s - loss: 7.3596 - acc: 0.504 - ETA: 0s - loss: 7.3622 - acc: 0.505 - 4s 626us/step - loss: 7.3778 - acc: 0.5042 - val_loss: 7.6456 - val_acc: 0.4419\n",
      "\n",
      "Epoch 00009: val_loss improved from 8.18524 to 7.64556, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 7.2550 - acc: 0.550 - ETA: 4s - loss: 6.1652 - acc: 0.610 - ETA: 4s - loss: 7.0683 - acc: 0.545 - ETA: 4s - loss: 7.4061 - acc: 0.510 - ETA: 3s - loss: 7.5739 - acc: 0.500 - ETA: 3s - loss: 7.4725 - acc: 0.508 - ETA: 3s - loss: 7.4060 - acc: 0.505 - ETA: 3s - loss: 7.6073 - acc: 0.494 - ETA: 3s - loss: 7.5275 - acc: 0.496 - ETA: 3s - loss: 7.4065 - acc: 0.504 - ETA: 3s - loss: 7.3380 - acc: 0.512 - ETA: 3s - loss: 7.3008 - acc: 0.513 - ETA: 3s - loss: 7.2512 - acc: 0.516 - ETA: 3s - loss: 7.3545 - acc: 0.511 - ETA: 3s - loss: 7.3408 - acc: 0.514 - ETA: 3s - loss: 7.3926 - acc: 0.510 - ETA: 3s - loss: 7.4292 - acc: 0.508 - ETA: 3s - loss: 7.4459 - acc: 0.508 - ETA: 2s - loss: 7.3619 - acc: 0.513 - ETA: 2s - loss: 7.3215 - acc: 0.517 - ETA: 2s - loss: 7.3022 - acc: 0.519 - ETA: 2s - loss: 7.3049 - acc: 0.519 - ETA: 2s - loss: 7.2840 - acc: 0.520 - ETA: 2s - loss: 7.2965 - acc: 0.520 - ETA: 2s - loss: 7.2862 - acc: 0.520 - ETA: 2s - loss: 7.2545 - acc: 0.522 - ETA: 2s - loss: 7.2803 - acc: 0.520 - ETA: 2s - loss: 7.3131 - acc: 0.516 - ETA: 2s - loss: 7.3367 - acc: 0.515 - ETA: 2s - loss: 7.2966 - acc: 0.518 - ETA: 2s - loss: 7.2510 - acc: 0.521 - ETA: 2s - loss: 7.2433 - acc: 0.522 - ETA: 2s - loss: 7.2501 - acc: 0.522 - ETA: 2s - loss: 7.2536 - acc: 0.521 - ETA: 2s - loss: 7.2377 - acc: 0.521 - ETA: 1s - loss: 7.2258 - acc: 0.521 - ETA: 1s - loss: 7.2024 - acc: 0.523 - ETA: 1s - loss: 7.1826 - acc: 0.524 - ETA: 1s - loss: 7.1827 - acc: 0.523 - ETA: 1s - loss: 7.1650 - acc: 0.524 - ETA: 1s - loss: 7.1654 - acc: 0.524 - ETA: 1s - loss: 7.1557 - acc: 0.524 - ETA: 1s - loss: 7.1716 - acc: 0.524 - ETA: 1s - loss: 7.1543 - acc: 0.525 - ETA: 1s - loss: 7.1536 - acc: 0.525 - ETA: 1s - loss: 7.1346 - acc: 0.527 - ETA: 1s - loss: 7.1333 - acc: 0.527 - ETA: 1s - loss: 7.1301 - acc: 0.527 - ETA: 1s - loss: 7.1551 - acc: 0.526 - ETA: 1s - loss: 7.1823 - acc: 0.524 - ETA: 1s - loss: 7.1990 - acc: 0.523 - ETA: 1s - loss: 7.1844 - acc: 0.524 - ETA: 1s - loss: 7.1923 - acc: 0.524 - ETA: 1s - loss: 7.1880 - acc: 0.524 - ETA: 0s - loss: 7.1763 - acc: 0.525 - ETA: 0s - loss: 7.1799 - acc: 0.524 - ETA: 0s - loss: 7.1727 - acc: 0.524 - ETA: 0s - loss: 7.1642 - acc: 0.525 - ETA: 0s - loss: 7.1719 - acc: 0.525 - ETA: 0s - loss: 7.1947 - acc: 0.523 - ETA: 0s - loss: 7.1979 - acc: 0.523 - ETA: 0s - loss: 7.1861 - acc: 0.523 - ETA: 0s - loss: 7.1645 - acc: 0.525 - ETA: 0s - loss: 7.1478 - acc: 0.526 - ETA: 0s - loss: 7.1402 - acc: 0.527 - ETA: 0s - loss: 7.1426 - acc: 0.526 - ETA: 0s - loss: 7.1299 - acc: 0.527 - ETA: 0s - loss: 7.1135 - acc: 0.528 - ETA: 0s - loss: 7.0973 - acc: 0.529 - ETA: 0s - loss: 7.0927 - acc: 0.530 - ETA: 0s - loss: 7.0978 - acc: 0.530 - 4s 634us/step - loss: 7.1106 - acc: 0.5293 - val_loss: 7.4581 - val_acc: 0.4587\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.64556 to 7.45811, saving model to saved_models/weights.best.VGG16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5a68cd30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 训练模型\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=10, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载具有最好验证loss的模型\n",
    "\n",
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型\n",
    "现在，我们可以测试此CNN在狗图像测试数据集中识别品种的效果如何。我们在下方打印出测试准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 44.6172%\n"
     ]
    }
   ],
   "source": [
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型预测狗的品种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # 提取bottleneck特征\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # 获取预测向量\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # 返回此模型预测的狗的品种\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## 步骤 5: 建立一个CNN来分类狗的品种（使用迁移学习）\n",
    "\n",
    "现在你将使用迁移学习来建立一个CNN，从而可以从图像中识别狗的品种。你的 CNN 在测试集上的准确率必须至少达到60%。\n",
    "\n",
    "在步骤4中，我们使用了迁移学习来创建一个使用基于 VGG-16 提取的特征向量来搭建一个 CNN。在本部分内容中，你必须使用另一个预训练模型来搭建一个 CNN。为了让这个任务更易实现，我们已经预先对目前 keras 中可用的几种网络进行了预训练：\n",
    "\n",
    "- [VGG-19](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/DLND+documents/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "这些文件被命名为为：\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "\n",
    "其中 `{network}` 可以是 `VGG19`、`Resnet50`、`InceptionV3` 或 `Xception` 中的一个。选择上方网络架构中的一个，下载相对应的bottleneck特征，并将所下载的文件保存在目录 `bottleneck_features/` 中。\n",
    "\n",
    "\n",
    "### 【练习】获取模型的特征向量\n",
    "\n",
    "在下方代码块中，通过运行下方代码提取训练、测试与验证集相对应的bottleneck特征。\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 从另一个预训练的CNN获取bottleneck特征\n",
    "bottleneck_features = np.load('bottleneck_features/DogVGG19Data.npz')\n",
    "train_VGG19 = bottleneck_features['train']\n",
    "valid_VGG19 = bottleneck_features['valid']\n",
    "test_VGG19 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【练习】模型架构\n",
    "\n",
    "建立一个CNN来分类狗品种。在你的代码单元块的最后，通过运行如下代码输出网络的结构：\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "---\n",
    "\n",
    "<a id='question6'></a>  \n",
    "\n",
    "### __问题 6:__ \n",
    "\n",
    "\n",
    "在下方的代码块中尝试使用 Keras 搭建最终的网络架构，并回答你实现最终 CNN 架构的步骤与每一步的作用，并描述你在迁移学习过程中，使用该网络架构的原因。\n",
    "\n",
    "\n",
    "__回答:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: 定义你的框架\n",
    "VGG19_model = Sequential()\n",
    "VGG19_model.add(GlobalAveragePooling2D(input_shape=train_VGG19.shape[1:]))\n",
    "VGG19_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 编译模型\n",
    "VGG19_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 【练习】训练模型\n",
    "\n",
    "<a id='question7'></a>  \n",
    "\n",
    "### __问题 7:__ \n",
    "\n",
    "在下方代码单元中训练你的模型。使用模型检查点（model checkpointing）来储存具有最低验证集 loss 的模型。\n",
    "\n",
    "当然，你也可以对训练集进行 [数据增强](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) 以优化模型的表现，不过这不是必须的步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 5.1579 - acc: 0.680 - ETA: 2s - loss: 4.7599 - acc: 0.705 - ETA: 2s - loss: 4.5438 - acc: 0.708 - ETA: 2s - loss: 4.2448 - acc: 0.730 - ETA: 2s - loss: 4.1828 - acc: 0.735 - ETA: 2s - loss: 4.3057 - acc: 0.728 - ETA: 2s - loss: 4.5421 - acc: 0.714 - ETA: 2s - loss: 4.4357 - acc: 0.721 - ETA: 1s - loss: 4.4322 - acc: 0.722 - ETA: 1s - loss: 4.3662 - acc: 0.726 - ETA: 1s - loss: 4.3527 - acc: 0.727 - ETA: 1s - loss: 4.3666 - acc: 0.726 - ETA: 1s - loss: 4.4121 - acc: 0.724 - ETA: 1s - loss: 4.3050 - acc: 0.731 - ETA: 1s - loss: 4.2909 - acc: 0.731 - ETA: 1s - loss: 4.2516 - acc: 0.733 - ETA: 1s - loss: 4.2357 - acc: 0.735 - ETA: 1s - loss: 4.2568 - acc: 0.733 - ETA: 1s - loss: 4.2923 - acc: 0.731 - ETA: 1s - loss: 4.3191 - acc: 0.730 - ETA: 1s - loss: 4.3118 - acc: 0.730 - ETA: 1s - loss: 4.2582 - acc: 0.733 - ETA: 1s - loss: 4.2510 - acc: 0.734 - ETA: 1s - loss: 4.3088 - acc: 0.730 - ETA: 0s - loss: 4.3356 - acc: 0.729 - ETA: 0s - loss: 4.3382 - acc: 0.729 - ETA: 0s - loss: 4.3103 - acc: 0.731 - ETA: 0s - loss: 4.3246 - acc: 0.730 - ETA: 0s - loss: 4.3013 - acc: 0.731 - ETA: 0s - loss: 4.3016 - acc: 0.731 - ETA: 0s - loss: 4.2950 - acc: 0.731 - ETA: 0s - loss: 4.3100 - acc: 0.731 - ETA: 0s - loss: 4.3035 - acc: 0.731 - ETA: 0s - loss: 4.2865 - acc: 0.732 - ETA: 0s - loss: 4.2956 - acc: 0.731 - ETA: 0s - loss: 4.2901 - acc: 0.732 - ETA: 0s - loss: 4.2738 - acc: 0.733 - ETA: 0s - loss: 4.2719 - acc: 0.733 - ETA: 0s - loss: 4.2543 - acc: 0.734 - ETA: 0s - loss: 4.2372 - acc: 0.735 - ETA: 0s - loss: 4.2313 - acc: 0.735 - ETA: 0s - loss: 4.2449 - acc: 0.735 - 2s 371us/step - loss: 4.2524 - acc: 0.7346 - val_loss: 5.2526 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.25256, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 2/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 5.4802 - acc: 0.660 - ETA: 2s - loss: 4.4328 - acc: 0.725 - ETA: 2s - loss: 4.1483 - acc: 0.740 - ETA: 2s - loss: 4.2900 - acc: 0.732 - ETA: 2s - loss: 4.2387 - acc: 0.735 - ETA: 1s - loss: 4.1924 - acc: 0.738 - ETA: 1s - loss: 4.4179 - acc: 0.725 - ETA: 1s - loss: 4.3462 - acc: 0.729 - ETA: 1s - loss: 4.3159 - acc: 0.731 - ETA: 1s - loss: 4.2585 - acc: 0.735 - ETA: 1s - loss: 4.1817 - acc: 0.740 - ETA: 1s - loss: 4.0904 - acc: 0.745 - ETA: 1s - loss: 4.1238 - acc: 0.743 - ETA: 1s - loss: 4.2078 - acc: 0.738 - ETA: 1s - loss: 4.1993 - acc: 0.739 - ETA: 1s - loss: 4.2056 - acc: 0.738 - ETA: 1s - loss: 4.2628 - acc: 0.735 - ETA: 1s - loss: 4.2466 - acc: 0.736 - ETA: 1s - loss: 4.1860 - acc: 0.740 - ETA: 1s - loss: 4.1535 - acc: 0.742 - ETA: 1s - loss: 4.1976 - acc: 0.739 - ETA: 1s - loss: 4.2122 - acc: 0.738 - ETA: 1s - loss: 4.2234 - acc: 0.737 - ETA: 1s - loss: 4.2171 - acc: 0.737 - ETA: 0s - loss: 4.2210 - acc: 0.737 - ETA: 0s - loss: 4.2484 - acc: 0.735 - ETA: 0s - loss: 4.2503 - acc: 0.735 - ETA: 0s - loss: 4.2331 - acc: 0.736 - ETA: 0s - loss: 4.2388 - acc: 0.736 - ETA: 0s - loss: 4.2125 - acc: 0.737 - ETA: 0s - loss: 4.2253 - acc: 0.736 - ETA: 0s - loss: 4.2451 - acc: 0.735 - ETA: 0s - loss: 4.2697 - acc: 0.733 - ETA: 0s - loss: 4.2858 - acc: 0.732 - ETA: 0s - loss: 4.2883 - acc: 0.732 - ETA: 0s - loss: 4.2915 - acc: 0.732 - ETA: 0s - loss: 4.2909 - acc: 0.732 - ETA: 0s - loss: 4.2802 - acc: 0.733 - ETA: 0s - loss: 4.2728 - acc: 0.733 - ETA: 0s - loss: 4.2842 - acc: 0.733 - ETA: 0s - loss: 4.2596 - acc: 0.734 - ETA: 0s - loss: 4.2752 - acc: 0.733 - 2s 362us/step - loss: 4.2500 - acc: 0.7350 - val_loss: 5.3455 - val_acc: 0.5689\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.25256\n",
      "Epoch 3/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.5134 - acc: 0.720 - ETA: 2s - loss: 4.4338 - acc: 0.725 - ETA: 2s - loss: 4.3758 - acc: 0.728 - ETA: 2s - loss: 4.4495 - acc: 0.724 - ETA: 2s - loss: 4.4642 - acc: 0.723 - ETA: 1s - loss: 4.3811 - acc: 0.728 - ETA: 1s - loss: 4.4815 - acc: 0.722 - ETA: 1s - loss: 4.3659 - acc: 0.729 - ETA: 1s - loss: 4.4215 - acc: 0.725 - ETA: 1s - loss: 4.3423 - acc: 0.730 - ETA: 1s - loss: 4.3794 - acc: 0.728 - ETA: 1s - loss: 4.3365 - acc: 0.731 - ETA: 1s - loss: 4.2664 - acc: 0.735 - ETA: 1s - loss: 4.2334 - acc: 0.737 - ETA: 1s - loss: 4.2637 - acc: 0.735 - ETA: 1s - loss: 4.2047 - acc: 0.738 - ETA: 1s - loss: 4.2046 - acc: 0.738 - ETA: 1s - loss: 4.1823 - acc: 0.739 - ETA: 1s - loss: 4.2140 - acc: 0.737 - ETA: 1s - loss: 4.1534 - acc: 0.741 - ETA: 1s - loss: 4.1791 - acc: 0.740 - ETA: 1s - loss: 4.1752 - acc: 0.740 - ETA: 0s - loss: 4.1630 - acc: 0.741 - ETA: 0s - loss: 4.1764 - acc: 0.740 - ETA: 0s - loss: 4.2058 - acc: 0.738 - ETA: 0s - loss: 4.2015 - acc: 0.738 - ETA: 0s - loss: 4.1646 - acc: 0.740 - ETA: 0s - loss: 4.1667 - acc: 0.740 - ETA: 0s - loss: 4.1915 - acc: 0.738 - ETA: 0s - loss: 4.2154 - acc: 0.737 - ETA: 0s - loss: 4.1987 - acc: 0.738 - ETA: 0s - loss: 4.1924 - acc: 0.738 - ETA: 0s - loss: 4.1746 - acc: 0.739 - ETA: 0s - loss: 4.1952 - acc: 0.738 - ETA: 0s - loss: 4.2127 - acc: 0.737 - ETA: 0s - loss: 4.1986 - acc: 0.738 - ETA: 0s - loss: 4.1905 - acc: 0.738 - ETA: 0s - loss: 4.2086 - acc: 0.737 - ETA: 0s - loss: 4.2309 - acc: 0.736 - ETA: 0s - loss: 4.2541 - acc: 0.734 - 2s 357us/step - loss: 4.2490 - acc: 0.7352 - val_loss: 5.3211 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.25256\n",
      "Epoch 4/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 7.0928 - acc: 0.560 - ETA: 2s - loss: 4.3525 - acc: 0.730 - ETA: 2s - loss: 4.0530 - acc: 0.748 - ETA: 2s - loss: 4.1631 - acc: 0.740 - ETA: 2s - loss: 4.0789 - acc: 0.745 - ETA: 1s - loss: 4.0424 - acc: 0.748 - ETA: 1s - loss: 4.1775 - acc: 0.740 - ETA: 1s - loss: 4.1652 - acc: 0.740 - ETA: 1s - loss: 4.2168 - acc: 0.737 - ETA: 1s - loss: 4.3278 - acc: 0.731 - ETA: 1s - loss: 4.3632 - acc: 0.728 - ETA: 1s - loss: 4.3056 - acc: 0.732 - ETA: 1s - loss: 4.4260 - acc: 0.725 - ETA: 1s - loss: 4.3646 - acc: 0.728 - ETA: 1s - loss: 4.4165 - acc: 0.725 - ETA: 1s - loss: 4.4027 - acc: 0.726 - ETA: 1s - loss: 4.3685 - acc: 0.728 - ETA: 1s - loss: 4.3360 - acc: 0.730 - ETA: 1s - loss: 4.3177 - acc: 0.731 - ETA: 1s - loss: 4.3168 - acc: 0.731 - ETA: 1s - loss: 4.2994 - acc: 0.733 - ETA: 1s - loss: 4.2620 - acc: 0.735 - ETA: 1s - loss: 4.2814 - acc: 0.734 - ETA: 0s - loss: 4.3036 - acc: 0.732 - ETA: 0s - loss: 4.2910 - acc: 0.733 - ETA: 0s - loss: 4.3115 - acc: 0.732 - ETA: 0s - loss: 4.2881 - acc: 0.733 - ETA: 0s - loss: 4.2746 - acc: 0.734 - ETA: 0s - loss: 4.2646 - acc: 0.734 - ETA: 0s - loss: 4.2821 - acc: 0.733 - ETA: 0s - loss: 4.2759 - acc: 0.734 - ETA: 0s - loss: 4.2694 - acc: 0.734 - ETA: 0s - loss: 4.2664 - acc: 0.734 - ETA: 0s - loss: 4.2464 - acc: 0.735 - ETA: 0s - loss: 4.2439 - acc: 0.736 - ETA: 0s - loss: 4.2197 - acc: 0.737 - ETA: 0s - loss: 4.2193 - acc: 0.737 - ETA: 0s - loss: 4.2303 - acc: 0.736 - ETA: 0s - loss: 4.2242 - acc: 0.737 - ETA: 0s - loss: 4.2259 - acc: 0.737 - ETA: 0s - loss: 4.2327 - acc: 0.736 - 2s 359us/step - loss: 4.2499 - acc: 0.7355 - val_loss: 5.3471 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 5.25256\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 4.5133 - acc: 0.720 - ETA: 2s - loss: 4.0298 - acc: 0.750 - ETA: 2s - loss: 4.2371 - acc: 0.737 - ETA: 2s - loss: 4.2235 - acc: 0.738 - ETA: 2s - loss: 4.3896 - acc: 0.727 - ETA: 1s - loss: 4.0774 - acc: 0.747 - ETA: 1s - loss: 4.1451 - acc: 0.742 - ETA: 1s - loss: 4.1267 - acc: 0.744 - ETA: 1s - loss: 4.0646 - acc: 0.747 - ETA: 1s - loss: 4.1392 - acc: 0.743 - ETA: 1s - loss: 4.2196 - acc: 0.738 - ETA: 1s - loss: 4.2086 - acc: 0.738 - ETA: 1s - loss: 4.1831 - acc: 0.740 - ETA: 1s - loss: 4.1836 - acc: 0.740 - ETA: 1s - loss: 4.2682 - acc: 0.735 - ETA: 1s - loss: 4.2651 - acc: 0.735 - ETA: 1s - loss: 4.2236 - acc: 0.737 - ETA: 1s - loss: 4.1756 - acc: 0.740 - ETA: 1s - loss: 4.1991 - acc: 0.738 - ETA: 1s - loss: 4.2299 - acc: 0.736 - ETA: 1s - loss: 4.2289 - acc: 0.736 - ETA: 1s - loss: 4.2556 - acc: 0.735 - ETA: 1s - loss: 4.2756 - acc: 0.733 - ETA: 1s - loss: 4.2809 - acc: 0.733 - ETA: 0s - loss: 4.2942 - acc: 0.732 - ETA: 0s - loss: 4.3011 - acc: 0.732 - ETA: 0s - loss: 4.3280 - acc: 0.730 - ETA: 0s - loss: 4.3087 - acc: 0.731 - ETA: 0s - loss: 4.3318 - acc: 0.730 - ETA: 0s - loss: 4.3100 - acc: 0.731 - ETA: 0s - loss: 4.2866 - acc: 0.732 - ETA: 0s - loss: 4.2902 - acc: 0.732 - ETA: 0s - loss: 4.2652 - acc: 0.733 - ETA: 0s - loss: 4.2661 - acc: 0.733 - ETA: 0s - loss: 4.2790 - acc: 0.733 - ETA: 0s - loss: 4.2853 - acc: 0.732 - ETA: 0s - loss: 4.3083 - acc: 0.731 - ETA: 0s - loss: 4.3053 - acc: 0.731 - ETA: 0s - loss: 4.2959 - acc: 0.732 - ETA: 0s - loss: 4.2829 - acc: 0.733 - ETA: 0s - loss: 4.2782 - acc: 0.733 - ETA: 0s - loss: 4.2637 - acc: 0.734 - ETA: 0s - loss: 4.2621 - acc: 0.734 - 2s 372us/step - loss: 4.2496 - acc: 0.7352 - val_loss: 5.3306 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.25256\n",
      "Epoch 6/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 3.2236 - acc: 0.800 - ETA: 2s - loss: 4.1101 - acc: 0.745 - ETA: 2s - loss: 4.5131 - acc: 0.720 - ETA: 2s - loss: 4.7710 - acc: 0.704 - ETA: 2s - loss: 4.5379 - acc: 0.718 - ETA: 2s - loss: 4.7407 - acc: 0.705 - ETA: 1s - loss: 4.7871 - acc: 0.703 - ETA: 1s - loss: 4.7234 - acc: 0.707 - ETA: 1s - loss: 4.7239 - acc: 0.706 - ETA: 1s - loss: 4.5669 - acc: 0.716 - ETA: 1s - loss: 4.5447 - acc: 0.717 - ETA: 1s - loss: 4.5326 - acc: 0.718 - ETA: 1s - loss: 4.4453 - acc: 0.723 - ETA: 1s - loss: 4.4074 - acc: 0.725 - ETA: 1s - loss: 4.3894 - acc: 0.726 - ETA: 1s - loss: 4.4472 - acc: 0.723 - ETA: 1s - loss: 4.3853 - acc: 0.727 - ETA: 1s - loss: 4.3719 - acc: 0.727 - ETA: 1s - loss: 4.3498 - acc: 0.729 - ETA: 1s - loss: 4.3474 - acc: 0.729 - ETA: 1s - loss: 4.3310 - acc: 0.730 - ETA: 1s - loss: 4.2919 - acc: 0.733 - ETA: 0s - loss: 4.2793 - acc: 0.733 - ETA: 0s - loss: 4.2926 - acc: 0.733 - ETA: 0s - loss: 4.3286 - acc: 0.730 - ETA: 0s - loss: 4.3007 - acc: 0.732 - ETA: 0s - loss: 4.3191 - acc: 0.731 - ETA: 0s - loss: 4.3041 - acc: 0.732 - ETA: 0s - loss: 4.2695 - acc: 0.734 - ETA: 0s - loss: 4.2963 - acc: 0.732 - ETA: 0s - loss: 4.2985 - acc: 0.732 - ETA: 0s - loss: 4.3005 - acc: 0.732 - ETA: 0s - loss: 4.2647 - acc: 0.734 - ETA: 0s - loss: 4.2764 - acc: 0.734 - ETA: 0s - loss: 4.2826 - acc: 0.733 - ETA: 0s - loss: 4.2857 - acc: 0.733 - ETA: 0s - loss: 4.2755 - acc: 0.734 - ETA: 0s - loss: 4.2667 - acc: 0.734 - ETA: 0s - loss: 4.2598 - acc: 0.735 - ETA: 0s - loss: 4.2408 - acc: 0.736 - 2s 360us/step - loss: 4.2472 - acc: 0.7359 - val_loss: 5.3832 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 5.25256\n",
      "Epoch 7/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5461 - acc: 0.780 - ETA: 2s - loss: 3.8691 - acc: 0.760 - ETA: 2s - loss: 3.9896 - acc: 0.752 - ETA: 2s - loss: 4.0750 - acc: 0.747 - ETA: 2s - loss: 4.2778 - acc: 0.734 - ETA: 1s - loss: 4.1577 - acc: 0.742 - ETA: 1s - loss: 4.1915 - acc: 0.740 - ETA: 1s - loss: 4.0550 - acc: 0.748 - ETA: 1s - loss: 4.0468 - acc: 0.749 - ETA: 1s - loss: 4.0402 - acc: 0.749 - ETA: 1s - loss: 3.9886 - acc: 0.752 - ETA: 1s - loss: 4.0672 - acc: 0.747 - ETA: 1s - loss: 4.1067 - acc: 0.745 - ETA: 1s - loss: 4.1195 - acc: 0.744 - ETA: 1s - loss: 4.0332 - acc: 0.749 - ETA: 1s - loss: 4.1415 - acc: 0.743 - ETA: 1s - loss: 4.1735 - acc: 0.741 - ETA: 1s - loss: 4.2081 - acc: 0.738 - ETA: 1s - loss: 4.1808 - acc: 0.740 - ETA: 1s - loss: 4.1666 - acc: 0.741 - ETA: 1s - loss: 4.1772 - acc: 0.740 - ETA: 1s - loss: 4.1778 - acc: 0.740 - ETA: 0s - loss: 4.2301 - acc: 0.737 - ETA: 0s - loss: 4.2118 - acc: 0.738 - ETA: 0s - loss: 4.2189 - acc: 0.738 - ETA: 0s - loss: 4.2444 - acc: 0.736 - ETA: 0s - loss: 4.2240 - acc: 0.737 - ETA: 0s - loss: 4.2265 - acc: 0.737 - ETA: 0s - loss: 4.1983 - acc: 0.739 - ETA: 0s - loss: 4.2338 - acc: 0.737 - ETA: 0s - loss: 4.2267 - acc: 0.737 - ETA: 0s - loss: 4.2321 - acc: 0.736 - ETA: 0s - loss: 4.2250 - acc: 0.737 - ETA: 0s - loss: 4.2595 - acc: 0.735 - ETA: 0s - loss: 4.2409 - acc: 0.736 - ETA: 0s - loss: 4.2529 - acc: 0.735 - ETA: 0s - loss: 4.2514 - acc: 0.735 - ETA: 0s - loss: 4.2551 - acc: 0.735 - ETA: 0s - loss: 4.2636 - acc: 0.734 - ETA: 0s - loss: 4.2572 - acc: 0.734 - 2s 354us/step - loss: 4.2478 - acc: 0.7353 - val_loss: 5.3581 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.25256\n",
      "Epoch 8/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 5.1578 - acc: 0.680 - ETA: 2s - loss: 4.1908 - acc: 0.740 - ETA: 2s - loss: 4.1908 - acc: 0.740 - ETA: 1s - loss: 4.0565 - acc: 0.748 - ETA: 1s - loss: 3.8686 - acc: 0.760 - ETA: 1s - loss: 4.1570 - acc: 0.742 - ETA: 1s - loss: 4.1489 - acc: 0.742 - ETA: 1s - loss: 4.2529 - acc: 0.736 - ETA: 1s - loss: 4.3689 - acc: 0.729 - ETA: 1s - loss: 4.4935 - acc: 0.721 - ETA: 1s - loss: 4.4424 - acc: 0.724 - ETA: 1s - loss: 4.4656 - acc: 0.723 - ETA: 1s - loss: 4.4389 - acc: 0.724 - ETA: 1s - loss: 4.4298 - acc: 0.725 - ETA: 1s - loss: 4.3978 - acc: 0.727 - ETA: 1s - loss: 4.4182 - acc: 0.725 - ETA: 1s - loss: 4.4063 - acc: 0.726 - ETA: 1s - loss: 4.3708 - acc: 0.728 - ETA: 1s - loss: 4.3898 - acc: 0.727 - ETA: 1s - loss: 4.3686 - acc: 0.728 - ETA: 1s - loss: 4.3945 - acc: 0.727 - ETA: 0s - loss: 4.3564 - acc: 0.729 - ETA: 0s - loss: 4.3748 - acc: 0.728 - ETA: 0s - loss: 4.3609 - acc: 0.729 - ETA: 0s - loss: 4.3971 - acc: 0.726 - ETA: 0s - loss: 4.3937 - acc: 0.727 - ETA: 0s - loss: 4.3777 - acc: 0.728 - ETA: 0s - loss: 4.3664 - acc: 0.728 - ETA: 0s - loss: 4.4017 - acc: 0.726 - ETA: 0s - loss: 4.3622 - acc: 0.729 - ETA: 0s - loss: 4.3498 - acc: 0.729 - ETA: 0s - loss: 4.3220 - acc: 0.731 - ETA: 0s - loss: 4.2957 - acc: 0.733 - ETA: 0s - loss: 4.2784 - acc: 0.734 - ETA: 0s - loss: 4.2732 - acc: 0.734 - ETA: 0s - loss: 4.2582 - acc: 0.735 - ETA: 0s - loss: 4.2460 - acc: 0.736 - ETA: 0s - loss: 4.2495 - acc: 0.735 - 2s 350us/step - loss: 4.2469 - acc: 0.7361 - val_loss: 5.3416 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.25256\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 3.2557 - acc: 0.780 - ETA: 2s - loss: 3.7154 - acc: 0.765 - ETA: 2s - loss: 3.9202 - acc: 0.754 - ETA: 2s - loss: 4.0799 - acc: 0.743 - ETA: 1s - loss: 4.0707 - acc: 0.744 - ETA: 1s - loss: 4.0960 - acc: 0.743 - ETA: 1s - loss: 4.0845 - acc: 0.744 - ETA: 1s - loss: 4.0047 - acc: 0.749 - ETA: 1s - loss: 3.9373 - acc: 0.754 - ETA: 1s - loss: 3.9897 - acc: 0.750 - ETA: 1s - loss: 4.0602 - acc: 0.746 - ETA: 1s - loss: 4.0867 - acc: 0.745 - ETA: 1s - loss: 4.1249 - acc: 0.742 - ETA: 1s - loss: 4.0863 - acc: 0.745 - ETA: 1s - loss: 4.0727 - acc: 0.746 - ETA: 1s - loss: 4.0447 - acc: 0.748 - ETA: 1s - loss: 4.0351 - acc: 0.748 - ETA: 1s - loss: 4.0456 - acc: 0.748 - ETA: 1s - loss: 4.0555 - acc: 0.747 - ETA: 1s - loss: 4.1105 - acc: 0.743 - ETA: 1s - loss: 4.0737 - acc: 0.746 - ETA: 0s - loss: 4.0715 - acc: 0.746 - ETA: 0s - loss: 4.0929 - acc: 0.744 - ETA: 0s - loss: 4.0885 - acc: 0.745 - ETA: 0s - loss: 4.1117 - acc: 0.743 - ETA: 0s - loss: 4.1079 - acc: 0.744 - ETA: 0s - loss: 4.1542 - acc: 0.741 - ETA: 0s - loss: 4.1794 - acc: 0.739 - ETA: 0s - loss: 4.2296 - acc: 0.736 - ETA: 0s - loss: 4.2381 - acc: 0.736 - ETA: 0s - loss: 4.2350 - acc: 0.736 - ETA: 0s - loss: 4.2613 - acc: 0.734 - ETA: 0s - loss: 4.2502 - acc: 0.735 - ETA: 0s - loss: 4.2762 - acc: 0.733 - ETA: 0s - loss: 4.2489 - acc: 0.735 - ETA: 0s - loss: 4.2686 - acc: 0.734 - ETA: 0s - loss: 4.2642 - acc: 0.734 - ETA: 0s - loss: 4.2700 - acc: 0.734 - ETA: 0s - loss: 4.2535 - acc: 0.735 - 2s 351us/step - loss: 4.2480 - acc: 0.7355 - val_loss: 5.3798 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.25256\n",
      "Epoch 10/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5460 - acc: 0.780 - ETA: 2s - loss: 4.5131 - acc: 0.720 - ETA: 2s - loss: 4.4210 - acc: 0.725 - ETA: 2s - loss: 4.3209 - acc: 0.732 - ETA: 2s - loss: 4.6139 - acc: 0.713 - ETA: 2s - loss: 4.4764 - acc: 0.722 - ETA: 1s - loss: 4.3530 - acc: 0.730 - ETA: 1s - loss: 4.3599 - acc: 0.729 - ETA: 1s - loss: 4.3276 - acc: 0.731 - ETA: 1s - loss: 4.2420 - acc: 0.736 - ETA: 1s - loss: 4.2080 - acc: 0.738 - ETA: 1s - loss: 4.2515 - acc: 0.735 - ETA: 1s - loss: 4.2964 - acc: 0.732 - ETA: 1s - loss: 4.2588 - acc: 0.734 - ETA: 1s - loss: 4.2113 - acc: 0.737 - ETA: 1s - loss: 4.2705 - acc: 0.734 - ETA: 1s - loss: 4.2595 - acc: 0.734 - ETA: 1s - loss: 4.2442 - acc: 0.735 - ETA: 1s - loss: 4.2304 - acc: 0.736 - ETA: 1s - loss: 4.2228 - acc: 0.737 - ETA: 1s - loss: 4.2560 - acc: 0.735 - ETA: 1s - loss: 4.2247 - acc: 0.737 - ETA: 1s - loss: 4.2006 - acc: 0.738 - ETA: 0s - loss: 4.2086 - acc: 0.738 - ETA: 0s - loss: 4.2200 - acc: 0.737 - ETA: 0s - loss: 4.2346 - acc: 0.736 - ETA: 0s - loss: 4.2408 - acc: 0.736 - ETA: 0s - loss: 4.2462 - acc: 0.735 - ETA: 0s - loss: 4.2693 - acc: 0.734 - ETA: 0s - loss: 4.2942 - acc: 0.732 - ETA: 0s - loss: 4.3010 - acc: 0.732 - ETA: 0s - loss: 4.2913 - acc: 0.733 - ETA: 0s - loss: 4.2852 - acc: 0.733 - ETA: 0s - loss: 4.2673 - acc: 0.734 - ETA: 0s - loss: 4.2563 - acc: 0.735 - ETA: 0s - loss: 4.2624 - acc: 0.734 - ETA: 0s - loss: 4.2549 - acc: 0.735 - ETA: 0s - loss: 4.2714 - acc: 0.734 - ETA: 0s - loss: 4.2747 - acc: 0.733 - ETA: 0s - loss: 4.2634 - acc: 0.734 - ETA: 0s - loss: 4.2741 - acc: 0.733 - ETA: 0s - loss: 4.2384 - acc: 0.736 - 2s 362us/step - loss: 4.2427 - acc: 0.7358 - val_loss: 5.5153 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.25256\n",
      "Epoch 11/50\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 3.2367 - acc: 0.800 - ETA: 2s - loss: 4.2579 - acc: 0.736 - ETA: 2s - loss: 3.9058 - acc: 0.757 - ETA: 2s - loss: 3.8012 - acc: 0.761 - ETA: 2s - loss: 3.8155 - acc: 0.760 - ETA: 2s - loss: 4.0192 - acc: 0.747 - ETA: 2s - loss: 3.9023 - acc: 0.753 - ETA: 1s - loss: 4.0180 - acc: 0.746 - ETA: 1s - loss: 4.0914 - acc: 0.742 - ETA: 1s - loss: 4.1759 - acc: 0.737 - ETA: 1s - loss: 4.1956 - acc: 0.736 - ETA: 1s - loss: 4.1877 - acc: 0.736 - ETA: 1s - loss: 4.2414 - acc: 0.733 - ETA: 1s - loss: 4.2321 - acc: 0.733 - ETA: 1s - loss: 4.2143 - acc: 0.734 - ETA: 1s - loss: 4.1758 - acc: 0.736 - ETA: 1s - loss: 4.2475 - acc: 0.732 - ETA: 1s - loss: 4.2335 - acc: 0.733 - ETA: 1s - loss: 4.2587 - acc: 0.732 - ETA: 1s - loss: 4.2805 - acc: 0.730 - ETA: 1s - loss: 4.2942 - acc: 0.730 - ETA: 1s - loss: 4.2707 - acc: 0.730 - ETA: 0s - loss: 4.3064 - acc: 0.727 - ETA: 0s - loss: 4.3031 - acc: 0.728 - ETA: 0s - loss: 4.2795 - acc: 0.729 - ETA: 0s - loss: 4.2571 - acc: 0.730 - ETA: 0s - loss: 4.2413 - acc: 0.730 - ETA: 0s - loss: 4.2213 - acc: 0.731 - ETA: 0s - loss: 4.2409 - acc: 0.729 - ETA: 0s - loss: 4.2422 - acc: 0.729 - ETA: 0s - loss: 4.2029 - acc: 0.731 - ETA: 0s - loss: 4.1623 - acc: 0.734 - ETA: 0s - loss: 4.1621 - acc: 0.734 - ETA: 0s - loss: 4.1705 - acc: 0.733 - ETA: 0s - loss: 4.1910 - acc: 0.732 - ETA: 0s - loss: 4.1917 - acc: 0.731 - ETA: 0s - loss: 4.1931 - acc: 0.731 - ETA: 0s - loss: 4.1815 - acc: 0.731 - ETA: 0s - loss: 4.1663 - acc: 0.732 - ETA: 0s - loss: 4.1758 - acc: 0.731 - 2s 359us/step - loss: 4.1691 - acc: 0.7323 - val_loss: 5.3186 - val_acc: 0.5665\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.25256\n",
      "Epoch 12/50\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 3.0243 - acc: 0.800 - ETA: 2s - loss: 3.8527 - acc: 0.750 - ETA: 2s - loss: 3.5307 - acc: 0.775 - ETA: 2s - loss: 3.7675 - acc: 0.760 - ETA: 2s - loss: 3.8278 - acc: 0.753 - ETA: 1s - loss: 3.8045 - acc: 0.756 - ETA: 1s - loss: 3.8330 - acc: 0.753 - ETA: 1s - loss: 3.7148 - acc: 0.761 - ETA: 1s - loss: 3.8154 - acc: 0.756 - ETA: 1s - loss: 3.8826 - acc: 0.750 - ETA: 1s - loss: 3.8538 - acc: 0.751 - ETA: 1s - loss: 3.9307 - acc: 0.746 - ETA: 1s - loss: 3.9025 - acc: 0.748 - ETA: 1s - loss: 3.9418 - acc: 0.745 - ETA: 1s - loss: 3.9316 - acc: 0.745 - ETA: 1s - loss: 3.9795 - acc: 0.741 - ETA: 1s - loss: 3.9814 - acc: 0.741 - ETA: 1s - loss: 3.9647 - acc: 0.742 - ETA: 1s - loss: 3.9538 - acc: 0.742 - ETA: 1s - loss: 3.9821 - acc: 0.741 - ETA: 0s - loss: 3.9851 - acc: 0.741 - ETA: 0s - loss: 3.9847 - acc: 0.740 - ETA: 0s - loss: 3.9483 - acc: 0.742 - ETA: 0s - loss: 3.9689 - acc: 0.740 - ETA: 0s - loss: 4.0192 - acc: 0.737 - ETA: 0s - loss: 4.0268 - acc: 0.737 - ETA: 0s - loss: 4.0164 - acc: 0.737 - ETA: 0s - loss: 4.0244 - acc: 0.736 - ETA: 0s - loss: 4.0065 - acc: 0.738 - ETA: 0s - loss: 4.0099 - acc: 0.737 - ETA: 0s - loss: 3.9731 - acc: 0.740 - ETA: 0s - loss: 3.9612 - acc: 0.741 - ETA: 0s - loss: 3.9392 - acc: 0.742 - ETA: 0s - loss: 3.9435 - acc: 0.741 - ETA: 0s - loss: 3.9353 - acc: 0.742 - ETA: 0s - loss: 3.9446 - acc: 0.741 - ETA: 0s - loss: 3.9393 - acc: 0.742 - ETA: 0s - loss: 3.9347 - acc: 0.742 - ETA: 0s - loss: 3.9563 - acc: 0.741 - 2s 355us/step - loss: 3.9521 - acc: 0.7419 - val_loss: 5.3037 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.25256\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 3.7449 - acc: 0.740 - ETA: 2s - loss: 3.3580 - acc: 0.785 - ETA: 2s - loss: 2.8784 - acc: 0.814 - ETA: 2s - loss: 3.3458 - acc: 0.780 - ETA: 2s - loss: 3.2445 - acc: 0.787 - ETA: 2s - loss: 3.1647 - acc: 0.787 - ETA: 1s - loss: 3.2703 - acc: 0.781 - ETA: 1s - loss: 3.2506 - acc: 0.780 - ETA: 1s - loss: 3.2113 - acc: 0.783 - ETA: 1s - loss: 3.1895 - acc: 0.782 - ETA: 1s - loss: 3.2249 - acc: 0.780 - ETA: 1s - loss: 3.3015 - acc: 0.775 - ETA: 1s - loss: 3.4036 - acc: 0.770 - ETA: 1s - loss: 3.4804 - acc: 0.766 - ETA: 1s - loss: 3.5132 - acc: 0.764 - ETA: 1s - loss: 3.6008 - acc: 0.760 - ETA: 1s - loss: 3.6651 - acc: 0.756 - ETA: 1s - loss: 3.6473 - acc: 0.757 - ETA: 1s - loss: 3.6498 - acc: 0.757 - ETA: 1s - loss: 3.7151 - acc: 0.753 - ETA: 1s - loss: 3.7260 - acc: 0.752 - ETA: 1s - loss: 3.6871 - acc: 0.754 - ETA: 0s - loss: 3.6941 - acc: 0.754 - ETA: 0s - loss: 3.6825 - acc: 0.755 - ETA: 0s - loss: 3.6980 - acc: 0.753 - ETA: 0s - loss: 3.6922 - acc: 0.753 - ETA: 0s - loss: 3.6759 - acc: 0.755 - ETA: 0s - loss: 3.6965 - acc: 0.754 - ETA: 0s - loss: 3.7001 - acc: 0.754 - ETA: 0s - loss: 3.7129 - acc: 0.753 - ETA: 0s - loss: 3.7175 - acc: 0.753 - ETA: 0s - loss: 3.7361 - acc: 0.752 - ETA: 0s - loss: 3.7811 - acc: 0.750 - ETA: 0s - loss: 3.7916 - acc: 0.749 - ETA: 0s - loss: 3.7909 - acc: 0.749 - ETA: 0s - loss: 3.7785 - acc: 0.750 - ETA: 0s - loss: 3.7920 - acc: 0.750 - ETA: 0s - loss: 3.8010 - acc: 0.749 - ETA: 0s - loss: 3.8154 - acc: 0.748 - ETA: 0s - loss: 3.8173 - acc: 0.748 - 2s 355us/step - loss: 3.8288 - acc: 0.7481 - val_loss: 5.1020 - val_acc: 0.5916\n",
      "\n",
      "Epoch 00013: val_loss improved from 5.25256 to 5.10199, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 14/50\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 3.5470 - acc: 0.780 - ETA: 2s - loss: 3.9455 - acc: 0.735 - ETA: 2s - loss: 4.0144 - acc: 0.737 - ETA: 1s - loss: 3.9415 - acc: 0.745 - ETA: 1s - loss: 3.9846 - acc: 0.742 - ETA: 1s - loss: 4.1881 - acc: 0.728 - ETA: 1s - loss: 4.0581 - acc: 0.732 - ETA: 1s - loss: 3.9842 - acc: 0.737 - ETA: 1s - loss: 3.9714 - acc: 0.738 - ETA: 1s - loss: 3.8430 - acc: 0.748 - ETA: 1s - loss: 3.8749 - acc: 0.747 - ETA: 1s - loss: 3.7728 - acc: 0.753 - ETA: 1s - loss: 3.7459 - acc: 0.755 - ETA: 1s - loss: 3.7930 - acc: 0.753 - ETA: 1s - loss: 3.7480 - acc: 0.757 - ETA: 1s - loss: 3.6496 - acc: 0.763 - ETA: 1s - loss: 3.6518 - acc: 0.763 - ETA: 1s - loss: 3.6342 - acc: 0.765 - ETA: 1s - loss: 3.6710 - acc: 0.762 - ETA: 1s - loss: 3.6941 - acc: 0.761 - ETA: 1s - loss: 3.6721 - acc: 0.762 - ETA: 1s - loss: 3.6453 - acc: 0.763 - ETA: 1s - loss: 3.6457 - acc: 0.764 - ETA: 1s - loss: 3.6541 - acc: 0.763 - ETA: 0s - loss: 3.6473 - acc: 0.763 - ETA: 0s - loss: 3.6951 - acc: 0.760 - ETA: 0s - loss: 3.7043 - acc: 0.760 - ETA: 0s - loss: 3.7252 - acc: 0.759 - ETA: 0s - loss: 3.6928 - acc: 0.761 - ETA: 0s - loss: 3.7173 - acc: 0.760 - ETA: 0s - loss: 3.7219 - acc: 0.759 - ETA: 0s - loss: 3.7554 - acc: 0.757 - ETA: 0s - loss: 3.7486 - acc: 0.757 - ETA: 0s - loss: 3.7515 - acc: 0.757 - ETA: 0s - loss: 3.7545 - acc: 0.757 - ETA: 0s - loss: 3.7594 - acc: 0.756 - ETA: 0s - loss: 3.7741 - acc: 0.755 - ETA: 0s - loss: 3.7780 - acc: 0.755 - ETA: 0s - loss: 3.7882 - acc: 0.754 - ETA: 0s - loss: 3.7792 - acc: 0.755 - 2s 357us/step - loss: 3.7763 - acc: 0.7551 - val_loss: 5.1889 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.10199\n",
      "Epoch 15/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.3097 - acc: 0.820 - ETA: 2s - loss: 2.8453 - acc: 0.805 - ETA: 2s - loss: 3.3149 - acc: 0.780 - ETA: 2s - loss: 3.5483 - acc: 0.767 - ETA: 2s - loss: 3.6683 - acc: 0.758 - ETA: 1s - loss: 3.5597 - acc: 0.766 - ETA: 1s - loss: 3.5438 - acc: 0.769 - ETA: 1s - loss: 3.5082 - acc: 0.771 - ETA: 1s - loss: 3.4552 - acc: 0.775 - ETA: 1s - loss: 3.4162 - acc: 0.778 - ETA: 1s - loss: 3.4353 - acc: 0.775 - ETA: 1s - loss: 3.4396 - acc: 0.776 - ETA: 1s - loss: 3.4365 - acc: 0.776 - ETA: 1s - loss: 3.4832 - acc: 0.773 - ETA: 1s - loss: 3.4886 - acc: 0.773 - ETA: 1s - loss: 3.4682 - acc: 0.774 - ETA: 1s - loss: 3.4929 - acc: 0.773 - ETA: 1s - loss: 3.5301 - acc: 0.770 - ETA: 1s - loss: 3.5368 - acc: 0.770 - ETA: 1s - loss: 3.5446 - acc: 0.770 - ETA: 1s - loss: 3.4761 - acc: 0.774 - ETA: 1s - loss: 3.4839 - acc: 0.773 - ETA: 0s - loss: 3.4813 - acc: 0.773 - ETA: 0s - loss: 3.5346 - acc: 0.770 - ETA: 0s - loss: 3.5678 - acc: 0.768 - ETA: 0s - loss: 3.5633 - acc: 0.768 - ETA: 0s - loss: 3.5707 - acc: 0.768 - ETA: 0s - loss: 3.5826 - acc: 0.766 - ETA: 0s - loss: 3.5882 - acc: 0.766 - ETA: 0s - loss: 3.5873 - acc: 0.767 - ETA: 0s - loss: 3.6148 - acc: 0.765 - ETA: 0s - loss: 3.6151 - acc: 0.765 - ETA: 0s - loss: 3.6259 - acc: 0.764 - ETA: 0s - loss: 3.6511 - acc: 0.763 - ETA: 0s - loss: 3.6825 - acc: 0.761 - ETA: 0s - loss: 3.6912 - acc: 0.761 - ETA: 0s - loss: 3.6823 - acc: 0.761 - ETA: 0s - loss: 3.6733 - acc: 0.762 - ETA: 0s - loss: 3.6951 - acc: 0.760 - 2s 355us/step - loss: 3.6909 - acc: 0.7614 - val_loss: 5.0422 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00015: val_loss improved from 5.10199 to 5.04224, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 16/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 2.5792 - acc: 0.840 - ETA: 2s - loss: 3.0561 - acc: 0.808 - ETA: 2s - loss: 3.1996 - acc: 0.800 - ETA: 2s - loss: 3.6848 - acc: 0.767 - ETA: 2s - loss: 3.7031 - acc: 0.765 - ETA: 1s - loss: 3.7333 - acc: 0.763 - ETA: 1s - loss: 3.6489 - acc: 0.768 - ETA: 1s - loss: 3.6640 - acc: 0.767 - ETA: 1s - loss: 3.6279 - acc: 0.770 - ETA: 1s - loss: 3.6563 - acc: 0.767 - ETA: 1s - loss: 3.6904 - acc: 0.764 - ETA: 1s - loss: 3.6787 - acc: 0.766 - ETA: 1s - loss: 3.7022 - acc: 0.764 - ETA: 1s - loss: 3.7219 - acc: 0.763 - ETA: 1s - loss: 3.7390 - acc: 0.763 - ETA: 1s - loss: 3.7306 - acc: 0.762 - ETA: 1s - loss: 3.7280 - acc: 0.763 - ETA: 1s - loss: 3.6839 - acc: 0.765 - ETA: 1s - loss: 3.7472 - acc: 0.761 - ETA: 1s - loss: 3.7909 - acc: 0.759 - ETA: 1s - loss: 3.7123 - acc: 0.764 - ETA: 1s - loss: 3.6723 - acc: 0.766 - ETA: 1s - loss: 3.6671 - acc: 0.767 - ETA: 1s - loss: 3.6715 - acc: 0.766 - ETA: 0s - loss: 3.6693 - acc: 0.766 - ETA: 0s - loss: 3.6849 - acc: 0.765 - ETA: 0s - loss: 3.6489 - acc: 0.767 - ETA: 0s - loss: 3.6608 - acc: 0.766 - ETA: 0s - loss: 3.6393 - acc: 0.768 - ETA: 0s - loss: 3.6433 - acc: 0.768 - ETA: 0s - loss: 3.6576 - acc: 0.767 - ETA: 0s - loss: 3.6444 - acc: 0.768 - ETA: 0s - loss: 3.6291 - acc: 0.769 - ETA: 0s - loss: 3.6139 - acc: 0.770 - ETA: 0s - loss: 3.6130 - acc: 0.770 - ETA: 0s - loss: 3.6172 - acc: 0.769 - ETA: 0s - loss: 3.6296 - acc: 0.768 - ETA: 0s - loss: 3.6657 - acc: 0.766 - ETA: 0s - loss: 3.6579 - acc: 0.767 - ETA: 0s - loss: 3.6634 - acc: 0.766 - ETA: 0s - loss: 3.6520 - acc: 0.767 - ETA: 0s - loss: 3.6489 - acc: 0.767 - 2s 360us/step - loss: 3.6591 - acc: 0.7668 - val_loss: 5.0068 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00016: val_loss improved from 5.04224 to 5.00676, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 6.1280 - acc: 0.620 - ETA: 2s - loss: 3.9364 - acc: 0.756 - ETA: 2s - loss: 4.0336 - acc: 0.747 - ETA: 2s - loss: 3.7541 - acc: 0.765 - ETA: 1s - loss: 3.6348 - acc: 0.773 - ETA: 1s - loss: 3.7425 - acc: 0.765 - ETA: 1s - loss: 3.8457 - acc: 0.758 - ETA: 1s - loss: 3.8381 - acc: 0.758 - ETA: 1s - loss: 3.7771 - acc: 0.762 - ETA: 1s - loss: 3.7681 - acc: 0.762 - ETA: 1s - loss: 3.8126 - acc: 0.760 - ETA: 1s - loss: 3.7677 - acc: 0.763 - ETA: 1s - loss: 3.7758 - acc: 0.762 - ETA: 1s - loss: 3.7541 - acc: 0.763 - ETA: 1s - loss: 3.6996 - acc: 0.766 - ETA: 1s - loss: 3.6911 - acc: 0.766 - ETA: 1s - loss: 3.6246 - acc: 0.771 - ETA: 1s - loss: 3.5830 - acc: 0.773 - ETA: 1s - loss: 3.5817 - acc: 0.773 - ETA: 1s - loss: 3.5705 - acc: 0.774 - ETA: 1s - loss: 3.5818 - acc: 0.773 - ETA: 1s - loss: 3.5894 - acc: 0.773 - ETA: 0s - loss: 3.5796 - acc: 0.773 - ETA: 0s - loss: 3.5785 - acc: 0.773 - ETA: 0s - loss: 3.5737 - acc: 0.773 - ETA: 0s - loss: 3.5766 - acc: 0.773 - ETA: 0s - loss: 3.5619 - acc: 0.774 - ETA: 0s - loss: 3.5626 - acc: 0.774 - ETA: 0s - loss: 3.5624 - acc: 0.774 - ETA: 0s - loss: 3.5718 - acc: 0.774 - ETA: 0s - loss: 3.5742 - acc: 0.774 - ETA: 0s - loss: 3.5763 - acc: 0.774 - ETA: 0s - loss: 3.5814 - acc: 0.773 - ETA: 0s - loss: 3.5689 - acc: 0.774 - ETA: 0s - loss: 3.5712 - acc: 0.774 - ETA: 0s - loss: 3.5966 - acc: 0.773 - ETA: 0s - loss: 3.5981 - acc: 0.773 - ETA: 0s - loss: 3.6239 - acc: 0.771 - ETA: 0s - loss: 3.6452 - acc: 0.770 - ETA: 0s - loss: 3.6379 - acc: 0.770 - ETA: 0s - loss: 3.6408 - acc: 0.770 - 2s 360us/step - loss: 3.6480 - acc: 0.7701 - val_loss: 5.0101 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.00676\n",
      "Epoch 18/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.5165 - acc: 0.720 - ETA: 2s - loss: 4.1308 - acc: 0.740 - ETA: 2s - loss: 3.7512 - acc: 0.762 - ETA: 2s - loss: 3.6948 - acc: 0.766 - ETA: 2s - loss: 3.4624 - acc: 0.781 - ETA: 2s - loss: 3.3583 - acc: 0.788 - ETA: 1s - loss: 3.3882 - acc: 0.787 - ETA: 1s - loss: 3.3912 - acc: 0.786 - ETA: 1s - loss: 3.4157 - acc: 0.784 - ETA: 1s - loss: 3.4506 - acc: 0.782 - ETA: 1s - loss: 3.4720 - acc: 0.781 - ETA: 1s - loss: 3.4475 - acc: 0.782 - ETA: 1s - loss: 3.4073 - acc: 0.784 - ETA: 1s - loss: 3.4544 - acc: 0.781 - ETA: 1s - loss: 3.5102 - acc: 0.778 - ETA: 1s - loss: 3.5001 - acc: 0.778 - ETA: 1s - loss: 3.5035 - acc: 0.778 - ETA: 1s - loss: 3.5770 - acc: 0.773 - ETA: 1s - loss: 3.5588 - acc: 0.775 - ETA: 1s - loss: 3.5599 - acc: 0.774 - ETA: 1s - loss: 3.5543 - acc: 0.775 - ETA: 1s - loss: 3.5932 - acc: 0.772 - ETA: 1s - loss: 3.6050 - acc: 0.772 - ETA: 1s - loss: 3.5895 - acc: 0.773 - ETA: 0s - loss: 3.5922 - acc: 0.773 - ETA: 0s - loss: 3.6079 - acc: 0.772 - ETA: 0s - loss: 3.6575 - acc: 0.769 - ETA: 0s - loss: 3.6976 - acc: 0.766 - ETA: 0s - loss: 3.6458 - acc: 0.769 - ETA: 0s - loss: 3.6117 - acc: 0.772 - ETA: 0s - loss: 3.6226 - acc: 0.771 - ETA: 0s - loss: 3.6163 - acc: 0.771 - ETA: 0s - loss: 3.6193 - acc: 0.771 - ETA: 0s - loss: 3.6294 - acc: 0.770 - ETA: 0s - loss: 3.6183 - acc: 0.771 - ETA: 0s - loss: 3.6366 - acc: 0.770 - ETA: 0s - loss: 3.6505 - acc: 0.769 - ETA: 0s - loss: 3.6397 - acc: 0.770 - ETA: 0s - loss: 3.6512 - acc: 0.769 - ETA: 0s - loss: 3.6636 - acc: 0.769 - ETA: 0s - loss: 3.6584 - acc: 0.769 - ETA: 0s - loss: 3.6437 - acc: 0.770 - 2s 363us/step - loss: 3.6460 - acc: 0.7704 - val_loss: 5.0438 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.00676\n",
      "Epoch 19/50\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 1.6119 - acc: 0.900 - ETA: 2s - loss: 3.3056 - acc: 0.795 - ETA: 2s - loss: 3.7790 - acc: 0.765 - ETA: 1s - loss: 3.8119 - acc: 0.763 - ETA: 1s - loss: 3.6752 - acc: 0.772 - ETA: 1s - loss: 3.7859 - acc: 0.764 - ETA: 1s - loss: 3.7826 - acc: 0.764 - ETA: 1s - loss: 3.7026 - acc: 0.769 - ETA: 1s - loss: 3.7095 - acc: 0.769 - ETA: 1s - loss: 3.6729 - acc: 0.771 - ETA: 1s - loss: 3.6902 - acc: 0.770 - ETA: 1s - loss: 3.7069 - acc: 0.768 - ETA: 1s - loss: 3.7180 - acc: 0.767 - ETA: 1s - loss: 3.6918 - acc: 0.769 - ETA: 1s - loss: 3.6962 - acc: 0.769 - ETA: 1s - loss: 3.6550 - acc: 0.772 - ETA: 1s - loss: 3.6550 - acc: 0.772 - ETA: 1s - loss: 3.6667 - acc: 0.770 - ETA: 1s - loss: 3.6716 - acc: 0.770 - ETA: 1s - loss: 3.6240 - acc: 0.773 - ETA: 1s - loss: 3.6650 - acc: 0.771 - ETA: 1s - loss: 3.6930 - acc: 0.769 - ETA: 1s - loss: 3.6551 - acc: 0.771 - ETA: 0s - loss: 3.6512 - acc: 0.771 - ETA: 0s - loss: 3.6386 - acc: 0.772 - ETA: 0s - loss: 3.6763 - acc: 0.770 - ETA: 0s - loss: 3.6826 - acc: 0.769 - ETA: 0s - loss: 3.6667 - acc: 0.769 - ETA: 0s - loss: 3.6562 - acc: 0.769 - ETA: 0s - loss: 3.6391 - acc: 0.770 - ETA: 0s - loss: 3.6294 - acc: 0.771 - ETA: 0s - loss: 3.6320 - acc: 0.771 - ETA: 0s - loss: 3.6233 - acc: 0.771 - ETA: 0s - loss: 3.6292 - acc: 0.771 - ETA: 0s - loss: 3.6440 - acc: 0.770 - ETA: 0s - loss: 3.6494 - acc: 0.769 - ETA: 0s - loss: 3.6469 - acc: 0.770 - ETA: 0s - loss: 3.6268 - acc: 0.771 - ETA: 0s - loss: 3.6200 - acc: 0.771 - ETA: 0s - loss: 3.6109 - acc: 0.772 - 2s 356us/step - loss: 3.6170 - acc: 0.7720 - val_loss: 5.0232 - val_acc: 0.5952\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.00676\n",
      "Epoch 20/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.2255 - acc: 0.800 - ETA: 2s - loss: 3.1581 - acc: 0.795 - ETA: 2s - loss: 3.7977 - acc: 0.757 - ETA: 1s - loss: 3.6879 - acc: 0.766 - ETA: 1s - loss: 3.6624 - acc: 0.768 - ETA: 1s - loss: 3.7240 - acc: 0.765 - ETA: 1s - loss: 3.7839 - acc: 0.761 - ETA: 1s - loss: 3.6416 - acc: 0.771 - ETA: 1s - loss: 3.6237 - acc: 0.772 - ETA: 1s - loss: 3.5980 - acc: 0.774 - ETA: 1s - loss: 3.5846 - acc: 0.775 - ETA: 1s - loss: 3.5733 - acc: 0.776 - ETA: 1s - loss: 3.5765 - acc: 0.775 - ETA: 1s - loss: 3.5743 - acc: 0.776 - ETA: 1s - loss: 3.6093 - acc: 0.773 - ETA: 1s - loss: 3.6346 - acc: 0.772 - ETA: 1s - loss: 3.5831 - acc: 0.775 - ETA: 1s - loss: 3.5864 - acc: 0.775 - ETA: 1s - loss: 3.5702 - acc: 0.776 - ETA: 1s - loss: 3.5467 - acc: 0.777 - ETA: 0s - loss: 3.5075 - acc: 0.780 - ETA: 0s - loss: 3.4848 - acc: 0.781 - ETA: 0s - loss: 3.4673 - acc: 0.783 - ETA: 0s - loss: 3.4982 - acc: 0.780 - ETA: 0s - loss: 3.4860 - acc: 0.781 - ETA: 0s - loss: 3.5125 - acc: 0.780 - ETA: 0s - loss: 3.4932 - acc: 0.781 - ETA: 0s - loss: 3.5149 - acc: 0.780 - ETA: 0s - loss: 3.5216 - acc: 0.779 - ETA: 0s - loss: 3.5139 - acc: 0.780 - ETA: 0s - loss: 3.5325 - acc: 0.779 - ETA: 0s - loss: 3.5502 - acc: 0.778 - ETA: 0s - loss: 3.5774 - acc: 0.776 - ETA: 0s - loss: 3.5982 - acc: 0.774 - ETA: 0s - loss: 3.5867 - acc: 0.775 - ETA: 0s - loss: 3.5832 - acc: 0.775 - ETA: 0s - loss: 3.5849 - acc: 0.775 - ETA: 0s - loss: 3.5723 - acc: 0.776 - 2s 350us/step - loss: 3.5756 - acc: 0.7763 - val_loss: 4.9825 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00020: val_loss improved from 5.00676 to 4.98250, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 3.2257 - acc: 0.800 - ETA: 1s - loss: 2.9942 - acc: 0.808 - ETA: 2s - loss: 3.4549 - acc: 0.780 - ETA: 1s - loss: 3.3786 - acc: 0.786 - ETA: 1s - loss: 3.4406 - acc: 0.783 - ETA: 1s - loss: 3.3725 - acc: 0.788 - ETA: 1s - loss: 3.3525 - acc: 0.790 - ETA: 1s - loss: 3.3203 - acc: 0.792 - ETA: 1s - loss: 3.4439 - acc: 0.784 - ETA: 1s - loss: 3.4131 - acc: 0.786 - ETA: 1s - loss: 3.4707 - acc: 0.783 - ETA: 1s - loss: 3.5199 - acc: 0.780 - ETA: 1s - loss: 3.5679 - acc: 0.777 - ETA: 1s - loss: 3.6361 - acc: 0.773 - ETA: 1s - loss: 3.6032 - acc: 0.775 - ETA: 1s - loss: 3.5879 - acc: 0.776 - ETA: 1s - loss: 3.5587 - acc: 0.777 - ETA: 1s - loss: 3.5310 - acc: 0.779 - ETA: 1s - loss: 3.6074 - acc: 0.775 - ETA: 1s - loss: 3.5848 - acc: 0.776 - ETA: 1s - loss: 3.5851 - acc: 0.776 - ETA: 0s - loss: 3.5848 - acc: 0.776 - ETA: 0s - loss: 3.5852 - acc: 0.776 - ETA: 0s - loss: 3.5884 - acc: 0.775 - ETA: 0s - loss: 3.6132 - acc: 0.774 - ETA: 0s - loss: 3.6212 - acc: 0.773 - ETA: 0s - loss: 3.5958 - acc: 0.775 - ETA: 0s - loss: 3.5855 - acc: 0.776 - ETA: 0s - loss: 3.5425 - acc: 0.778 - ETA: 0s - loss: 3.5489 - acc: 0.778 - ETA: 0s - loss: 3.5458 - acc: 0.778 - ETA: 0s - loss: 3.5195 - acc: 0.780 - ETA: 0s - loss: 3.5233 - acc: 0.780 - ETA: 0s - loss: 3.5323 - acc: 0.779 - ETA: 0s - loss: 3.5384 - acc: 0.779 - ETA: 0s - loss: 3.5598 - acc: 0.777 - ETA: 0s - loss: 3.5645 - acc: 0.777 - ETA: 0s - loss: 3.5468 - acc: 0.778 - 2s 354us/step - loss: 3.5526 - acc: 0.7780 - val_loss: 4.9467 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00021: val_loss improved from 4.98250 to 4.94673, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 22/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9027 - acc: 0.820 - ETA: 2s - loss: 3.1549 - acc: 0.804 - ETA: 2s - loss: 3.2949 - acc: 0.795 - ETA: 1s - loss: 3.5993 - acc: 0.773 - ETA: 1s - loss: 3.3348 - acc: 0.790 - ETA: 1s - loss: 3.3972 - acc: 0.786 - ETA: 1s - loss: 3.4045 - acc: 0.785 - ETA: 1s - loss: 3.4602 - acc: 0.782 - ETA: 1s - loss: 3.3438 - acc: 0.789 - ETA: 1s - loss: 3.4614 - acc: 0.781 - ETA: 1s - loss: 3.5187 - acc: 0.777 - ETA: 1s - loss: 3.5876 - acc: 0.773 - ETA: 1s - loss: 3.5866 - acc: 0.773 - ETA: 1s - loss: 3.5643 - acc: 0.774 - ETA: 1s - loss: 3.5959 - acc: 0.773 - ETA: 1s - loss: 3.5861 - acc: 0.774 - ETA: 1s - loss: 3.5509 - acc: 0.776 - ETA: 1s - loss: 3.5356 - acc: 0.777 - ETA: 1s - loss: 3.5313 - acc: 0.777 - ETA: 1s - loss: 3.5394 - acc: 0.776 - ETA: 1s - loss: 3.5212 - acc: 0.778 - ETA: 0s - loss: 3.5406 - acc: 0.776 - ETA: 0s - loss: 3.5074 - acc: 0.778 - ETA: 0s - loss: 3.5089 - acc: 0.778 - ETA: 0s - loss: 3.5085 - acc: 0.778 - ETA: 0s - loss: 3.4926 - acc: 0.779 - ETA: 0s - loss: 3.4693 - acc: 0.780 - ETA: 0s - loss: 3.4581 - acc: 0.781 - ETA: 0s - loss: 3.4416 - acc: 0.782 - ETA: 0s - loss: 3.4745 - acc: 0.780 - ETA: 0s - loss: 3.4960 - acc: 0.779 - ETA: 0s - loss: 3.4945 - acc: 0.779 - ETA: 0s - loss: 3.5115 - acc: 0.778 - ETA: 0s - loss: 3.4986 - acc: 0.778 - ETA: 0s - loss: 3.4954 - acc: 0.779 - ETA: 0s - loss: 3.5076 - acc: 0.778 - ETA: 0s - loss: 3.4963 - acc: 0.779 - ETA: 0s - loss: 3.5055 - acc: 0.778 - ETA: 0s - loss: 3.4914 - acc: 0.779 - ETA: 0s - loss: 3.5173 - acc: 0.777 - 2s 358us/step - loss: 3.5236 - acc: 0.7774 - val_loss: 4.8435 - val_acc: 0.6168\n",
      "\n",
      "Epoch 00022: val_loss improved from 4.94673 to 4.84350, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 23/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5500 - acc: 0.780 - ETA: 2s - loss: 3.1347 - acc: 0.800 - ETA: 1s - loss: 3.3534 - acc: 0.788 - ETA: 1s - loss: 3.4623 - acc: 0.783 - ETA: 1s - loss: 3.3114 - acc: 0.792 - ETA: 1s - loss: 3.2341 - acc: 0.798 - ETA: 1s - loss: 3.2999 - acc: 0.794 - ETA: 1s - loss: 3.2212 - acc: 0.798 - ETA: 1s - loss: 3.3021 - acc: 0.793 - ETA: 1s - loss: 3.3653 - acc: 0.790 - ETA: 1s - loss: 3.3040 - acc: 0.793 - ETA: 1s - loss: 3.3585 - acc: 0.790 - ETA: 1s - loss: 3.3882 - acc: 0.788 - ETA: 1s - loss: 3.4007 - acc: 0.787 - ETA: 1s - loss: 3.4351 - acc: 0.785 - ETA: 1s - loss: 3.4465 - acc: 0.784 - ETA: 1s - loss: 3.4676 - acc: 0.783 - ETA: 1s - loss: 3.4427 - acc: 0.785 - ETA: 1s - loss: 3.4473 - acc: 0.785 - ETA: 1s - loss: 3.4277 - acc: 0.785 - ETA: 0s - loss: 3.4642 - acc: 0.783 - ETA: 0s - loss: 3.4439 - acc: 0.785 - ETA: 0s - loss: 3.4681 - acc: 0.783 - ETA: 0s - loss: 3.4747 - acc: 0.783 - ETA: 0s - loss: 3.4820 - acc: 0.782 - ETA: 0s - loss: 3.4680 - acc: 0.783 - ETA: 0s - loss: 3.4557 - acc: 0.783 - ETA: 0s - loss: 3.4619 - acc: 0.783 - ETA: 0s - loss: 3.4658 - acc: 0.783 - ETA: 0s - loss: 3.4781 - acc: 0.782 - ETA: 0s - loss: 3.4717 - acc: 0.782 - ETA: 0s - loss: 3.4796 - acc: 0.782 - ETA: 0s - loss: 3.4841 - acc: 0.781 - ETA: 0s - loss: 3.4939 - acc: 0.781 - ETA: 0s - loss: 3.4868 - acc: 0.781 - ETA: 0s - loss: 3.4888 - acc: 0.781 - ETA: 0s - loss: 3.4821 - acc: 0.782 - ETA: 0s - loss: 3.4783 - acc: 0.782 - 2s 349us/step - loss: 3.5039 - acc: 0.7807 - val_loss: 4.9206 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 4.84350\n",
      "Epoch 24/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9022 - acc: 0.820 - ETA: 2s - loss: 3.0328 - acc: 0.810 - ETA: 2s - loss: 2.9550 - acc: 0.811 - ETA: 2s - loss: 3.1733 - acc: 0.792 - ETA: 2s - loss: 3.1868 - acc: 0.792 - ETA: 1s - loss: 3.2104 - acc: 0.787 - ETA: 1s - loss: 3.3053 - acc: 0.778 - ETA: 1s - loss: 3.3364 - acc: 0.779 - ETA: 1s - loss: 3.3272 - acc: 0.780 - ETA: 1s - loss: 3.2928 - acc: 0.781 - ETA: 1s - loss: 3.3227 - acc: 0.780 - ETA: 1s - loss: 3.3022 - acc: 0.781 - ETA: 1s - loss: 3.3643 - acc: 0.778 - ETA: 1s - loss: 3.3845 - acc: 0.776 - ETA: 1s - loss: 3.4106 - acc: 0.775 - ETA: 1s - loss: 3.4129 - acc: 0.775 - ETA: 1s - loss: 3.4538 - acc: 0.773 - ETA: 1s - loss: 3.4415 - acc: 0.774 - ETA: 1s - loss: 3.3902 - acc: 0.777 - ETA: 1s - loss: 3.3884 - acc: 0.778 - ETA: 1s - loss: 3.3909 - acc: 0.778 - ETA: 1s - loss: 3.3769 - acc: 0.779 - ETA: 1s - loss: 3.3886 - acc: 0.779 - ETA: 0s - loss: 3.3890 - acc: 0.779 - ETA: 0s - loss: 3.4082 - acc: 0.778 - ETA: 0s - loss: 3.3919 - acc: 0.779 - ETA: 0s - loss: 3.3943 - acc: 0.779 - ETA: 0s - loss: 3.3726 - acc: 0.781 - ETA: 0s - loss: 3.3646 - acc: 0.781 - ETA: 0s - loss: 3.3310 - acc: 0.783 - ETA: 0s - loss: 3.3161 - acc: 0.784 - ETA: 0s - loss: 3.3284 - acc: 0.784 - ETA: 0s - loss: 3.3490 - acc: 0.783 - ETA: 0s - loss: 3.3743 - acc: 0.782 - ETA: 0s - loss: 3.3760 - acc: 0.782 - ETA: 0s - loss: 3.3777 - acc: 0.782 - ETA: 0s - loss: 3.3856 - acc: 0.781 - ETA: 0s - loss: 3.3863 - acc: 0.781 - ETA: 0s - loss: 3.3987 - acc: 0.781 - ETA: 0s - loss: 3.3938 - acc: 0.781 - ETA: 0s - loss: 3.3921 - acc: 0.781 - 2s 356us/step - loss: 3.3867 - acc: 0.7819 - val_loss: 4.8498 - val_acc: 0.6012\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.84350\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 4.5759 - acc: 0.700 - ETA: 2s - loss: 4.3330 - acc: 0.728 - ETA: 2s - loss: 3.8506 - acc: 0.757 - ETA: 2s - loss: 3.5755 - acc: 0.772 - ETA: 2s - loss: 3.7118 - acc: 0.764 - ETA: 1s - loss: 3.6940 - acc: 0.766 - ETA: 1s - loss: 3.5966 - acc: 0.773 - ETA: 1s - loss: 3.4843 - acc: 0.780 - ETA: 1s - loss: 3.6125 - acc: 0.771 - ETA: 1s - loss: 3.5415 - acc: 0.776 - ETA: 1s - loss: 3.5713 - acc: 0.775 - ETA: 1s - loss: 3.5692 - acc: 0.775 - ETA: 1s - loss: 3.5679 - acc: 0.775 - ETA: 1s - loss: 3.5050 - acc: 0.780 - ETA: 1s - loss: 3.4863 - acc: 0.781 - ETA: 1s - loss: 3.4675 - acc: 0.782 - ETA: 1s - loss: 3.4341 - acc: 0.784 - ETA: 1s - loss: 3.4380 - acc: 0.783 - ETA: 1s - loss: 3.3843 - acc: 0.787 - ETA: 1s - loss: 3.4112 - acc: 0.785 - ETA: 1s - loss: 3.4090 - acc: 0.785 - ETA: 1s - loss: 3.4087 - acc: 0.785 - ETA: 1s - loss: 3.4497 - acc: 0.783 - ETA: 1s - loss: 3.4467 - acc: 0.783 - ETA: 0s - loss: 3.4053 - acc: 0.785 - ETA: 0s - loss: 3.4243 - acc: 0.784 - ETA: 0s - loss: 3.4406 - acc: 0.783 - ETA: 0s - loss: 3.4520 - acc: 0.782 - ETA: 0s - loss: 3.4587 - acc: 0.782 - ETA: 0s - loss: 3.4582 - acc: 0.782 - ETA: 0s - loss: 3.4421 - acc: 0.783 - ETA: 0s - loss: 3.4258 - acc: 0.784 - ETA: 0s - loss: 3.4164 - acc: 0.784 - ETA: 0s - loss: 3.4239 - acc: 0.784 - ETA: 0s - loss: 3.3935 - acc: 0.786 - ETA: 0s - loss: 3.3719 - acc: 0.787 - ETA: 0s - loss: 3.3598 - acc: 0.788 - ETA: 0s - loss: 3.3809 - acc: 0.787 - ETA: 0s - loss: 3.3774 - acc: 0.787 - ETA: 0s - loss: 3.3746 - acc: 0.787 - ETA: 0s - loss: 3.3577 - acc: 0.788 - 2s 366us/step - loss: 3.3532 - acc: 0.7886 - val_loss: 4.8026 - val_acc: 0.6084\n",
      "\n",
      "Epoch 00025: val_loss improved from 4.84350 to 4.80258, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 26/50\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 2.2578 - acc: 0.860 - ETA: 2s - loss: 3.0310 - acc: 0.812 - ETA: 1s - loss: 3.1166 - acc: 0.806 - ETA: 1s - loss: 3.4121 - acc: 0.788 - ETA: 1s - loss: 3.4604 - acc: 0.785 - ETA: 1s - loss: 3.2964 - acc: 0.793 - ETA: 1s - loss: 3.3539 - acc: 0.790 - ETA: 1s - loss: 3.3642 - acc: 0.790 - ETA: 1s - loss: 3.3275 - acc: 0.792 - ETA: 1s - loss: 3.3883 - acc: 0.788 - ETA: 1s - loss: 3.4992 - acc: 0.781 - ETA: 1s - loss: 3.5368 - acc: 0.779 - ETA: 1s - loss: 3.5477 - acc: 0.778 - ETA: 1s - loss: 3.5476 - acc: 0.778 - ETA: 1s - loss: 3.5151 - acc: 0.780 - ETA: 1s - loss: 3.5062 - acc: 0.780 - ETA: 1s - loss: 3.5028 - acc: 0.781 - ETA: 1s - loss: 3.5166 - acc: 0.780 - ETA: 1s - loss: 3.5083 - acc: 0.781 - ETA: 1s - loss: 3.4576 - acc: 0.784 - ETA: 1s - loss: 3.4218 - acc: 0.786 - ETA: 0s - loss: 3.4007 - acc: 0.787 - ETA: 0s - loss: 3.4022 - acc: 0.787 - ETA: 0s - loss: 3.4301 - acc: 0.786 - ETA: 0s - loss: 3.4226 - acc: 0.786 - ETA: 0s - loss: 3.3983 - acc: 0.787 - ETA: 0s - loss: 3.4105 - acc: 0.787 - ETA: 0s - loss: 3.4159 - acc: 0.786 - ETA: 0s - loss: 3.4122 - acc: 0.786 - ETA: 0s - loss: 3.4049 - acc: 0.787 - ETA: 0s - loss: 3.3967 - acc: 0.787 - ETA: 0s - loss: 3.3796 - acc: 0.788 - ETA: 0s - loss: 3.3724 - acc: 0.789 - ETA: 0s - loss: 3.3598 - acc: 0.790 - ETA: 0s - loss: 3.3648 - acc: 0.789 - ETA: 0s - loss: 3.3476 - acc: 0.790 - ETA: 0s - loss: 3.3392 - acc: 0.791 - ETA: 0s - loss: 3.3463 - acc: 0.791 - ETA: 0s - loss: 3.3620 - acc: 0.789 - ETA: 0s - loss: 3.3439 - acc: 0.790 - ETA: 0s - loss: 3.3388 - acc: 0.791 - 2s 362us/step - loss: 3.3432 - acc: 0.7910 - val_loss: 4.7967 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00026: val_loss improved from 4.80258 to 4.79672, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 27/50\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 4.8354 - acc: 0.700 - ETA: 2s - loss: 3.1860 - acc: 0.800 - ETA: 1s - loss: 3.2027 - acc: 0.800 - ETA: 1s - loss: 3.1617 - acc: 0.801 - ETA: 1s - loss: 3.2183 - acc: 0.797 - ETA: 1s - loss: 3.1297 - acc: 0.803 - ETA: 1s - loss: 3.2045 - acc: 0.799 - ETA: 1s - loss: 3.2610 - acc: 0.795 - ETA: 1s - loss: 3.3419 - acc: 0.790 - ETA: 1s - loss: 3.2148 - acc: 0.797 - ETA: 1s - loss: 3.1874 - acc: 0.799 - ETA: 1s - loss: 3.1730 - acc: 0.800 - ETA: 1s - loss: 3.1771 - acc: 0.800 - ETA: 1s - loss: 3.2327 - acc: 0.797 - ETA: 1s - loss: 3.2801 - acc: 0.794 - ETA: 1s - loss: 3.2319 - acc: 0.797 - ETA: 1s - loss: 3.2742 - acc: 0.795 - ETA: 1s - loss: 3.3442 - acc: 0.790 - ETA: 1s - loss: 3.3152 - acc: 0.792 - ETA: 1s - loss: 3.3018 - acc: 0.793 - ETA: 1s - loss: 3.3542 - acc: 0.790 - ETA: 1s - loss: 3.3533 - acc: 0.790 - ETA: 1s - loss: 3.3742 - acc: 0.788 - ETA: 0s - loss: 3.3779 - acc: 0.788 - ETA: 0s - loss: 3.3883 - acc: 0.787 - ETA: 0s - loss: 3.3871 - acc: 0.787 - ETA: 0s - loss: 3.4040 - acc: 0.786 - ETA: 0s - loss: 3.3960 - acc: 0.787 - ETA: 0s - loss: 3.3800 - acc: 0.788 - ETA: 0s - loss: 3.3902 - acc: 0.787 - ETA: 0s - loss: 3.3421 - acc: 0.790 - ETA: 0s - loss: 3.3525 - acc: 0.790 - ETA: 0s - loss: 3.3399 - acc: 0.791 - ETA: 0s - loss: 3.3367 - acc: 0.791 - ETA: 0s - loss: 3.3223 - acc: 0.792 - ETA: 0s - loss: 3.3226 - acc: 0.792 - ETA: 0s - loss: 3.3220 - acc: 0.792 - ETA: 0s - loss: 3.3110 - acc: 0.793 - ETA: 0s - loss: 3.3299 - acc: 0.791 - ETA: 0s - loss: 3.3255 - acc: 0.792 - 2s 361us/step - loss: 3.3396 - acc: 0.7912 - val_loss: 4.8460 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 4.79672\n",
      "Epoch 28/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.2571 - acc: 0.860 - ETA: 2s - loss: 3.1438 - acc: 0.805 - ETA: 2s - loss: 3.0399 - acc: 0.811 - ETA: 2s - loss: 3.1596 - acc: 0.804 - ETA: 2s - loss: 3.2931 - acc: 0.795 - ETA: 1s - loss: 3.3031 - acc: 0.792 - ETA: 1s - loss: 3.4363 - acc: 0.785 - ETA: 1s - loss: 3.3528 - acc: 0.790 - ETA: 1s - loss: 3.4247 - acc: 0.786 - ETA: 1s - loss: 3.3706 - acc: 0.789 - ETA: 1s - loss: 3.4172 - acc: 0.786 - ETA: 1s - loss: 3.3730 - acc: 0.789 - ETA: 1s - loss: 3.3412 - acc: 0.791 - ETA: 1s - loss: 3.3098 - acc: 0.793 - ETA: 1s - loss: 3.3183 - acc: 0.793 - ETA: 1s - loss: 3.3567 - acc: 0.791 - ETA: 1s - loss: 3.3304 - acc: 0.792 - ETA: 1s - loss: 3.3656 - acc: 0.790 - ETA: 1s - loss: 3.3694 - acc: 0.790 - ETA: 1s - loss: 3.4151 - acc: 0.787 - ETA: 1s - loss: 3.4182 - acc: 0.787 - ETA: 1s - loss: 3.4285 - acc: 0.786 - ETA: 1s - loss: 3.3926 - acc: 0.789 - ETA: 0s - loss: 3.3836 - acc: 0.789 - ETA: 0s - loss: 3.3775 - acc: 0.790 - ETA: 0s - loss: 3.3599 - acc: 0.791 - ETA: 0s - loss: 3.3437 - acc: 0.792 - ETA: 0s - loss: 3.3397 - acc: 0.792 - ETA: 0s - loss: 3.3689 - acc: 0.790 - ETA: 0s - loss: 3.3475 - acc: 0.791 - ETA: 0s - loss: 3.3605 - acc: 0.790 - ETA: 0s - loss: 3.3400 - acc: 0.792 - ETA: 0s - loss: 3.3398 - acc: 0.792 - ETA: 0s - loss: 3.3263 - acc: 0.793 - ETA: 0s - loss: 3.3234 - acc: 0.793 - ETA: 0s - loss: 3.3438 - acc: 0.792 - ETA: 0s - loss: 3.3631 - acc: 0.790 - ETA: 0s - loss: 3.3650 - acc: 0.790 - ETA: 0s - loss: 3.3780 - acc: 0.789 - ETA: 0s - loss: 3.3613 - acc: 0.790 - ETA: 0s - loss: 3.3557 - acc: 0.791 - ETA: 0s - loss: 3.3495 - acc: 0.791 - ETA: 0s - loss: 3.3323 - acc: 0.792 - 2s 373us/step - loss: 3.3366 - acc: 0.7922 - val_loss: 4.7790 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00028: val_loss improved from 4.79672 to 4.77903, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 3.4654 - acc: 0.785 - ETA: 2s - loss: 3.3618 - acc: 0.791 - ETA: 2s - loss: 3.2559 - acc: 0.798 - ETA: 2s - loss: 3.0997 - acc: 0.807 - ETA: 2s - loss: 3.2438 - acc: 0.798 - ETA: 1s - loss: 3.3255 - acc: 0.793 - ETA: 1s - loss: 3.2237 - acc: 0.800 - ETA: 1s - loss: 3.3229 - acc: 0.793 - ETA: 1s - loss: 3.3016 - acc: 0.795 - ETA: 1s - loss: 3.3345 - acc: 0.793 - ETA: 1s - loss: 3.3250 - acc: 0.793 - ETA: 1s - loss: 3.3001 - acc: 0.795 - ETA: 1s - loss: 3.3082 - acc: 0.794 - ETA: 1s - loss: 3.3312 - acc: 0.793 - ETA: 1s - loss: 3.3379 - acc: 0.792 - ETA: 1s - loss: 3.3755 - acc: 0.790 - ETA: 1s - loss: 3.3611 - acc: 0.791 - ETA: 1s - loss: 3.2916 - acc: 0.795 - ETA: 1s - loss: 3.3635 - acc: 0.791 - ETA: 1s - loss: 3.3722 - acc: 0.790 - ETA: 1s - loss: 3.3466 - acc: 0.792 - ETA: 1s - loss: 3.3414 - acc: 0.792 - ETA: 1s - loss: 3.3276 - acc: 0.793 - ETA: 1s - loss: 3.3449 - acc: 0.792 - ETA: 0s - loss: 3.3279 - acc: 0.793 - ETA: 0s - loss: 3.3161 - acc: 0.794 - ETA: 0s - loss: 3.2974 - acc: 0.795 - ETA: 0s - loss: 3.3023 - acc: 0.794 - ETA: 0s - loss: 3.3221 - acc: 0.793 - ETA: 0s - loss: 3.3189 - acc: 0.793 - ETA: 0s - loss: 3.2991 - acc: 0.795 - ETA: 0s - loss: 3.3229 - acc: 0.793 - ETA: 0s - loss: 3.3042 - acc: 0.794 - ETA: 0s - loss: 3.2896 - acc: 0.795 - ETA: 0s - loss: 3.2818 - acc: 0.796 - ETA: 0s - loss: 3.2947 - acc: 0.795 - ETA: 0s - loss: 3.2873 - acc: 0.795 - ETA: 0s - loss: 3.2995 - acc: 0.795 - ETA: 0s - loss: 3.2976 - acc: 0.795 - ETA: 0s - loss: 3.2958 - acc: 0.795 - ETA: 0s - loss: 3.3084 - acc: 0.794 - ETA: 0s - loss: 3.3139 - acc: 0.794 - ETA: 0s - loss: 3.3387 - acc: 0.792 - 2s 374us/step - loss: 3.3349 - acc: 0.7928 - val_loss: 4.7904 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4.77903\n",
      "Epoch 30/50\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 1.6118 - acc: 0.900 - ETA: 2s - loss: 3.3848 - acc: 0.790 - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 2.7724 - acc: 0.828 - ETA: 2s - loss: 2.7401 - acc: 0.830 - ETA: 2s - loss: 2.7517 - acc: 0.828 - ETA: 2s - loss: 2.9192 - acc: 0.818 - ETA: 1s - loss: 2.9449 - acc: 0.816 - ETA: 1s - loss: 2.8779 - acc: 0.820 - ETA: 1s - loss: 3.0137 - acc: 0.812 - ETA: 1s - loss: 3.0177 - acc: 0.811 - ETA: 1s - loss: 3.0353 - acc: 0.810 - ETA: 1s - loss: 3.0756 - acc: 0.808 - ETA: 1s - loss: 3.1887 - acc: 0.801 - ETA: 1s - loss: 3.1764 - acc: 0.802 - ETA: 1s - loss: 3.1589 - acc: 0.803 - ETA: 1s - loss: 3.2595 - acc: 0.797 - ETA: 1s - loss: 3.2389 - acc: 0.798 - ETA: 1s - loss: 3.2777 - acc: 0.796 - ETA: 1s - loss: 3.2804 - acc: 0.796 - ETA: 1s - loss: 3.2618 - acc: 0.797 - ETA: 1s - loss: 3.2697 - acc: 0.796 - ETA: 1s - loss: 3.2862 - acc: 0.795 - ETA: 1s - loss: 3.2900 - acc: 0.795 - ETA: 1s - loss: 3.2959 - acc: 0.794 - ETA: 0s - loss: 3.2809 - acc: 0.795 - ETA: 0s - loss: 3.3417 - acc: 0.792 - ETA: 0s - loss: 3.3679 - acc: 0.790 - ETA: 0s - loss: 3.3593 - acc: 0.790 - ETA: 0s - loss: 3.3816 - acc: 0.789 - ETA: 0s - loss: 3.3697 - acc: 0.790 - ETA: 0s - loss: 3.3619 - acc: 0.790 - ETA: 0s - loss: 3.3679 - acc: 0.790 - ETA: 0s - loss: 3.3606 - acc: 0.790 - ETA: 0s - loss: 3.3385 - acc: 0.792 - ETA: 0s - loss: 3.3471 - acc: 0.791 - ETA: 0s - loss: 3.3409 - acc: 0.792 - ETA: 0s - loss: 3.3519 - acc: 0.791 - ETA: 0s - loss: 3.3596 - acc: 0.790 - ETA: 0s - loss: 3.3536 - acc: 0.791 - ETA: 0s - loss: 3.3505 - acc: 0.791 - ETA: 0s - loss: 3.3297 - acc: 0.792 - ETA: 0s - loss: 3.3372 - acc: 0.792 - ETA: 0s - loss: 3.3346 - acc: 0.792 - 3s 382us/step - loss: 3.3365 - acc: 0.7924 - val_loss: 4.7429 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00030: val_loss improved from 4.77903 to 4.74291, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 31/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.5790 - acc: 0.840 - ETA: 2s - loss: 2.4984 - acc: 0.845 - ETA: 2s - loss: 3.1431 - acc: 0.805 - ETA: 2s - loss: 3.3409 - acc: 0.792 - ETA: 2s - loss: 3.2500 - acc: 0.797 - ETA: 2s - loss: 3.1695 - acc: 0.802 - ETA: 1s - loss: 3.3549 - acc: 0.791 - ETA: 1s - loss: 3.3939 - acc: 0.788 - ETA: 1s - loss: 3.3328 - acc: 0.792 - ETA: 1s - loss: 3.3004 - acc: 0.794 - ETA: 1s - loss: 3.3130 - acc: 0.793 - ETA: 1s - loss: 3.3055 - acc: 0.794 - ETA: 1s - loss: 3.3406 - acc: 0.792 - ETA: 1s - loss: 3.3859 - acc: 0.789 - ETA: 1s - loss: 3.3966 - acc: 0.788 - ETA: 1s - loss: 3.3825 - acc: 0.789 - ETA: 1s - loss: 3.3919 - acc: 0.789 - ETA: 1s - loss: 3.3593 - acc: 0.791 - ETA: 1s - loss: 3.3523 - acc: 0.791 - ETA: 1s - loss: 3.3700 - acc: 0.790 - ETA: 1s - loss: 3.3583 - acc: 0.791 - ETA: 1s - loss: 3.3621 - acc: 0.790 - ETA: 1s - loss: 3.3336 - acc: 0.792 - ETA: 1s - loss: 3.3248 - acc: 0.793 - ETA: 0s - loss: 3.3376 - acc: 0.792 - ETA: 0s - loss: 3.3336 - acc: 0.792 - ETA: 0s - loss: 3.3335 - acc: 0.792 - ETA: 0s - loss: 3.3409 - acc: 0.792 - ETA: 0s - loss: 3.3008 - acc: 0.794 - ETA: 0s - loss: 3.3088 - acc: 0.794 - ETA: 0s - loss: 3.3298 - acc: 0.793 - ETA: 0s - loss: 3.3349 - acc: 0.792 - ETA: 0s - loss: 3.3576 - acc: 0.791 - ETA: 0s - loss: 3.3661 - acc: 0.790 - ETA: 0s - loss: 3.3802 - acc: 0.789 - ETA: 0s - loss: 3.3826 - acc: 0.789 - ETA: 0s - loss: 3.3870 - acc: 0.789 - ETA: 0s - loss: 3.3800 - acc: 0.789 - ETA: 0s - loss: 3.3869 - acc: 0.789 - ETA: 0s - loss: 3.3644 - acc: 0.790 - ETA: 0s - loss: 3.3610 - acc: 0.790 - ETA: 0s - loss: 3.3427 - acc: 0.792 - ETA: 0s - loss: 3.3399 - acc: 0.792 - 3s 380us/step - loss: 3.3353 - acc: 0.7925 - val_loss: 4.7436 - val_acc: 0.6216\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 4.74291\n",
      "Epoch 32/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 3.8684 - acc: 0.760 - ETA: 2s - loss: 3.8684 - acc: 0.760 - ETA: 2s - loss: 3.6749 - acc: 0.772 - ETA: 2s - loss: 3.8436 - acc: 0.761 - ETA: 2s - loss: 3.7273 - acc: 0.768 - ETA: 1s - loss: 3.6672 - acc: 0.771 - ETA: 1s - loss: 3.5504 - acc: 0.778 - ETA: 1s - loss: 3.5628 - acc: 0.777 - ETA: 1s - loss: 3.5149 - acc: 0.780 - ETA: 1s - loss: 3.5491 - acc: 0.778 - ETA: 1s - loss: 3.5204 - acc: 0.780 - ETA: 1s - loss: 3.4702 - acc: 0.783 - ETA: 1s - loss: 3.4885 - acc: 0.782 - ETA: 1s - loss: 3.5150 - acc: 0.780 - ETA: 1s - loss: 3.4329 - acc: 0.786 - ETA: 1s - loss: 3.4004 - acc: 0.788 - ETA: 1s - loss: 3.4460 - acc: 0.785 - ETA: 1s - loss: 3.3987 - acc: 0.788 - ETA: 1s - loss: 3.3841 - acc: 0.789 - ETA: 1s - loss: 3.3997 - acc: 0.788 - ETA: 1s - loss: 3.3834 - acc: 0.789 - ETA: 1s - loss: 3.3717 - acc: 0.790 - ETA: 1s - loss: 3.3563 - acc: 0.791 - ETA: 1s - loss: 3.3553 - acc: 0.791 - ETA: 0s - loss: 3.3711 - acc: 0.790 - ETA: 0s - loss: 3.3696 - acc: 0.790 - ETA: 0s - loss: 3.3566 - acc: 0.791 - ETA: 0s - loss: 3.3356 - acc: 0.792 - ETA: 0s - loss: 3.3307 - acc: 0.792 - ETA: 0s - loss: 3.3307 - acc: 0.792 - ETA: 0s - loss: 3.3208 - acc: 0.793 - ETA: 0s - loss: 3.3243 - acc: 0.793 - ETA: 0s - loss: 3.3151 - acc: 0.793 - ETA: 0s - loss: 3.3247 - acc: 0.793 - ETA: 0s - loss: 3.3249 - acc: 0.793 - ETA: 0s - loss: 3.3078 - acc: 0.794 - ETA: 0s - loss: 3.3063 - acc: 0.794 - ETA: 0s - loss: 3.3206 - acc: 0.793 - ETA: 0s - loss: 3.3174 - acc: 0.793 - ETA: 0s - loss: 3.3178 - acc: 0.793 - ETA: 0s - loss: 3.3307 - acc: 0.792 - ETA: 0s - loss: 3.3479 - acc: 0.791 - 2s 371us/step - loss: 3.3358 - acc: 0.7925 - val_loss: 4.7511 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 4.74291\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 4.1908 - acc: 0.740 - ETA: 2s - loss: 4.0427 - acc: 0.745 - ETA: 2s - loss: 3.7838 - acc: 0.762 - ETA: 2s - loss: 3.6480 - acc: 0.772 - ETA: 2s - loss: 3.5253 - acc: 0.780 - ETA: 2s - loss: 3.5896 - acc: 0.776 - ETA: 2s - loss: 3.4979 - acc: 0.782 - ETA: 2s - loss: 3.4898 - acc: 0.782 - ETA: 1s - loss: 3.4870 - acc: 0.782 - ETA: 1s - loss: 3.3552 - acc: 0.790 - ETA: 1s - loss: 3.2905 - acc: 0.794 - ETA: 1s - loss: 3.3604 - acc: 0.790 - ETA: 1s - loss: 3.3667 - acc: 0.790 - ETA: 1s - loss: 3.3882 - acc: 0.789 - ETA: 1s - loss: 3.3543 - acc: 0.791 - ETA: 1s - loss: 3.3668 - acc: 0.790 - ETA: 1s - loss: 3.3843 - acc: 0.789 - ETA: 1s - loss: 3.3998 - acc: 0.788 - ETA: 1s - loss: 3.4195 - acc: 0.787 - ETA: 1s - loss: 3.4261 - acc: 0.786 - ETA: 1s - loss: 3.4320 - acc: 0.786 - ETA: 1s - loss: 3.4323 - acc: 0.786 - ETA: 1s - loss: 3.4518 - acc: 0.785 - ETA: 1s - loss: 3.4743 - acc: 0.784 - ETA: 1s - loss: 3.4596 - acc: 0.784 - ETA: 1s - loss: 3.4248 - acc: 0.787 - ETA: 0s - loss: 3.3968 - acc: 0.788 - ETA: 0s - loss: 3.3904 - acc: 0.789 - ETA: 0s - loss: 3.3883 - acc: 0.789 - ETA: 0s - loss: 3.3901 - acc: 0.789 - ETA: 0s - loss: 3.3653 - acc: 0.790 - ETA: 0s - loss: 3.3676 - acc: 0.790 - ETA: 0s - loss: 3.3830 - acc: 0.789 - ETA: 0s - loss: 3.3782 - acc: 0.790 - ETA: 0s - loss: 3.3769 - acc: 0.790 - ETA: 0s - loss: 3.3696 - acc: 0.790 - ETA: 0s - loss: 3.3861 - acc: 0.789 - ETA: 0s - loss: 3.3732 - acc: 0.790 - ETA: 0s - loss: 3.3650 - acc: 0.790 - ETA: 0s - loss: 3.3587 - acc: 0.791 - ETA: 0s - loss: 3.3701 - acc: 0.790 - ETA: 0s - loss: 3.3528 - acc: 0.791 - ETA: 0s - loss: 3.3498 - acc: 0.791 - ETA: 0s - loss: 3.3428 - acc: 0.792 - 3s 377us/step - loss: 3.3351 - acc: 0.7927 - val_loss: 4.7458 - val_acc: 0.6251\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 4.74291\n",
      "Epoch 34/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 3.6946 - acc: 0.766 - ETA: 2s - loss: 3.0293 - acc: 0.810 - ETA: 2s - loss: 2.9508 - acc: 0.815 - ETA: 2s - loss: 2.9116 - acc: 0.818 - ETA: 2s - loss: 3.0599 - acc: 0.809 - ETA: 2s - loss: 2.9977 - acc: 0.813 - ETA: 2s - loss: 3.1374 - acc: 0.804 - ETA: 1s - loss: 3.2959 - acc: 0.795 - ETA: 1s - loss: 3.3357 - acc: 0.792 - ETA: 1s - loss: 3.2922 - acc: 0.795 - ETA: 1s - loss: 3.2762 - acc: 0.796 - ETA: 1s - loss: 3.2360 - acc: 0.798 - ETA: 1s - loss: 3.2103 - acc: 0.800 - ETA: 1s - loss: 3.1960 - acc: 0.801 - ETA: 1s - loss: 3.1334 - acc: 0.805 - ETA: 1s - loss: 3.1591 - acc: 0.803 - ETA: 1s - loss: 3.2013 - acc: 0.801 - ETA: 1s - loss: 3.2201 - acc: 0.800 - ETA: 1s - loss: 3.2592 - acc: 0.797 - ETA: 1s - loss: 3.2944 - acc: 0.795 - ETA: 1s - loss: 3.3012 - acc: 0.795 - ETA: 1s - loss: 3.3314 - acc: 0.793 - ETA: 1s - loss: 3.3406 - acc: 0.792 - ETA: 1s - loss: 3.3446 - acc: 0.792 - ETA: 1s - loss: 3.3780 - acc: 0.790 - ETA: 0s - loss: 3.3885 - acc: 0.789 - ETA: 0s - loss: 3.3707 - acc: 0.790 - ETA: 0s - loss: 3.3275 - acc: 0.793 - ETA: 0s - loss: 3.3423 - acc: 0.792 - ETA: 0s - loss: 3.3175 - acc: 0.793 - ETA: 0s - loss: 3.3068 - acc: 0.794 - ETA: 0s - loss: 3.3207 - acc: 0.793 - ETA: 0s - loss: 3.3056 - acc: 0.794 - ETA: 0s - loss: 3.3095 - acc: 0.794 - ETA: 0s - loss: 3.3191 - acc: 0.793 - ETA: 0s - loss: 3.3048 - acc: 0.794 - ETA: 0s - loss: 3.3027 - acc: 0.794 - ETA: 0s - loss: 3.3055 - acc: 0.794 - ETA: 0s - loss: 3.2980 - acc: 0.795 - ETA: 0s - loss: 3.2991 - acc: 0.794 - ETA: 0s - loss: 3.3025 - acc: 0.794 - ETA: 0s - loss: 3.3094 - acc: 0.794 - ETA: 0s - loss: 3.3172 - acc: 0.793 - 2s 373us/step - loss: 3.3354 - acc: 0.7925 - val_loss: 4.7380 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00034: val_loss improved from 4.74291 to 4.73799, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 35/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.5790 - acc: 0.840 - ETA: 2s - loss: 3.4654 - acc: 0.785 - ETA: 2s - loss: 3.4079 - acc: 0.788 - ETA: 2s - loss: 3.2559 - acc: 0.798 - ETA: 2s - loss: 3.2254 - acc: 0.798 - ETA: 2s - loss: 3.2654 - acc: 0.796 - ETA: 2s - loss: 3.2079 - acc: 0.800 - ETA: 1s - loss: 3.2687 - acc: 0.796 - ETA: 1s - loss: 3.2504 - acc: 0.797 - ETA: 1s - loss: 3.2936 - acc: 0.795 - ETA: 1s - loss: 3.2036 - acc: 0.800 - ETA: 1s - loss: 3.2528 - acc: 0.797 - ETA: 1s - loss: 3.2679 - acc: 0.796 - ETA: 1s - loss: 3.3290 - acc: 0.793 - ETA: 1s - loss: 3.3142 - acc: 0.794 - ETA: 1s - loss: 3.3223 - acc: 0.793 - ETA: 1s - loss: 3.3096 - acc: 0.794 - ETA: 1s - loss: 3.2621 - acc: 0.797 - ETA: 1s - loss: 3.1967 - acc: 0.801 - ETA: 1s - loss: 3.1981 - acc: 0.801 - ETA: 1s - loss: 3.1734 - acc: 0.802 - ETA: 1s - loss: 3.1459 - acc: 0.804 - ETA: 1s - loss: 3.1731 - acc: 0.802 - ETA: 1s - loss: 3.1752 - acc: 0.802 - ETA: 1s - loss: 3.1734 - acc: 0.802 - ETA: 0s - loss: 3.1921 - acc: 0.801 - ETA: 0s - loss: 3.2312 - acc: 0.798 - ETA: 0s - loss: 3.2693 - acc: 0.796 - ETA: 0s - loss: 3.2750 - acc: 0.796 - ETA: 0s - loss: 3.2746 - acc: 0.796 - ETA: 0s - loss: 3.2868 - acc: 0.795 - ETA: 0s - loss: 3.3050 - acc: 0.794 - ETA: 0s - loss: 3.2797 - acc: 0.795 - ETA: 0s - loss: 3.2717 - acc: 0.796 - ETA: 0s - loss: 3.2649 - acc: 0.796 - ETA: 0s - loss: 3.2787 - acc: 0.795 - ETA: 0s - loss: 3.2743 - acc: 0.796 - ETA: 0s - loss: 3.2956 - acc: 0.794 - ETA: 0s - loss: 3.2965 - acc: 0.794 - ETA: 0s - loss: 3.3108 - acc: 0.793 - ETA: 0s - loss: 3.3322 - acc: 0.792 - ETA: 0s - loss: 3.3567 - acc: 0.791 - ETA: 0s - loss: 3.3462 - acc: 0.791 - 2s 372us/step - loss: 3.3357 - acc: 0.7924 - val_loss: 4.7271 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00035: val_loss improved from 4.73799 to 4.72712, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 36/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.2237 - acc: 0.800 - ETA: 2s - loss: 3.9490 - acc: 0.755 - ETA: 2s - loss: 3.5921 - acc: 0.777 - ETA: 2s - loss: 3.3848 - acc: 0.790 - ETA: 2s - loss: 3.2980 - acc: 0.795 - ETA: 2s - loss: 3.2639 - acc: 0.797 - ETA: 1s - loss: 3.2927 - acc: 0.795 - ETA: 1s - loss: 3.3257 - acc: 0.793 - ETA: 1s - loss: 3.4136 - acc: 0.787 - ETA: 1s - loss: 3.5144 - acc: 0.780 - ETA: 1s - loss: 3.5270 - acc: 0.780 - ETA: 1s - loss: 3.4839 - acc: 0.782 - ETA: 1s - loss: 3.3611 - acc: 0.790 - ETA: 1s - loss: 3.3365 - acc: 0.792 - ETA: 1s - loss: 3.3082 - acc: 0.793 - ETA: 1s - loss: 3.3777 - acc: 0.789 - ETA: 1s - loss: 3.3601 - acc: 0.790 - ETA: 1s - loss: 3.3854 - acc: 0.788 - ETA: 1s - loss: 3.3881 - acc: 0.788 - ETA: 1s - loss: 3.3700 - acc: 0.789 - ETA: 1s - loss: 3.3340 - acc: 0.792 - ETA: 1s - loss: 3.3386 - acc: 0.791 - ETA: 1s - loss: 3.2970 - acc: 0.794 - ETA: 0s - loss: 3.2764 - acc: 0.795 - ETA: 0s - loss: 3.2623 - acc: 0.796 - ETA: 0s - loss: 3.2104 - acc: 0.800 - ETA: 0s - loss: 3.2072 - acc: 0.800 - ETA: 0s - loss: 3.1896 - acc: 0.801 - ETA: 0s - loss: 3.1907 - acc: 0.801 - ETA: 0s - loss: 3.2117 - acc: 0.800 - ETA: 0s - loss: 3.2220 - acc: 0.799 - ETA: 0s - loss: 3.2540 - acc: 0.797 - ETA: 0s - loss: 3.2684 - acc: 0.796 - ETA: 0s - loss: 3.2642 - acc: 0.796 - ETA: 0s - loss: 3.2753 - acc: 0.795 - ETA: 0s - loss: 3.2629 - acc: 0.796 - ETA: 0s - loss: 3.2922 - acc: 0.794 - ETA: 0s - loss: 3.2984 - acc: 0.794 - ETA: 0s - loss: 3.3149 - acc: 0.793 - ETA: 0s - loss: 3.3146 - acc: 0.793 - ETA: 0s - loss: 3.3422 - acc: 0.791 - 2s 360us/step - loss: 3.3311 - acc: 0.7924 - val_loss: 4.7207 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00036: val_loss improved from 4.72712 to 4.72071, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 2.5789 - acc: 0.840 - ETA: 2s - loss: 3.0625 - acc: 0.810 - ETA: 2s - loss: 3.0395 - acc: 0.811 - ETA: 2s - loss: 3.0710 - acc: 0.808 - ETA: 2s - loss: 3.1532 - acc: 0.803 - ETA: 2s - loss: 3.0750 - acc: 0.808 - ETA: 1s - loss: 3.0556 - acc: 0.807 - ETA: 1s - loss: 3.0847 - acc: 0.805 - ETA: 1s - loss: 3.0592 - acc: 0.804 - ETA: 1s - loss: 3.0681 - acc: 0.803 - ETA: 1s - loss: 3.1591 - acc: 0.797 - ETA: 1s - loss: 3.1625 - acc: 0.796 - ETA: 1s - loss: 3.1373 - acc: 0.798 - ETA: 1s - loss: 3.2072 - acc: 0.794 - ETA: 1s - loss: 3.2364 - acc: 0.792 - ETA: 1s - loss: 3.2816 - acc: 0.790 - ETA: 1s - loss: 3.2971 - acc: 0.789 - ETA: 1s - loss: 3.3094 - acc: 0.789 - ETA: 1s - loss: 3.2718 - acc: 0.791 - ETA: 1s - loss: 3.2716 - acc: 0.791 - ETA: 1s - loss: 3.3141 - acc: 0.789 - ETA: 1s - loss: 3.3086 - acc: 0.789 - ETA: 1s - loss: 3.3309 - acc: 0.788 - ETA: 1s - loss: 3.2737 - acc: 0.791 - ETA: 0s - loss: 3.2910 - acc: 0.790 - ETA: 0s - loss: 3.3054 - acc: 0.789 - ETA: 0s - loss: 3.2761 - acc: 0.791 - ETA: 0s - loss: 3.2699 - acc: 0.791 - ETA: 0s - loss: 3.2593 - acc: 0.791 - ETA: 0s - loss: 3.2512 - acc: 0.792 - ETA: 0s - loss: 3.2681 - acc: 0.791 - ETA: 0s - loss: 3.2378 - acc: 0.793 - ETA: 0s - loss: 3.2612 - acc: 0.791 - ETA: 0s - loss: 3.2644 - acc: 0.791 - ETA: 0s - loss: 3.2734 - acc: 0.791 - ETA: 0s - loss: 3.2463 - acc: 0.792 - ETA: 0s - loss: 3.2547 - acc: 0.792 - ETA: 0s - loss: 3.2687 - acc: 0.791 - ETA: 0s - loss: 3.2598 - acc: 0.791 - ETA: 0s - loss: 3.2499 - acc: 0.792 - ETA: 0s - loss: 3.2363 - acc: 0.792 - ETA: 0s - loss: 3.2533 - acc: 0.791 - ETA: 0s - loss: 3.2483 - acc: 0.791 - 2s 373us/step - loss: 3.2498 - acc: 0.7913 - val_loss: 4.8045 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 4.72071\n",
      "Epoch 38/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.0895 - acc: 0.800 - ETA: 2s - loss: 2.9132 - acc: 0.810 - ETA: 2s - loss: 3.2438 - acc: 0.792 - ETA: 2s - loss: 3.2227 - acc: 0.792 - ETA: 2s - loss: 3.0930 - acc: 0.800 - ETA: 1s - loss: 3.0051 - acc: 0.804 - ETA: 1s - loss: 2.9944 - acc: 0.803 - ETA: 1s - loss: 3.1693 - acc: 0.794 - ETA: 1s - loss: 3.0560 - acc: 0.802 - ETA: 1s - loss: 3.1231 - acc: 0.797 - ETA: 1s - loss: 3.1797 - acc: 0.793 - ETA: 1s - loss: 3.1157 - acc: 0.797 - ETA: 1s - loss: 3.0713 - acc: 0.800 - ETA: 1s - loss: 3.0697 - acc: 0.800 - ETA: 1s - loss: 3.0784 - acc: 0.800 - ETA: 1s - loss: 3.1204 - acc: 0.797 - ETA: 1s - loss: 3.1089 - acc: 0.798 - ETA: 1s - loss: 3.0810 - acc: 0.800 - ETA: 1s - loss: 3.0744 - acc: 0.799 - ETA: 1s - loss: 3.0732 - acc: 0.800 - ETA: 1s - loss: 3.0901 - acc: 0.799 - ETA: 1s - loss: 3.0581 - acc: 0.801 - ETA: 1s - loss: 3.0313 - acc: 0.803 - ETA: 1s - loss: 3.0439 - acc: 0.803 - ETA: 0s - loss: 3.0508 - acc: 0.802 - ETA: 0s - loss: 3.0703 - acc: 0.801 - ETA: 0s - loss: 3.0958 - acc: 0.799 - ETA: 0s - loss: 3.1312 - acc: 0.797 - ETA: 0s - loss: 3.1236 - acc: 0.798 - ETA: 0s - loss: 3.1385 - acc: 0.797 - ETA: 0s - loss: 3.1337 - acc: 0.797 - ETA: 0s - loss: 3.1397 - acc: 0.797 - ETA: 0s - loss: 3.1150 - acc: 0.798 - ETA: 0s - loss: 3.1133 - acc: 0.798 - ETA: 0s - loss: 3.0986 - acc: 0.799 - ETA: 0s - loss: 3.1067 - acc: 0.799 - ETA: 0s - loss: 3.1153 - acc: 0.798 - ETA: 0s - loss: 3.1174 - acc: 0.798 - ETA: 0s - loss: 3.1097 - acc: 0.799 - ETA: 0s - loss: 3.1101 - acc: 0.799 - ETA: 0s - loss: 3.1187 - acc: 0.798 - ETA: 0s - loss: 3.1173 - acc: 0.798 - ETA: 0s - loss: 3.1331 - acc: 0.797 - 2s 368us/step - loss: 3.1263 - acc: 0.7982 - val_loss: 4.6803 - val_acc: 0.6216\n",
      "\n",
      "Epoch 00038: val_loss improved from 4.72071 to 4.68034, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 39/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 3.2331 - acc: 0.800 - ETA: 2s - loss: 2.8272 - acc: 0.820 - ETA: 2s - loss: 3.1854 - acc: 0.797 - ETA: 2s - loss: 3.3031 - acc: 0.790 - ETA: 2s - loss: 3.2815 - acc: 0.792 - ETA: 1s - loss: 3.3261 - acc: 0.790 - ETA: 1s - loss: 3.2077 - acc: 0.797 - ETA: 1s - loss: 3.1073 - acc: 0.804 - ETA: 1s - loss: 3.1435 - acc: 0.801 - ETA: 1s - loss: 3.1981 - acc: 0.798 - ETA: 1s - loss: 3.1531 - acc: 0.801 - ETA: 1s - loss: 3.1691 - acc: 0.800 - ETA: 1s - loss: 3.1052 - acc: 0.804 - ETA: 1s - loss: 3.2288 - acc: 0.796 - ETA: 1s - loss: 3.1758 - acc: 0.800 - ETA: 1s - loss: 3.1544 - acc: 0.801 - ETA: 1s - loss: 3.1589 - acc: 0.800 - ETA: 1s - loss: 3.1906 - acc: 0.798 - ETA: 1s - loss: 3.1621 - acc: 0.800 - ETA: 1s - loss: 3.1872 - acc: 0.798 - ETA: 1s - loss: 3.1799 - acc: 0.798 - ETA: 1s - loss: 3.1484 - acc: 0.800 - ETA: 0s - loss: 3.1227 - acc: 0.802 - ETA: 0s - loss: 3.1473 - acc: 0.800 - ETA: 0s - loss: 3.1820 - acc: 0.798 - ETA: 0s - loss: 3.1840 - acc: 0.798 - ETA: 0s - loss: 3.1445 - acc: 0.800 - ETA: 0s - loss: 3.1226 - acc: 0.802 - ETA: 0s - loss: 3.1170 - acc: 0.802 - ETA: 0s - loss: 3.1116 - acc: 0.803 - ETA: 0s - loss: 3.0761 - acc: 0.805 - ETA: 0s - loss: 3.0682 - acc: 0.806 - ETA: 0s - loss: 3.0578 - acc: 0.806 - ETA: 0s - loss: 3.0621 - acc: 0.806 - ETA: 0s - loss: 3.0525 - acc: 0.807 - ETA: 0s - loss: 3.0645 - acc: 0.806 - ETA: 0s - loss: 3.0627 - acc: 0.806 - ETA: 0s - loss: 3.0716 - acc: 0.805 - ETA: 0s - loss: 3.0737 - acc: 0.805 - ETA: 0s - loss: 3.0801 - acc: 0.805 - 2s 357us/step - loss: 3.0807 - acc: 0.8049 - val_loss: 4.6666 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00039: val_loss improved from 4.68034 to 4.66663, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 40/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.8355 - acc: 0.700 - ETA: 2s - loss: 2.9152 - acc: 0.815 - ETA: 2s - loss: 2.7714 - acc: 0.825 - ETA: 2s - loss: 2.6499 - acc: 0.834 - ETA: 2s - loss: 2.6586 - acc: 0.833 - ETA: 2s - loss: 2.7243 - acc: 0.830 - ETA: 1s - loss: 2.6630 - acc: 0.834 - ETA: 1s - loss: 2.7223 - acc: 0.830 - ETA: 1s - loss: 2.7249 - acc: 0.830 - ETA: 1s - loss: 2.7211 - acc: 0.830 - ETA: 1s - loss: 2.7802 - acc: 0.827 - ETA: 1s - loss: 2.8354 - acc: 0.823 - ETA: 1s - loss: 2.8647 - acc: 0.821 - ETA: 1s - loss: 2.8600 - acc: 0.822 - ETA: 1s - loss: 2.9095 - acc: 0.819 - ETA: 1s - loss: 2.9189 - acc: 0.818 - ETA: 1s - loss: 2.9742 - acc: 0.814 - ETA: 1s - loss: 2.9321 - acc: 0.817 - ETA: 1s - loss: 2.9704 - acc: 0.813 - ETA: 1s - loss: 2.9713 - acc: 0.813 - ETA: 1s - loss: 3.0131 - acc: 0.811 - ETA: 0s - loss: 3.0072 - acc: 0.811 - ETA: 0s - loss: 2.9988 - acc: 0.812 - ETA: 0s - loss: 2.9953 - acc: 0.812 - ETA: 0s - loss: 3.0445 - acc: 0.809 - ETA: 0s - loss: 3.0253 - acc: 0.810 - ETA: 0s - loss: 3.0394 - acc: 0.809 - ETA: 0s - loss: 3.0535 - acc: 0.808 - ETA: 0s - loss: 3.0590 - acc: 0.808 - ETA: 0s - loss: 3.0480 - acc: 0.808 - ETA: 0s - loss: 3.0564 - acc: 0.808 - ETA: 0s - loss: 3.0387 - acc: 0.809 - ETA: 0s - loss: 3.0524 - acc: 0.808 - ETA: 0s - loss: 3.0657 - acc: 0.807 - ETA: 0s - loss: 3.0549 - acc: 0.808 - ETA: 0s - loss: 3.0625 - acc: 0.807 - ETA: 0s - loss: 3.0457 - acc: 0.808 - ETA: 0s - loss: 3.0573 - acc: 0.807 - ETA: 0s - loss: 3.0766 - acc: 0.806 - 2s 355us/step - loss: 3.0699 - acc: 0.8070 - val_loss: 4.5652 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00040: val_loss improved from 4.66663 to 4.56521, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5460 - acc: 0.780 - ETA: 2s - loss: 2.7724 - acc: 0.828 - ETA: 2s - loss: 2.9015 - acc: 0.820 - ETA: 1s - loss: 2.9591 - acc: 0.815 - ETA: 1s - loss: 3.0587 - acc: 0.808 - ETA: 1s - loss: 2.9408 - acc: 0.815 - ETA: 1s - loss: 3.0323 - acc: 0.810 - ETA: 1s - loss: 2.9636 - acc: 0.814 - ETA: 1s - loss: 2.8905 - acc: 0.819 - ETA: 1s - loss: 2.9217 - acc: 0.817 - ETA: 1s - loss: 2.9476 - acc: 0.816 - ETA: 1s - loss: 2.9441 - acc: 0.815 - ETA: 1s - loss: 2.9411 - acc: 0.816 - ETA: 1s - loss: 2.9762 - acc: 0.813 - ETA: 1s - loss: 3.0089 - acc: 0.811 - ETA: 1s - loss: 2.9949 - acc: 0.812 - ETA: 1s - loss: 3.0013 - acc: 0.812 - ETA: 1s - loss: 3.0434 - acc: 0.809 - ETA: 1s - loss: 3.0418 - acc: 0.809 - ETA: 1s - loss: 3.0307 - acc: 0.810 - ETA: 1s - loss: 3.0237 - acc: 0.810 - ETA: 0s - loss: 3.0000 - acc: 0.812 - ETA: 0s - loss: 3.0543 - acc: 0.808 - ETA: 0s - loss: 3.0852 - acc: 0.806 - ETA: 0s - loss: 3.0710 - acc: 0.807 - ETA: 0s - loss: 3.0878 - acc: 0.806 - ETA: 0s - loss: 3.1007 - acc: 0.805 - ETA: 0s - loss: 3.1014 - acc: 0.805 - ETA: 0s - loss: 3.1065 - acc: 0.805 - ETA: 0s - loss: 3.1447 - acc: 0.802 - ETA: 0s - loss: 3.1293 - acc: 0.803 - ETA: 0s - loss: 3.1409 - acc: 0.803 - ETA: 0s - loss: 3.1156 - acc: 0.804 - ETA: 0s - loss: 3.0932 - acc: 0.805 - ETA: 0s - loss: 3.0911 - acc: 0.805 - ETA: 0s - loss: 3.0733 - acc: 0.807 - ETA: 0s - loss: 3.0658 - acc: 0.807 - ETA: 0s - loss: 3.0619 - acc: 0.807 - ETA: 0s - loss: 3.0664 - acc: 0.807 - 2s 353us/step - loss: 3.0695 - acc: 0.8072 - val_loss: 4.5659 - val_acc: 0.6287\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 4.56521\n",
      "Epoch 42/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.2565 - acc: 0.860 - ETA: 2s - loss: 3.3526 - acc: 0.792 - ETA: 2s - loss: 3.2646 - acc: 0.797 - ETA: 2s - loss: 3.1948 - acc: 0.801 - ETA: 2s - loss: 3.1319 - acc: 0.805 - ETA: 2s - loss: 3.4136 - acc: 0.788 - ETA: 1s - loss: 3.3944 - acc: 0.788 - ETA: 1s - loss: 3.3186 - acc: 0.792 - ETA: 1s - loss: 3.2455 - acc: 0.796 - ETA: 1s - loss: 3.1701 - acc: 0.801 - ETA: 1s - loss: 3.2433 - acc: 0.796 - ETA: 1s - loss: 3.1490 - acc: 0.801 - ETA: 1s - loss: 3.1309 - acc: 0.802 - ETA: 1s - loss: 3.1229 - acc: 0.803 - ETA: 1s - loss: 3.0950 - acc: 0.805 - ETA: 1s - loss: 3.1387 - acc: 0.802 - ETA: 1s - loss: 3.1209 - acc: 0.803 - ETA: 1s - loss: 3.1437 - acc: 0.802 - ETA: 1s - loss: 3.1204 - acc: 0.803 - ETA: 1s - loss: 3.0838 - acc: 0.806 - ETA: 1s - loss: 3.0606 - acc: 0.807 - ETA: 1s - loss: 3.0441 - acc: 0.808 - ETA: 1s - loss: 3.0245 - acc: 0.810 - ETA: 1s - loss: 3.0282 - acc: 0.810 - ETA: 0s - loss: 3.0275 - acc: 0.810 - ETA: 0s - loss: 3.0195 - acc: 0.810 - ETA: 0s - loss: 2.9842 - acc: 0.812 - ETA: 0s - loss: 2.9925 - acc: 0.812 - ETA: 0s - loss: 3.0004 - acc: 0.811 - ETA: 0s - loss: 2.9937 - acc: 0.812 - ETA: 0s - loss: 2.9843 - acc: 0.812 - ETA: 0s - loss: 2.9835 - acc: 0.812 - ETA: 0s - loss: 3.0002 - acc: 0.811 - ETA: 0s - loss: 3.0159 - acc: 0.811 - ETA: 0s - loss: 3.0235 - acc: 0.810 - ETA: 0s - loss: 3.0279 - acc: 0.810 - ETA: 0s - loss: 3.0376 - acc: 0.809 - ETA: 0s - loss: 3.0589 - acc: 0.808 - ETA: 0s - loss: 3.0389 - acc: 0.809 - ETA: 0s - loss: 3.0250 - acc: 0.810 - ETA: 0s - loss: 3.0374 - acc: 0.809 - ETA: 0s - loss: 3.0581 - acc: 0.808 - ETA: 0s - loss: 3.0618 - acc: 0.808 - 2s 370us/step - loss: 3.0649 - acc: 0.8081 - val_loss: 4.5869 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 4.56521\n",
      "Epoch 43/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 3.8683 - acc: 0.760 - ETA: 2s - loss: 3.1430 - acc: 0.805 - ETA: 2s - loss: 2.6998 - acc: 0.832 - ETA: 2s - loss: 2.8741 - acc: 0.820 - ETA: 2s - loss: 2.8339 - acc: 0.822 - ETA: 2s - loss: 2.9976 - acc: 0.812 - ETA: 2s - loss: 3.0326 - acc: 0.810 - ETA: 1s - loss: 3.0163 - acc: 0.810 - ETA: 1s - loss: 3.1148 - acc: 0.804 - ETA: 1s - loss: 3.2367 - acc: 0.797 - ETA: 1s - loss: 3.1688 - acc: 0.801 - ETA: 1s - loss: 3.1994 - acc: 0.800 - ETA: 1s - loss: 3.1368 - acc: 0.804 - ETA: 1s - loss: 3.1447 - acc: 0.803 - ETA: 1s - loss: 3.1398 - acc: 0.803 - ETA: 1s - loss: 3.1004 - acc: 0.806 - ETA: 1s - loss: 3.1444 - acc: 0.803 - ETA: 1s - loss: 3.1372 - acc: 0.803 - ETA: 1s - loss: 3.1373 - acc: 0.803 - ETA: 1s - loss: 3.1414 - acc: 0.803 - ETA: 1s - loss: 3.1058 - acc: 0.805 - ETA: 1s - loss: 3.0876 - acc: 0.807 - ETA: 1s - loss: 3.1246 - acc: 0.804 - ETA: 1s - loss: 3.1332 - acc: 0.804 - ETA: 0s - loss: 3.1077 - acc: 0.805 - ETA: 0s - loss: 3.0939 - acc: 0.806 - ETA: 0s - loss: 3.1129 - acc: 0.805 - ETA: 0s - loss: 3.1160 - acc: 0.804 - ETA: 0s - loss: 3.1240 - acc: 0.804 - ETA: 0s - loss: 3.1172 - acc: 0.804 - ETA: 0s - loss: 3.1205 - acc: 0.804 - ETA: 0s - loss: 3.1047 - acc: 0.805 - ETA: 0s - loss: 3.1020 - acc: 0.806 - ETA: 0s - loss: 3.0935 - acc: 0.806 - ETA: 0s - loss: 3.0781 - acc: 0.807 - ETA: 0s - loss: 3.0708 - acc: 0.808 - ETA: 0s - loss: 3.0421 - acc: 0.809 - ETA: 0s - loss: 3.0598 - acc: 0.808 - ETA: 0s - loss: 3.0573 - acc: 0.809 - ETA: 0s - loss: 3.0537 - acc: 0.809 - ETA: 0s - loss: 3.0502 - acc: 0.809 - 2s 362us/step - loss: 3.0620 - acc: 0.8088 - val_loss: 4.5704 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 4.56521\n",
      "Epoch 44/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.2569 - acc: 0.860 - ETA: 2s - loss: 3.3043 - acc: 0.795 - ETA: 2s - loss: 3.3043 - acc: 0.795 - ETA: 2s - loss: 3.1651 - acc: 0.803 - ETA: 2s - loss: 3.0858 - acc: 0.808 - ETA: 2s - loss: 3.0343 - acc: 0.811 - ETA: 1s - loss: 3.0466 - acc: 0.811 - ETA: 1s - loss: 3.1569 - acc: 0.804 - ETA: 1s - loss: 3.1565 - acc: 0.803 - ETA: 1s - loss: 3.1343 - acc: 0.804 - ETA: 1s - loss: 3.0838 - acc: 0.807 - ETA: 1s - loss: 3.1313 - acc: 0.805 - ETA: 1s - loss: 3.0805 - acc: 0.808 - ETA: 1s - loss: 3.0275 - acc: 0.811 - ETA: 1s - loss: 3.0191 - acc: 0.811 - ETA: 1s - loss: 2.9917 - acc: 0.813 - ETA: 1s - loss: 3.0306 - acc: 0.811 - ETA: 1s - loss: 3.0295 - acc: 0.811 - ETA: 1s - loss: 3.0284 - acc: 0.811 - ETA: 1s - loss: 2.9945 - acc: 0.813 - ETA: 1s - loss: 3.0116 - acc: 0.811 - ETA: 1s - loss: 3.0406 - acc: 0.809 - ETA: 1s - loss: 3.0438 - acc: 0.809 - ETA: 1s - loss: 3.0484 - acc: 0.809 - ETA: 0s - loss: 3.0468 - acc: 0.809 - ETA: 0s - loss: 3.0315 - acc: 0.810 - ETA: 0s - loss: 3.0501 - acc: 0.809 - ETA: 0s - loss: 3.0674 - acc: 0.808 - ETA: 0s - loss: 3.0908 - acc: 0.806 - ETA: 0s - loss: 3.0881 - acc: 0.807 - ETA: 0s - loss: 3.0993 - acc: 0.806 - ETA: 0s - loss: 3.0718 - acc: 0.808 - ETA: 0s - loss: 3.1047 - acc: 0.806 - ETA: 0s - loss: 3.0788 - acc: 0.807 - ETA: 0s - loss: 3.0887 - acc: 0.807 - ETA: 0s - loss: 3.0981 - acc: 0.806 - ETA: 0s - loss: 3.1024 - acc: 0.806 - ETA: 0s - loss: 3.0753 - acc: 0.807 - ETA: 0s - loss: 3.0816 - acc: 0.807 - ETA: 0s - loss: 3.0721 - acc: 0.808 - ETA: 0s - loss: 3.0608 - acc: 0.808 - ETA: 0s - loss: 3.0596 - acc: 0.808 - 2s 364us/step - loss: 3.0628 - acc: 0.8085 - val_loss: 4.5601 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00044: val_loss improved from 4.56521 to 4.56010, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 2.5789 - acc: 0.840 - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 3.0087 - acc: 0.813 - ETA: 2s - loss: 2.9282 - acc: 0.818 - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 1s - loss: 3.0464 - acc: 0.811 - ETA: 1s - loss: 3.0274 - acc: 0.812 - ETA: 1s - loss: 3.0377 - acc: 0.811 - ETA: 1s - loss: 3.0347 - acc: 0.811 - ETA: 1s - loss: 3.0381 - acc: 0.811 - ETA: 1s - loss: 3.0433 - acc: 0.810 - ETA: 1s - loss: 3.0732 - acc: 0.809 - ETA: 1s - loss: 3.0613 - acc: 0.809 - ETA: 1s - loss: 3.0578 - acc: 0.810 - ETA: 1s - loss: 3.0746 - acc: 0.809 - ETA: 1s - loss: 3.0770 - acc: 0.808 - ETA: 1s - loss: 3.0499 - acc: 0.810 - ETA: 1s - loss: 3.0701 - acc: 0.809 - ETA: 1s - loss: 3.0883 - acc: 0.808 - ETA: 1s - loss: 3.0795 - acc: 0.808 - ETA: 1s - loss: 3.0934 - acc: 0.807 - ETA: 1s - loss: 3.0672 - acc: 0.809 - ETA: 1s - loss: 3.0517 - acc: 0.810 - ETA: 0s - loss: 3.0208 - acc: 0.812 - ETA: 0s - loss: 3.0445 - acc: 0.810 - ETA: 0s - loss: 3.0318 - acc: 0.811 - ETA: 0s - loss: 3.0113 - acc: 0.812 - ETA: 0s - loss: 3.0064 - acc: 0.813 - ETA: 0s - loss: 2.9986 - acc: 0.813 - ETA: 0s - loss: 3.0134 - acc: 0.812 - ETA: 0s - loss: 3.0356 - acc: 0.811 - ETA: 0s - loss: 3.0392 - acc: 0.810 - ETA: 0s - loss: 3.0504 - acc: 0.810 - ETA: 0s - loss: 3.0346 - acc: 0.811 - ETA: 0s - loss: 3.0647 - acc: 0.809 - ETA: 0s - loss: 3.0826 - acc: 0.808 - ETA: 0s - loss: 3.0941 - acc: 0.807 - ETA: 0s - loss: 3.0838 - acc: 0.807 - ETA: 0s - loss: 3.0856 - acc: 0.807 - ETA: 0s - loss: 3.0740 - acc: 0.808 - 2s 367us/step - loss: 3.0602 - acc: 0.8093 - val_loss: 4.5863 - val_acc: 0.6383\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 4.56010\n",
      "Epoch 46/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 2.9013 - acc: 0.820 - ETA: 2s - loss: 3.1437 - acc: 0.805 - ETA: 2s - loss: 2.9017 - acc: 0.820 - ETA: 2s - loss: 3.1067 - acc: 0.807 - ETA: 2s - loss: 2.8785 - acc: 0.821 - ETA: 2s - loss: 2.8836 - acc: 0.820 - ETA: 1s - loss: 2.9200 - acc: 0.817 - ETA: 1s - loss: 2.9315 - acc: 0.816 - ETA: 1s - loss: 2.8041 - acc: 0.824 - ETA: 1s - loss: 2.8922 - acc: 0.819 - ETA: 1s - loss: 2.8853 - acc: 0.819 - ETA: 1s - loss: 2.9852 - acc: 0.813 - ETA: 1s - loss: 2.9870 - acc: 0.813 - ETA: 1s - loss: 2.9811 - acc: 0.813 - ETA: 1s - loss: 2.9973 - acc: 0.812 - ETA: 1s - loss: 3.0181 - acc: 0.811 - ETA: 1s - loss: 3.0239 - acc: 0.811 - ETA: 1s - loss: 3.0470 - acc: 0.810 - ETA: 1s - loss: 3.0393 - acc: 0.810 - ETA: 1s - loss: 3.0055 - acc: 0.812 - ETA: 1s - loss: 3.0411 - acc: 0.810 - ETA: 1s - loss: 3.0235 - acc: 0.811 - ETA: 1s - loss: 3.0501 - acc: 0.809 - ETA: 1s - loss: 3.0311 - acc: 0.811 - ETA: 0s - loss: 3.0345 - acc: 0.810 - ETA: 0s - loss: 3.0255 - acc: 0.811 - ETA: 0s - loss: 3.0288 - acc: 0.811 - ETA: 0s - loss: 3.0169 - acc: 0.812 - ETA: 0s - loss: 3.0529 - acc: 0.809 - ETA: 0s - loss: 3.0602 - acc: 0.809 - ETA: 0s - loss: 3.0653 - acc: 0.809 - ETA: 0s - loss: 3.0669 - acc: 0.809 - ETA: 0s - loss: 3.0737 - acc: 0.808 - ETA: 0s - loss: 3.0492 - acc: 0.809 - ETA: 0s - loss: 3.0774 - acc: 0.808 - ETA: 0s - loss: 3.0927 - acc: 0.807 - ETA: 0s - loss: 3.0789 - acc: 0.808 - ETA: 0s - loss: 3.0664 - acc: 0.808 - ETA: 0s - loss: 3.0807 - acc: 0.808 - ETA: 0s - loss: 3.0560 - acc: 0.809 - ETA: 0s - loss: 3.0525 - acc: 0.809 - ETA: 0s - loss: 3.0686 - acc: 0.808 - 2s 364us/step - loss: 3.0584 - acc: 0.8094 - val_loss: 4.5307 - val_acc: 0.6335\n",
      "\n",
      "Epoch 00046: val_loss improved from 4.56010 to 4.53066, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 47/50\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 3.2236 - acc: 0.800 - ETA: 2s - loss: 2.9819 - acc: 0.815 - ETA: 2s - loss: 2.9473 - acc: 0.817 - ETA: 2s - loss: 3.1269 - acc: 0.806 - ETA: 2s - loss: 3.0748 - acc: 0.809 - ETA: 2s - loss: 3.0530 - acc: 0.810 - ETA: 1s - loss: 3.2083 - acc: 0.801 - ETA: 1s - loss: 3.1833 - acc: 0.802 - ETA: 1s - loss: 3.1880 - acc: 0.802 - ETA: 1s - loss: 3.2560 - acc: 0.798 - ETA: 1s - loss: 3.2824 - acc: 0.796 - ETA: 1s - loss: 3.2714 - acc: 0.796 - ETA: 1s - loss: 3.2760 - acc: 0.796 - ETA: 1s - loss: 3.2530 - acc: 0.797 - ETA: 1s - loss: 3.2504 - acc: 0.797 - ETA: 1s - loss: 3.2030 - acc: 0.800 - ETA: 1s - loss: 3.2352 - acc: 0.798 - ETA: 1s - loss: 3.2346 - acc: 0.798 - ETA: 1s - loss: 3.2340 - acc: 0.798 - ETA: 1s - loss: 3.2231 - acc: 0.799 - ETA: 1s - loss: 3.2131 - acc: 0.800 - ETA: 1s - loss: 3.1913 - acc: 0.801 - ETA: 1s - loss: 3.1789 - acc: 0.802 - ETA: 1s - loss: 3.1421 - acc: 0.804 - ETA: 0s - loss: 3.1463 - acc: 0.804 - ETA: 0s - loss: 3.1253 - acc: 0.805 - ETA: 0s - loss: 3.0909 - acc: 0.807 - ETA: 0s - loss: 3.0750 - acc: 0.808 - ETA: 0s - loss: 3.0870 - acc: 0.807 - ETA: 0s - loss: 3.0983 - acc: 0.806 - ETA: 0s - loss: 3.1037 - acc: 0.806 - ETA: 0s - loss: 3.1246 - acc: 0.805 - ETA: 0s - loss: 3.1150 - acc: 0.805 - ETA: 0s - loss: 3.1151 - acc: 0.805 - ETA: 0s - loss: 3.0741 - acc: 0.808 - ETA: 0s - loss: 3.0753 - acc: 0.808 - ETA: 0s - loss: 3.0749 - acc: 0.808 - ETA: 0s - loss: 3.0824 - acc: 0.807 - ETA: 0s - loss: 3.0781 - acc: 0.808 - ETA: 0s - loss: 3.0851 - acc: 0.807 - ETA: 0s - loss: 3.0686 - acc: 0.808 - 2s 361us/step - loss: 3.0596 - acc: 0.8093 - val_loss: 4.5349 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 4.53066\n",
      "Epoch 48/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.2236 - acc: 0.800 - ETA: 2s - loss: 2.5789 - acc: 0.840 - ETA: 2s - loss: 3.1833 - acc: 0.802 - ETA: 2s - loss: 3.1650 - acc: 0.803 - ETA: 2s - loss: 3.2018 - acc: 0.800 - ETA: 1s - loss: 3.3577 - acc: 0.790 - ETA: 1s - loss: 3.3537 - acc: 0.791 - ETA: 1s - loss: 3.2783 - acc: 0.795 - ETA: 1s - loss: 3.2244 - acc: 0.799 - ETA: 1s - loss: 3.1437 - acc: 0.804 - ETA: 1s - loss: 3.1634 - acc: 0.802 - ETA: 1s - loss: 3.1684 - acc: 0.802 - ETA: 1s - loss: 3.1813 - acc: 0.801 - ETA: 1s - loss: 3.1340 - acc: 0.804 - ETA: 1s - loss: 3.1676 - acc: 0.802 - ETA: 1s - loss: 3.1597 - acc: 0.803 - ETA: 1s - loss: 3.1517 - acc: 0.803 - ETA: 1s - loss: 3.1281 - acc: 0.805 - ETA: 1s - loss: 3.1068 - acc: 0.806 - ETA: 1s - loss: 3.0824 - acc: 0.808 - ETA: 1s - loss: 3.0887 - acc: 0.807 - ETA: 1s - loss: 3.1129 - acc: 0.806 - ETA: 1s - loss: 3.1130 - acc: 0.806 - ETA: 0s - loss: 3.0757 - acc: 0.808 - ETA: 0s - loss: 3.1054 - acc: 0.806 - ETA: 0s - loss: 3.0957 - acc: 0.807 - ETA: 0s - loss: 3.1199 - acc: 0.805 - ETA: 0s - loss: 3.0985 - acc: 0.807 - ETA: 0s - loss: 3.1093 - acc: 0.806 - ETA: 0s - loss: 3.1029 - acc: 0.806 - ETA: 0s - loss: 3.1065 - acc: 0.806 - ETA: 0s - loss: 3.0755 - acc: 0.808 - ETA: 0s - loss: 3.0554 - acc: 0.809 - ETA: 0s - loss: 3.0644 - acc: 0.809 - ETA: 0s - loss: 3.0587 - acc: 0.809 - ETA: 0s - loss: 3.0464 - acc: 0.810 - ETA: 0s - loss: 3.0401 - acc: 0.810 - ETA: 0s - loss: 3.0350 - acc: 0.811 - ETA: 0s - loss: 3.0475 - acc: 0.810 - ETA: 0s - loss: 3.0639 - acc: 0.809 - ETA: 0s - loss: 3.0530 - acc: 0.809 - 2s 362us/step - loss: 3.0587 - acc: 0.8094 - val_loss: 4.5135 - val_acc: 0.6479\n",
      "\n",
      "Epoch 00048: val_loss improved from 4.53066 to 4.51348, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 2.2565 - acc: 0.860 - ETA: 2s - loss: 3.4885 - acc: 0.780 - ETA: 2s - loss: 3.1908 - acc: 0.800 - ETA: 2s - loss: 3.0721 - acc: 0.808 - ETA: 2s - loss: 3.1566 - acc: 0.803 - ETA: 2s - loss: 3.0685 - acc: 0.808 - ETA: 1s - loss: 3.2117 - acc: 0.800 - ETA: 1s - loss: 3.3159 - acc: 0.793 - ETA: 1s - loss: 3.3693 - acc: 0.790 - ETA: 1s - loss: 3.3897 - acc: 0.788 - ETA: 1s - loss: 3.3186 - acc: 0.793 - ETA: 1s - loss: 3.2921 - acc: 0.794 - ETA: 1s - loss: 3.2272 - acc: 0.799 - ETA: 1s - loss: 3.2044 - acc: 0.800 - ETA: 1s - loss: 3.1986 - acc: 0.800 - ETA: 1s - loss: 3.2067 - acc: 0.800 - ETA: 1s - loss: 3.1229 - acc: 0.805 - ETA: 1s - loss: 3.0565 - acc: 0.809 - ETA: 1s - loss: 3.0357 - acc: 0.811 - ETA: 1s - loss: 3.0126 - acc: 0.812 - ETA: 1s - loss: 3.0435 - acc: 0.810 - ETA: 1s - loss: 3.0241 - acc: 0.811 - ETA: 0s - loss: 2.9753 - acc: 0.815 - ETA: 0s - loss: 2.9756 - acc: 0.815 - ETA: 0s - loss: 2.9605 - acc: 0.816 - ETA: 0s - loss: 2.9665 - acc: 0.815 - ETA: 0s - loss: 2.9751 - acc: 0.814 - ETA: 0s - loss: 2.9866 - acc: 0.814 - ETA: 0s - loss: 3.0213 - acc: 0.811 - ETA: 0s - loss: 3.0047 - acc: 0.812 - ETA: 0s - loss: 3.0188 - acc: 0.812 - ETA: 0s - loss: 2.9847 - acc: 0.814 - ETA: 0s - loss: 3.0137 - acc: 0.812 - ETA: 0s - loss: 3.0270 - acc: 0.811 - ETA: 0s - loss: 3.0267 - acc: 0.811 - ETA: 0s - loss: 3.0290 - acc: 0.811 - ETA: 0s - loss: 3.0288 - acc: 0.811 - ETA: 0s - loss: 3.0231 - acc: 0.811 - ETA: 0s - loss: 3.0370 - acc: 0.810 - ETA: 0s - loss: 3.0388 - acc: 0.810 - 2s 356us/step - loss: 3.0593 - acc: 0.8093 - val_loss: 4.5325 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 4.51348\n",
      "Epoch 50/50\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5463 - acc: 0.780 - ETA: 2s - loss: 3.5463 - acc: 0.780 - ETA: 2s - loss: 3.3619 - acc: 0.791 - ETA: 2s - loss: 2.8369 - acc: 0.824 - ETA: 2s - loss: 2.8766 - acc: 0.821 - ETA: 1s - loss: 2.9772 - acc: 0.815 - ETA: 1s - loss: 2.9174 - acc: 0.819 - ETA: 1s - loss: 2.8873 - acc: 0.820 - ETA: 1s - loss: 2.8918 - acc: 0.820 - ETA: 1s - loss: 3.0005 - acc: 0.813 - ETA: 1s - loss: 2.9622 - acc: 0.815 - ETA: 1s - loss: 3.0289 - acc: 0.811 - ETA: 1s - loss: 3.0162 - acc: 0.812 - ETA: 1s - loss: 3.0456 - acc: 0.810 - ETA: 1s - loss: 3.0152 - acc: 0.812 - ETA: 1s - loss: 3.0346 - acc: 0.811 - ETA: 1s - loss: 2.9897 - acc: 0.814 - ETA: 1s - loss: 2.9966 - acc: 0.813 - ETA: 1s - loss: 2.9861 - acc: 0.814 - ETA: 1s - loss: 2.9925 - acc: 0.814 - ETA: 1s - loss: 3.0402 - acc: 0.810 - ETA: 1s - loss: 3.0532 - acc: 0.810 - ETA: 1s - loss: 3.0355 - acc: 0.811 - ETA: 1s - loss: 3.0214 - acc: 0.812 - ETA: 0s - loss: 3.0251 - acc: 0.811 - ETA: 0s - loss: 3.0003 - acc: 0.813 - ETA: 0s - loss: 3.0045 - acc: 0.813 - ETA: 0s - loss: 3.0346 - acc: 0.811 - ETA: 0s - loss: 3.0246 - acc: 0.811 - ETA: 0s - loss: 3.0193 - acc: 0.812 - ETA: 0s - loss: 3.0257 - acc: 0.811 - ETA: 0s - loss: 3.0089 - acc: 0.812 - ETA: 0s - loss: 3.0256 - acc: 0.811 - ETA: 0s - loss: 3.0466 - acc: 0.810 - ETA: 0s - loss: 3.0605 - acc: 0.809 - ETA: 0s - loss: 3.0591 - acc: 0.809 - ETA: 0s - loss: 3.0691 - acc: 0.809 - ETA: 0s - loss: 3.0596 - acc: 0.809 - ETA: 0s - loss: 3.0449 - acc: 0.810 - ETA: 0s - loss: 3.0436 - acc: 0.810 - ETA: 0s - loss: 3.0504 - acc: 0.810 - ETA: 0s - loss: 3.0469 - acc: 0.810 - ETA: 0s - loss: 3.0533 - acc: 0.809 - 2s 365us/step - loss: 3.0589 - acc: 0.8096 - val_loss: 4.5440 - val_acc: 0.6407\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 4.51348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5a578b70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: 训练模型\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG19_model.fit(train_VGG19, train_targets, \n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          epochs=50, batch_size=50, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载具有最好验证loss的模型\n",
    "\n",
    "VGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 【练习】测试模型\n",
    "\n",
    "<a id='question8'></a>  \n",
    "\n",
    "### __问题 8:__ \n",
    "\n",
    "在狗图像的测试数据集上试用你的模型。确保测试准确率大于60%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 63.9952%\n"
     ]
    }
   ],
   "source": [
    "### TODO: 在测试集上计算分类准确率\n",
    "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 【练习】使用模型测试狗的品种\n",
    "\n",
    "\n",
    "实现一个函数，它的输入为图像路径，功能为预测对应图像的类别，输出为你模型预测出的狗类别（`Affenpinscher`, `Afghan_hound` 等）。\n",
    "\n",
    "与步骤5中的模拟函数类似，你的函数应当包含如下三个步骤：\n",
    "\n",
    "1. 根据选定的模型载入图像特征（bottleneck features）\n",
    "2. 将图像特征输输入到你的模型中，并返回预测向量。注意，在该向量上使用 argmax 函数可以返回狗种类的序号。\n",
    "3. 使用在步骤0中定义的 `dog_names` 数组来返回对应的狗种类名称。\n",
    "\n",
    "提取图像特征过程中使用到的函数可以在 `extract_bottleneck_features.py` 中找到。同时，他们应已在之前的代码块中被导入。根据你选定的 CNN 网络，你可以使用 `extract_{network}` 函数来获得对应的图像特征，其中 `{network}` 代表 `VGG19`, `Resnet50`, `InceptionV3`, 或 `Xception` 中的一个。\n",
    " \n",
    "---\n",
    "\n",
    "<a id='question9'></a>  \n",
    "\n",
    "### __问题 9:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path is 'dogImages/test\\007.American_foxhound\\American_foxhound_00484.jpg'\n",
      "The species is American_foxhound,and the model predict results is American_foxhound\n"
     ]
    }
   ],
   "source": [
    "### TODO: 写一个函数，该函数将图像的路径作为输入\n",
    "import random\n",
    "def dog_species_by_path(path):\n",
    "    pathsplit=path.split('\\\\')\n",
    "    return pathsplit[-2].split('.')[-1]\n",
    "\n",
    "### 然后返回此模型所预测的狗的品种\n",
    "def dog_species_by_VGG19(path):\n",
    "    img = cv2.imread(path)\n",
    "    bot_features=extract_VGG19(path_to_tensor(path))\n",
    "    predict = VGG19_model.predict(bot_features)\n",
    "    return(dog_names[np.argmax(predict)])\n",
    "\n",
    "#Verfiy\n",
    "files = test_files\n",
    "path = files[random.randrange(0, len(files))]\n",
    "print('File path is \\'{}\\''.format(path))\n",
    "print('The species is {0},and the model predict results is {1}'.format(dog_species_by_path(path),dog_species_by_VGG19(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='step6'></a>\n",
    "## 步骤 6: 完成你的算法\n",
    "\n",
    "\n",
    "\n",
    "实现一个算法，它的输入为图像的路径，它能够区分图像是否包含一个人、狗或两者都不包含，然后：\n",
    "\n",
    "- 如果从图像中检测到一只__狗__，返回被预测的品种。\n",
    "- 如果从图像中检测到__人__，返回最相像的狗品种。\n",
    "- 如果两者都不能在图像中检测到，输出错误提示。\n",
    "\n",
    "我们非常欢迎你来自己编写检测图像中人类与狗的函数，你可以随意地使用上方完成的 `face_detector` 和 `dog_detector` 函数。你__需要__在步骤5使用你的CNN来预测狗品种。\n",
    "\n",
    "下面提供了算法的示例输出，但你可以自由地设计自己的模型！\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id='question10'></a>  \n",
    "\n",
    "### __问题 10:__\n",
    "\n",
    "在下方代码块中完成你的代码。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 设计你的算法\n",
    "### 自由地使用所需的代码单元数吧\n",
    "\n",
    "def human_or_dog(path):\n",
    "    if dog_detector(path):\n",
    "        return 'Detect a dog and the species maybe {0}'.format(dog_species_by_VGG19(path))\n",
    "    elif face_detector(path):\n",
    "        return 'Detect one peoson and looks little like {}.'.format(dog_species_by_VGG19(path))\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## 步骤 7: 测试你的算法\n",
    "\n",
    "在这个部分中，你将尝试一下你的新算法！算法认为__你__看起来像什么类型的狗？如果你有一只狗，它可以准确地预测你的狗的品种吗？如果你有一只猫，它会将你的猫误判为一只狗吗？\n",
    "\n",
    "\n",
    "<a id='question11'></a>  \n",
    "\n",
    "### __问题 11:__\n",
    "\n",
    "在下方编写代码，用至少6张现实中的图片来测试你的算法。你可以使用任意照片，不过请至少使用两张人类图片（要征得当事人同意哦）和两张狗的图片。\n",
    "同时请回答如下问题：\n",
    "\n",
    "1. 输出结果比你预想的要好吗 :) ？或者更糟 :( ？\n",
    "2. 提出至少三点改进你的模型的想法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name is Muhammad_Ali_0001.jpg\n",
      "Error\n",
      "\n",
      "File name is Yoko_Ono_0001.jpg\n",
      "Detect one peoson and looks little like Nova_scotia_duck_tolling_retriever.\n",
      "\n",
      "File name is Svetlana_Koroleva_0002.jpg\n",
      "Detect one peoson and looks little like Pharaoh_hound.\n",
      "\n",
      "File name is Dalmatian_04056.jpg\n",
      "Detect a dog and the species maybe Dalmatian\n",
      "\n",
      "File name is Doberman_pinscher_04156.jpg\n",
      "Detect a dog and the species maybe Doberman_pinscher\n",
      "\n",
      "File name is Papillon_07480.jpg\n",
      "Detect a dog and the species maybe Papillon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: 在你的电脑上，在步骤6中，至少在6张图片上运行你的算法。\n",
    "## 自由地使用所需的代码单元数吧\n",
    "from os.path import basename\n",
    "\n",
    "files = [*human_files[0:3] , *test_files[0:3]]\n",
    "for item in files:\n",
    "    print('File name is {}'.format(basename(item)))\n",
    "    print('{}\\n'.format(human_or_dog(item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 比预期差一些，有部份人脸无法辨识，而狗的品种准确率不足七成，换足话说三个就约有一个会错误，还有不少增进的机会。\n",
    "2. \n",
    "A: 由课程癌症检测深度学习里学习到，透过不相关的照片，如植物、猫、狗、鸟等去训练，可以提升癌症的预测，因此可以尝试非相关的图檔资料当作训练来源，如同人类多方增加不同领域的刺激，脑袋会更灵活。<br>\n",
    "B: 透过原有资料，经过旋转角度，移动不同角度，增加不同训练的元素，提升模型的预测能力。<br>\n",
    "C: 激活函式会影响模型的学习，可以多试试不同的激活函式(Activation Function)，比较对模型训练的差异。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意: 当你写完了所有的代码，并且回答了所有的问题。你就可以把你的 iPython Notebook 导出成 HTML 文件。你可以在菜单栏，这样导出File -> Download as -> HTML (.html)把这个 HTML 和这个 iPython notebook 一起做为你的作业提交。**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
